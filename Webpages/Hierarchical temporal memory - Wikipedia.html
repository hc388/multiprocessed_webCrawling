<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Hierarchical temporal memory - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRequestId":"XkWahgpAMFIAAEccqzMAAABT","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Hierarchical_temporal_memory","wgTitle":"Hierarchical temporal memory","wgCurRevisionId":938827777,"wgRevisionId":938827777,"wgArticleId":11273721,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Use American English from February 2019","All Wikipedia articles written in American English","Articles with short description",
"Wikipedia articles needing clarification from August 2018","Belief revision","Artificial neural networks","Deep learning","Unsupervised learning","Semisupervised learning"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Hierarchical_temporal_memory","wgRelevantArticleId":11273721,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q652594","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={
"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init",
"ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.tokens@tffin",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.35.0-wmf.18"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Hierarchical_temporal_memory"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Hierarchical_temporal_memory"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Hierarchical_temporal_memory rootpage-Hierarchical_temporal_memory skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">Hierarchical temporal memory</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><p class="mw-empty-elt">
</p>
<div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Biological theory of intelligence</div>
<p><b>Hierarchical temporal memory</b> (<b>HTM</b>) is a biologically constrained theory (or model) of intelligence, originally described in the 2004 book <i><a href="/wiki/On_Intelligence" title="On Intelligence">On Intelligence</a></i> by <a href="/wiki/Jeff_Hawkins" title="Jeff Hawkins">Jeff Hawkins</a> with <a href="/wiki/Sandra_Blakeslee" title="Sandra Blakeslee">Sandra Blakeslee</a>. HTM is based on <a href="/wiki/Neuroscience" title="Neuroscience">neuroscience</a> and the <a href="/wiki/Physiology" title="Physiology">physiology</a> and interaction of <a href="/wiki/Pyramidal_cell" title="Pyramidal cell">pyramidal neurons</a> in the <a href="/wiki/Neocortex" title="Neocortex">neocortex</a> of the <a href="/wiki/Mammal" title="Mammal">mammalian</a> (in particular, human) brain.
</p><p>At the core of HTM are learning algorithms that can store, learn, infer and recall high-order sequences. Unlike most other machine learning methods, HTM learns (in an <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> fashion) time-based patterns in unlabeled data on a continuous basis. HTM is robust to noise, and it has high capacity, meaning that it can learn multiple patterns simultaneously. When applied to computers, HTM is well suited for prediction,<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup> anomaly detection,<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> classification and ultimately sensorimotor applications.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup>
</p><p>The theory has been tested and implemented in software through example applications from <a href="/wiki/Numenta" title="Numenta">Numenta</a> and a few commercial applications from Numenta's partners.
</p>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Structure_and_algorithms"><span class="tocnumber">1</span> <span class="toctext">Structure and algorithms</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#As_an_evolving_theory"><span class="tocnumber">2</span> <span class="toctext">As an evolving theory</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#First_generation:_zeta_1"><span class="tocnumber">2.1</span> <span class="toctext">First generation: zeta 1</span></a>
<ul>
<li class="toclevel-3 tocsection-4"><a href="#Training"><span class="tocnumber">2.1.1</span> <span class="toctext">Training</span></a></li>
<li class="toclevel-3 tocsection-5"><a href="#Inference"><span class="tocnumber">2.1.2</span> <span class="toctext">Inference</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-6"><a href="#Second_generation:_cortical_learning_algorithms"><span class="tocnumber">2.2</span> <span class="toctext">Second generation: cortical learning algorithms</span></a>
<ul>
<li class="toclevel-3 tocsection-7"><a href="#Spatial_pooling"><span class="tocnumber">2.2.1</span> <span class="toctext">Spatial pooling</span></a>
<ul>
<li class="toclevel-4 tocsection-8"><a href="#Active,_inactive_and_predictive_cells"><span class="tocnumber">2.2.1.1</span> <span class="toctext">Active, inactive and predictive cells</span></a>
<ul>
<li class="toclevel-5 tocsection-9"><a href="#How_do_cells_become_active?"><span class="tocnumber">2.2.1.1.1</span> <span class="toctext">How do cells become active?</span></a></li>
<li class="toclevel-5 tocsection-10"><a href="#How_do_cells_become_predictive?"><span class="tocnumber">2.2.1.1.2</span> <span class="toctext">How do cells become predictive?</span></a></li>
</ul>
</li>
<li class="toclevel-4 tocsection-11"><a href="#The_ouput_of_a_minicolumn"><span class="tocnumber">2.2.1.2</span> <span class="toctext">The ouput of a minicolumn</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-12"><a href="#Inference_and_online_learning"><span class="tocnumber">2.2.2</span> <span class="toctext">Inference and online learning</span></a></li>
<li class="toclevel-3 tocsection-13"><a href="#Applications_of_the_CLAs"><span class="tocnumber">2.2.3</span> <span class="toctext">Applications of the CLAs</span></a></li>
<li class="toclevel-3 tocsection-14"><a href="#The_validity_of_the_CLAs"><span class="tocnumber">2.2.4</span> <span class="toctext">The validity of the CLAs</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-15"><a href="#Third_generation:_sensorimotor_inference"><span class="tocnumber">2.3</span> <span class="toctext">Third generation: sensorimotor inference</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-16"><a href="#Comparison_of_neuron_models"><span class="tocnumber">3</span> <span class="toctext">Comparison of neuron models</span></a></li>
<li class="toclevel-1 tocsection-17"><a href="#Comparing_HTM_and_neocortex"><span class="tocnumber">4</span> <span class="toctext">Comparing HTM and neocortex</span></a></li>
<li class="toclevel-1 tocsection-18"><a href="#Sparse_distributed_representations"><span class="tocnumber">5</span> <span class="toctext">Sparse distributed representations</span></a></li>
<li class="toclevel-1 tocsection-19"><a href="#Similarity_to_other_models"><span class="tocnumber">6</span> <span class="toctext">Similarity to other models</span></a>
<ul>
<li class="toclevel-2 tocsection-20"><a href="#Bayesian_networks"><span class="tocnumber">6.1</span> <span class="toctext">Bayesian networks</span></a></li>
<li class="toclevel-2 tocsection-21"><a href="#Neural_networks"><span class="tocnumber">6.2</span> <span class="toctext">Neural networks</span></a></li>
<li class="toclevel-2 tocsection-22"><a href="#Neocognitron"><span class="tocnumber">6.3</span> <span class="toctext">Neocognitron</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-23"><a href="#NuPIC_platform_and_development_tools"><span class="tocnumber">7</span> <span class="toctext">NuPIC platform and development tools</span></a></li>
<li class="toclevel-1 tocsection-24"><a href="#Applications"><span class="tocnumber">8</span> <span class="toctext">Applications</span></a></li>
<li class="toclevel-1 tocsection-25"><a href="#See_also"><span class="tocnumber">9</span> <span class="toctext">See also</span></a>
<ul>
<li class="toclevel-2 tocsection-26"><a href="#Related_models"><span class="tocnumber">9.1</span> <span class="toctext">Related models</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-27"><a href="#References"><span class="tocnumber">10</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-28"><a href="#External_links"><span class="tocnumber">11</span> <span class="toctext">External links</span></a>
<ul>
<li class="toclevel-2 tocsection-29"><a href="#Official"><span class="tocnumber">11.1</span> <span class="toctext">Official</span></a></li>
<li class="toclevel-2 tocsection-30"><a href="#Other"><span class="tocnumber">11.2</span> <span class="toctext">Other</span></a></li>
</ul>
</li>
</ul>
</div>

<h2><span class="mw-headline" id="Structure_and_algorithms">Structure and algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=1" title="Edit section: Structure and algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A typical HTM network is a <a href="/wiki/Tree_(data_structure)" title="Tree (data structure)">tree</a>-shaped hierarchy of <i>levels</i> (which should <i>not</i> be confused with the "<i>layers</i>" of the <a href="/wiki/Neocortex" title="Neocortex">neocortex</a>, as described <a href="/wiki/Hierarchical_temporal_memory#Second_generation:_cortical_learning_algorithms" title="Hierarchical temporal memory">below</a>) that are composed of smaller elements called <i>region</i>s (or nodes). A single level in the hierarchy possibly contains several regions. Higher hierarchy levels often have fewer regions. Higher hierarchy levels can reuse patterns learned at the lower levels by combining them to memorize more complex patterns.
</p><p>Each HTM region has the same basic functionality. In learning and inference modes, sensory data (e.g. data from the eyes) comes into the bottom level regions. In generation mode, the bottom level regions output the generated pattern of a given category. The top level usually has a single region that stores the most general categories (concepts) which determine, or are determined by, smaller concepts in the lower levels which are more restricted in time and space<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (August 2018)">clarification needed</span></a></i>&#93;</sup>. When set in inference mode, a region (in each level) interprets information coming in from its child regions in the lower level as probabilities of the categories it has in memory.
</p><p>Each HTM region learns by identifying and memorizing spatial patterns, which are combinations of input bits that often occur at the same time. It then identifies temporal sequences of spatial patterns that are likely to occur one after another.
</p>
<h2><span class="mw-headline" id="As_an_evolving_theory">As an evolving theory</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=2" title="Edit section: As an evolving theory">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>HTM is an <a href="/wiki/Evolving" class="mw-redirect" title="Evolving">evolving</a> theory (or model), that is, it isn't yet a complete theory, as our knowledge of the <a href="/wiki/Neocortex" title="Neocortex">neocortex</a> is incomplete. The new findings on the neocortex are thus progressively incorporated into the HTM model, which can thus change over time. The new findings do not necessarily invalidate the previous ones, so ideas from one generation are not necessarily excluded in its successive one. Because of this evolving nature of the theory, there have been several generations of HTM algorithms,<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup> which are briefly described below.
</p>
<h3><span class="mw-headline" id="First_generation:_zeta_1">First generation: zeta 1</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=3" title="Edit section: First generation: zeta 1">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The first generation of HTM algorithms (or the first version of the HTM theory) is sometimes referred to as <i>zeta 1</i>.
</p>
<h4><span class="mw-headline" id="Training">Training</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=4" title="Edit section: Training">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>During <i>training</i>, a node (or region) receives a temporal sequence of spatial patterns as its input. The learning process consists of two stages:
</p>
<ol><li>The <b>spatial pooling</b> identifies (in the input) frequently observed patterns and memorizes them as "coincidences". Patterns that are significantly similar to each other are treated as the same coincidence. A large number of possible input patterns are reduced to a manageable number of known coincidences.</li>
<li>The <b>temporal pooling</b> partitions coincidences that are likely to follow each other in the training sequence into temporal groups. Each group of patterns represents a "cause" of the input pattern (or "name" in <i>On Intelligence</i>).</li></ol>
<p>The concepts of <i>spatial pooling</i> and <i>temporal pooling</i> are still quite important in the current HTM theory. Temporal pooling is not yet well understood, and its meaning has changed over time (as the HTM theory evolved).
</p>
<h4><span class="mw-headline" id="Inference">Inference</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=5" title="Edit section: Inference">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>During <b>inference</b>, the node calculates the set of probabilities that a pattern belongs to each known coincidence. Then it calculates the probabilities that the input represents each temporal group. The set of probabilities assigned to the groups is called a node's "belief" about the input pattern. (In a simplified implementation, node's belief consists of only one winning group). This belief is the result of the inference that is passed to one or more "parent" nodes in the next higher level of the hierarchy.
</p><p>"Unexpected" patterns to the node do not have a dominant probability of belonging to any one temporal group, but have nearly equal probabilities of belonging to several of the groups. If sequences of patterns are similar to the training sequences, then the assigned probabilities to the groups will not change as often as patterns are received.  The output of the node will not change as much, and a resolution in time<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="What exactly is meant by &quot;resolution&quot; and &quot;resolution in time&quot; here? (August 2018)">clarification needed</span></a></i>&#93;</sup> is lost.
</p><p>In a more general scheme, the node's belief can be sent to the input of any node(s) in any level(s), but the connections between the nodes are still fixed. The higher-level node combines this output with the output from other child nodes thus forming its own input pattern.
</p><p>Since resolution in space and time is lost in each node as described above, beliefs formed by higher-level nodes represent an even larger range of space and time. This is meant to reflect the organization of the physical world as it is perceived by human brain. Larger concepts (e.g. causes, actions and objects) are perceived to change more slowly and consist of smaller concepts that change more quickly. Jeff Hawkins postulates that brains evolved this type of hierarchy to match, predict, and affect the organization of the external world.
</p><p>More details about the functioning of Zeta 1 HTM can be found in Numenta's old documentation.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Second_generation:_cortical_learning_algorithms">Second generation: cortical learning algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=6" title="Edit section: Second generation: cortical learning algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The second generation of HTM learning algorithms, often referred to as cortical learning algorithms (CLA), was drastically different from zeta 1. It relies on a <a href="/wiki/Data_structure" title="Data structure">data structure</a> called <b><a href="/wiki/Sparse_coding" class="mw-redirect" title="Sparse coding">sparse distributed representations</a></b> (that is, a data structure whose elements are binary, 1 or 0, and whose number of 1 bits is small compared to the number of 0 bits) to represent the brain activity and a more biologically-realistic neuron model (often also referred to as <b>cell</b>, in the context of the HTM theory).<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup> There are two core components in this HTM theory: a <b>spatial pooling</b> algorithm,<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup> which outputs <a href="/wiki/Sparse_distributed_representation" class="mw-redirect" title="Sparse distributed representation">sparse distributed representations</a> (SDR), and a <b>sequence memory</b> algorithm,<sup id="cite_ref-Neuronpaper_8-0" class="reference"><a href="#cite_note-Neuronpaper-8">&#91;8&#93;</a></sup> which learns to represent and predict complex sequences.
</p><p>In this new generation, the <b><a href="/wiki/Cerebral_cortex#Layers" title="Cerebral cortex">layers</a></b> and <b><a href="/wiki/Cortical_minicolumn" title="Cortical minicolumn">minicolumns</a></b> of the <a href="/wiki/Cerebral_cortex" title="Cerebral cortex">cerebral cortex</a> are addressed and partially modeled. Each HTM layer (not to be confused with an HTM level of an HTM hierarchy, as described <a href="/wiki/Hierarchical_temporal_memory#HTM_structure_and_algorithms" title="Hierarchical temporal memory">above</a>) consists of a number of highly interconnected minicolumns. An HTM layer creates a sparse distributed representation from its input, so that a fixed percentage of <i><a href="/wiki/Cortical_minicolumn" title="Cortical minicolumn">minicolumns</a></i> are active at any one time<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="What is the connection between the sparse distributed representation and the minicolumns? (August 2018)">clarification needed</span></a></i>&#93;</sup>. A minicolumn is understood as a group of cells that have the same <a href="/wiki/Receptive_field" title="Receptive field">receptive field</a>. Each minicolumn has a number of cells that are able to remember several previous states. A cell can be in one of three states: <i>active</i>, <i>inactive</i> and <i>predictive</i> state.
</p>
<h4><span class="mw-headline" id="Spatial_pooling">Spatial pooling</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=7" title="Edit section: Spatial pooling">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The receptive field of each minicolumn is a fixed number of inputs that are randomly selected from a much larger number of node inputs. Based on the (specific) input pattern, some minicolumns will be more or less associated with the active input values. <b>Spatial pooling</b> selects a relatively constant number of the most active minicolumns and inactivates (inhibits) other minicolumns in the vicinity of the active ones. Similar input patterns tend to activate a stable set of minicolumns. The amount of memory used by each layer can be increased to learn more complex spatial patterns or decreased to learn simpler patterns.
</p>
<h5><span id="Active.2C_inactive_and_predictive_cells"></span><span class="mw-headline" id="Active,_inactive_and_predictive_cells">Active, inactive and predictive cells</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=8" title="Edit section: Active, inactive and predictive cells">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>As mentioned above, a cell (or a neuron) of a minicolumn, at any point in time, can be in an active, inactive or predictive state. Initially, cells are inactive.
</p>
<h6><span id="How_do_cells_become_active.3F"></span><span class="mw-headline" id="How_do_cells_become_active?">How do cells become active?</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=9" title="Edit section: How do cells become active?">edit</a><span class="mw-editsection-bracket">]</span></span></h6>
<p>If one or more cells in the active minicolumn are in the <i>predictive</i> state (see below), they will be the only cells to become active in the current time step. If none of the cells in the active minicolumn are in the predictive state (which happens during the initial time step or when the activation of this minicolumn was not expected), all cells are made active.
</p>
<h6><span id="How_do_cells_become_predictive.3F"></span><span class="mw-headline" id="How_do_cells_become_predictive?">How do cells become predictive?</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=10" title="Edit section: How do cells become predictive?">edit</a><span class="mw-editsection-bracket">]</span></span></h6>
<p>When a cell becomes active, it gradually forms connections to nearby cells that tend to be active during several previous time steps. Thus a cell learns to recognize a known sequence by checking whether the connected cells are active. If a large number of connected cells are active, this cell switches to the <i>predictive</i> state in anticipation of one of the few next inputs of the sequence.
</p>
<h5><span class="mw-headline" id="The_ouput_of_a_minicolumn">The ouput of a minicolumn</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=11" title="Edit section: The ouput of a minicolumn">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>The output of a layer includes minicolumns in both active and predictive states. Thus minicolumns are active over longer periods of time, which leads to greater temporal stability seen by the parent layer.
</p>
<h4><span class="mw-headline" id="Inference_and_online_learning">Inference and online learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=12" title="Edit section: Inference and online learning">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Cortical learning algorithms are able to learn continuously from each new input pattern, therefore no separate inference mode is necessary. During inference, HTM tries to match the stream of inputs to fragments of previously learned sequences. This allows each HTM layer to be constantly predicting the likely continuation of the recognized sequences. The index of the predicted sequence is the output of the layer. Since predictions tend to change less frequently than the input patterns, this leads to increasing temporal stability of the output in higher hierarchy levels. Prediction also helps to fill in missing patterns in the sequence and to interpret ambiguous data by biasing the system to infer what it predicted.
</p>
<h4><span class="mw-headline" id="Applications_of_the_CLAs">Applications of the CLAs</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=13" title="Edit section: Applications of the CLAs">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Cortical learning algorithms are currently being offered as commercial <a href="/wiki/SaaS" class="mw-redirect" title="SaaS">SaaS</a> by Numenta (such as Grok<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup>).
</p>
<h4><span class="mw-headline" id="The_validity_of_the_CLAs">The validity of the CLAs</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=14" title="Edit section: The validity of the CLAs">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The following question was posed to Jeff Hawkins September 2011 with regard to cortical learning algorithms: "How do you know if the changes you are making to the model are good or not?" To which Jeff's response was "There are two categories for the answer:  one is to look at neuroscience, and the other is methods for machine intelligence. In the neuroscience realm there are many predictions that we can make, and those can be tested. If our theories explain a vast array of neuroscience observations then it tells us that we’re on the right track. In the machine learning world they don’t care about that, only how well it works on practical problems. In our case that remains to be seen. To the extent you can solve a problem that no one was able to solve before, people will take notice."<sup id="cite_ref-ai.stanford.edu_10-0" class="reference"><a href="#cite_note-ai.stanford.edu-10">&#91;10&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Third_generation:_sensorimotor_inference">Third generation: sensorimotor inference</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=15" title="Edit section: Third generation: sensorimotor inference">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The third generation builds on the second generation and adds in a theory of sensorimotor inference in the neocortex.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup><sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup> This theory proposes that <a href="/wiki/Cortical_column" title="Cortical column">cortical columns</a> at every level of the hierarchy can learn complete models of objects over time and that features are learned at specific locations on the objects. The theory was expanded in 2018 and referred to as the Thousand Brains Theory. <sup id="cite_ref-thousandbrains_13-0" class="reference"><a href="#cite_note-thousandbrains-13">&#91;13&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Comparison_of_neuron_models">Comparison of neuron models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=16" title="Edit section: Comparison of neuron models">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:428px;"><a href="/wiki/File:Neuron_comparison.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/0/05/Neuron_comparison.png" decoding="async" width="426" height="362" class="thumbimage" data-file-width="426" data-file-height="362" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Neuron_comparison.png" class="internal" title="Enlarge"></a></div>Comparing the artificial neural network (A), the biological neuron (B), and the HTM neuron (C).</div></div></div></div>
<dl><dd><table class="wikitable" border="1" cellpadding="4" cellspacing="4">
<caption>Comparison of Neuron Models
</caption>
<tbody><tr>
<th>Artificial Neural Network (ANN)
</th>
<th>Neocortical Pyramidal Neuron (Biological Neuron)
</th>
<th>HTM Model Neuron<sup id="cite_ref-Neuronpaper_8-1" class="reference"><a href="#cite_note-Neuronpaper-8">&#91;8&#93;</a></sup>
</th></tr>
<tr style="vertical-align:top">
<td><div><ul><li>Few synapses</li><li>No dendrites</li><li>Sum input x weights</li><li>Learns by modifying weights of synapses</li></ul></div>
</td>
<td><div><ul><li>Thousands of synapses on the dendrites</li><li>Active dendrites: cell recognizes hundreds of unique patterns</li><li>Co-activation of a set of synapses on a dendritic segment causes an NMDA spike<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="What is a NMDA spike? (August 2018)">clarification needed</span></a></i>&#93;</sup> and depolarization<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="What do we mean by &quot;depolarization&quot; here? (August 2018)">clarification needed</span></a></i>&#93;</sup> at the soma</li><li>Sources of input to the cell:
<ol><li>Feedforward inputs which form synapses proximal to the soma and directly lead to action potentials</li>
<li>NMDA spikes generated in the more distal basal<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="What is the distal basal? (August 2018)">clarification needed</span></a></i>&#93;</sup></li>
<li>Apical dendrites that depolarize the soma (usually not sufficient enough to generate a somatic action potential)</li><li>Learns by growing new synapses</li></ol></li></ul></div>
</td>
<td><div><ul><li>Inspired by the pyramidal cells in neocortex layers 2/3 and 5</li><li>Thousands of synapses</li><li>Active dendrites: cell recognizes hundreds of unique patterns</li><li>Models dendrites and NMDA spikes with each array of coincident detectors having a set of synapses</li><li>Learns by modeling the growth of new synapses</li></ul></div>
</td></tr></tbody></table></dd></dl>
<h2><span class="mw-headline" id="Comparing_HTM_and_neocortex">Comparing HTM and neocortex</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=17" title="Edit section: Comparing HTM and neocortex">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>HTM attempts to implement the functionality that is characteristic of a hierarchically related group of cortical regions in the neocortex. A <i>region</i> of the neocortex corresponds to one or more <i>levels</i> in the HTM hierarchy, while the <a href="/wiki/Hippocampus" title="Hippocampus">hippocampus</a> is remotely similar to the highest HTM level. A single HTM node may represent a group of <a href="/wiki/Cortical_column" title="Cortical column">cortical columns</a> within a certain region.
</p><p>Although it is primarily a functional model, several attempts have been made to relate the algorithms of the HTM with the structure of neuronal connections in the layers of neocortex.<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup><sup id="cite_ref-15" class="reference"><a href="#cite_note-15">&#91;15&#93;</a></sup> The neocortex is organized in vertical columns of 6 horizontal layers.  The 6 layers of cells in the neocortex should not be confused with levels in an HTM hierarchy.
</p><p>HTM nodes attempt to model a portion of cortical columns (80 to 100 neurons) with approximately 20 HTM "cells" per column.  HTMs model only layers 2 and 3 to detect spatial and temporal features of the input with 1 cell per column in layer 2 for spatial "pooling", and 1 to 2 dozen per column in layer 3 for temporal pooling. A key to HTMs and the cortex's is their ability to deal with noise and variation in the input which is a result of using a "sparse distributive representation" where only about 2% of the columns are active at any given time.
</p><p>An HTM attempts to model a portion of the cortex's learning and plasticity as described above. Differences between HTMs and neurons include:<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup>
</p>
<ul><li>strictly binary signals and synapses</li>
<li>no direct inhibition of synapses or dendrites (but simulated indirectly)</li>
<li>currently only models layers 2/3 and 4 (no 5 or 6)</li>
<li>no "motor" control (layer 5)</li>
<li>no feed-back between regions (layer 6 of high to layer 1 of low)</li></ul>
<h2><span class="mw-headline" id="Sparse_distributed_representations">Sparse distributed representations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=18" title="Edit section: Sparse distributed representations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Integrating memory component with neural networks has a long history dating back to early research in distributed representations<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup><sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup> and <a href="/wiki/Self-organizing_map" title="Self-organizing map">self-organizing maps</a>. For example, in <a href="/wiki/Sparse_distributed_memory" title="Sparse distributed memory">sparse distributed memory</a> (SDM), the patterns encoded by neural networks are used as memory addresses for <a href="/wiki/Content-addressable_memory" title="Content-addressable memory">content-addressable memory</a>, with "neurons" essentially serving as address encoders and decoders.<sup id="cite_ref-kanerva1988_19-0" class="reference"><a href="#cite_note-kanerva1988-19">&#91;19&#93;</a></sup><sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup>
</p><p>Computers store information in "dense" representations such as a 32 bit word where all combinations of 1s and 0s are possible.
By contrast, brains use sparse distributed representations (SDR).<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup> The human neocortex has roughly 16 billion neurons, but at any given time only a small percent are active. The activity of neurons are like bits in a computer, and therefore the representation is sparse. Similarly to <a href="/wiki/Sparse_distributed_memory" title="Sparse distributed memory">SDM</a> developed by <a href="/wiki/NASA" title="NASA">NASA</a> in the 80s<sup id="cite_ref-kanerva1988_19-1" class="reference"><a href="#cite_note-kanerva1988-19">&#91;19&#93;</a></sup> and <a href="/wiki/Vector_space" title="Vector space">vector space</a> models used in <a href="/wiki/Latent_semantic_analysis" title="Latent semantic analysis">Latent semantic analysis</a>, HTM also uses Sparse Distributed Representations.<sup id="cite_ref-nupicSDRs_22-0" class="reference"><a href="#cite_note-nupicSDRs-22">&#91;22&#93;</a></sup>
</p><p>The SDRs used in HTM are binary representations of data consisting of many bits with a small percentage of the bits active (1s); a typical implementation might have 2048 columns and 64K artificial neurons where as few as 40 might be active at once.  Although it may seem less efficient for the majority of bits to go "unused" in any given representation, SDRs have two major advantages over traditional dense representations.  First, SDRs are tolerant of corruption and ambiguity due to the meaning of the representation being shared (<i>distributed</i>) across a small percentage (<i>sparse</i>) of active bits.  In a dense representation, flipping a single bit completely changes the meaning, while in an SDR a single bit may not affect the overall meaning much.  This leads to the second advantage of SDRs: because the meaning of a representation is distributed across all active bits, similarity between two representations can be used as a measure of <a href="/wiki/Semantic" class="mw-redirect" title="Semantic">semantic</a> similarity in the objects they represent.  That is, if two vectors in an SDR have 1s in the same position, then they are semantically similar in that attribute.  The bits in SDRs have semantic meaning, and that meaning is distributed across the bits.<sup id="cite_ref-nupicSDRs_22-1" class="reference"><a href="#cite_note-nupicSDRs-22">&#91;22&#93;</a></sup>
</p><p>The <a href="/wiki/Semantic_folding" title="Semantic folding">semantic folding</a> theory<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup> builds on these SDR properties to propose a new model for language semantics, where words are encoded into word-SDRs and the similarity between terms, sentences and texts can be calculated with simple distance measures.
</p>
<h2><span class="mw-headline" id="Similarity_to_other_models">Similarity to other models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=19" title="Edit section: Similarity to other models">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Bayesian_networks">Bayesian networks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=20" title="Edit section: Bayesian networks">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Likened to a <a href="/wiki/Bayesian_network" title="Bayesian network">Bayesian network</a>, an HTM comprises a collection of nodes that are arranged in a tree-shaped hierarchy. Each node in the hierarchy discovers an array of causes in the input patterns and temporal sequences it receives. A Bayesian <a href="/wiki/Belief_revision" title="Belief revision">belief revision</a> algorithm is used to propagate feed-forward and feedback beliefs from child to parent nodes and vice versa. However, the analogy to Bayesian networks is limited, because HTMs can be self-trained (such that each node has an unambiguous family relationship), cope with time-sensitive data, and grant mechanisms for covert attention.
</p><p>A theory of hierarchical cortical computation based on Bayesian <a href="/wiki/Belief_propagation" title="Belief propagation">belief propagation</a> was proposed earlier by Tai Sing Lee and <a href="/wiki/David_Mumford" title="David Mumford">David Mumford</a>.<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup> While HTM is mostly consistent with these ideas, it adds details about handling invariant representations in the visual cortex.<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Neural_networks">Neural networks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=21" title="Edit section: Neural networks">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Like any system that models details of the neocortex, HTM can be viewed as an <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a>. The tree-shaped hierarchy commonly used in HTMs resembles the usual topology of traditional neural networks. HTMs attempt to model cortical columns (80 to 100 neurons) and their interactions with fewer HTM "neurons". The goal of current HTMs is to capture as much of the functions of neurons and the network (as they are currently understood) within the capability of typical computers and in areas that can be made readily useful such as image processing.  For example, feedback from higher levels and motor control are not attempted because it is not yet understood how to incorporate them and binary instead of variable synapses are used because they were determined to be sufficient in the current HTM capabilities.
</p><p>LAMINART and similar neural networks researched by Stephen Grossberg attempt to model both the infrastructure of the cortex and the behavior of neurons in a temporal framework to explain neurophysiological and psychophysical data.  However, these networks are, at present, too complex for realistic application.<sup id="cite_ref-26" class="reference"><a href="#cite_note-26">&#91;26&#93;</a></sup>
</p><p>HTM is also related to work by <a href="/wiki/Tomaso_Poggio" title="Tomaso Poggio">Tomaso Poggio</a>, including an approach for modeling the ventral stream of the visual cortex known as HMAX. Similarities of HTM to various AI ideas are described in the December 2005 issue of the Artificial Intelligence journal.<sup id="cite_ref-27" class="reference"><a href="#cite_note-27">&#91;27&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Neocognitron">Neocognitron</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=22" title="Edit section: Neocognitron">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Neocognitron" title="Neocognitron">Neocognitron</a>, a hierarchical multilayered neural network proposed by Professor Kunihiko Fukushima in 1987, is one of the first Deep Learning Neural Networks models.<sup id="cite_ref-28" class="reference"><a href="#cite_note-28">&#91;28&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="NuPIC_platform_and_development_tools">NuPIC platform and development tools</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=23" title="Edit section: NuPIC platform and development tools">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The <a rel="nofollow" class="external text" href="https://github.com/numenta/nupic">Numenta Platform for Intelligent Computer (NuPIC)</a> is one of several available <a rel="nofollow" class="external text" href="https://numenta.org/implementations/">HTM implementations</a>. Some are provided by <a rel="nofollow" class="external text" href="https://numenta.com/">Numenta</a>, while some are developed and maintained by the <a rel="nofollow" class="external text" href="https://numenta.org/">HTM open source community</a>.
</p><p>NuPIC includes implementations of Spatial Pooling and Temporal Memory in both C++ and Python. It also includes <a rel="nofollow" class="external text" href="http://nupic.docs.numenta.org/stable/api/">3 APIs</a>. Users can construct HTM systems using direct implementations of the <a rel="nofollow" class="external text" href="http://nupic.docs.numenta.org/stable/quick-start/algorithms.html">algorithms</a>, or construct a Network using the <a rel="nofollow" class="external text" href="http://nupic.docs.numenta.org/stable/quick-start/network.html">Network API</a>, which is a flexible framework for constructing complicated associations between different Layers of cortex.
</p><p><a rel="nofollow" class="external text" href="https://github.com/numenta/nupic/releases/tag/1.0.0">NuPIC 1.0</a> was released on July 2017, after which the codebase was put into maintenance mode. Current research continues in Numenta <a rel="nofollow" class="external text" href="https://github.com/numenta/htmresearch">research codebases</a>.
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=24" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The following commercial applications are available using NuPIC: 
</p>
<ul><li>Grok – anomaly detection for IT servers, see <a rel="nofollow" class="external text" href="http://www.grokstream.com">www.grokstream.com</a></li>
<li>Cortical.io – advanced natural language processing, see <a rel="nofollow" class="external text" href="http://www.cortical.io">www.cortical.io</a></li></ul>
<p>The following tools are available on NuPIC:
</p>
<ul><li>HTM Studio – find anomalies in time series using your own data, see www.<a rel="nofollow" class="external text" href="http://www.Numenta.com/htm-studio/">numenta.com/htm-studio/</a></li>
<li>Numenta Anomaly Benchmark – compare HTM anomalies with other anomaly detection techniques, see <a rel="nofollow" class="external free" href="https://numenta.com/numenta-anomaly-benchmark/">https://numenta.com/numenta-anomaly-benchmark/</a></li></ul>
<p>The following example applications are available on NuPIC, see <a rel="nofollow" class="external free" href="http://numenta.com/applications/">http://numenta.com/applications/</a>:
</p>
<ul><li>HTM for stocks – example of tracking anomalies in the stock market (sample code)</li>
<li>Rogue behavior detection – example of finding anomalies in human behavior (white paper and sample code)</li>
<li>Geospatial tracking – example of finding anomalies in objectives moving through space and time (white paper and sample code)</li></ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=25" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Neocognitron" title="Neocognitron">Neocognitron</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a></li>
<li><a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">Strong AI</a></li>
<li><a href="/wiki/Artificial_consciousness" title="Artificial consciousness">Artificial consciousness</a></li>
<li><a href="/wiki/Cognitive_architecture" title="Cognitive architecture">Cognitive architecture</a></li>
<li><i><a href="/wiki/On_Intelligence" title="On Intelligence">On Intelligence</a></i></li>
<li><a href="/wiki/Memory-prediction_framework" title="Memory-prediction framework">Memory-prediction framework</a></li>
<li><a href="/wiki/Belief_revision" title="Belief revision">Belief revision</a></li>
<li><a href="/wiki/Belief_propagation" title="Belief propagation">Belief propagation</a></li>
<li><a href="/wiki/Bionics" title="Bionics">Bionics</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_projects" title="List of artificial intelligence projects">List of artificial intelligence projects</a></li>
<li><a href="/w/index.php?title=Memory_Network&amp;action=edit&amp;redlink=1" class="new" title="Memory Network (page does not exist)">Memory Network</a></li>
<li><a href="/wiki/Neural_Turing_Machine" class="mw-redirect" title="Neural Turing Machine">Neural Turing Machine</a></li>
<li><a href="/wiki/Multiple_trace_theory" title="Multiple trace theory">Multiple trace theory</a></li></ul>
<h3><span class="mw-headline" id="Related_models">Related models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=26" title="Edit section: Related models">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li><a href="/wiki/Hierarchical_hidden_Markov_model" title="Hierarchical hidden Markov model">Hierarchical hidden Markov model</a></li>
<li><a href="/wiki/Bayesian_networks" class="mw-redirect" title="Bayesian networks">Bayesian networks</a></li>
<li><a href="/wiki/Neural_networks" class="mw-redirect" title="Neural networks">Neural networks</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=27" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation journal">Cui, Yuwei; Ahmad, Subutai; Hawkins, Jeff (2016). "Continuous Online Sequence Learning with an Unsupervised Neural Network Model". <i>Neural Computation</i>. <b>28</b> (11): 2474–2504. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1512.05463">1512.05463</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1162%2FNECO_a_00893">10.1162/NECO_a_00893</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/27626963">27626963</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Continuous+Online+Sequence+Learning+with+an+Unsupervised+Neural+Network+Model&amp;rft.volume=28&amp;rft.issue=11&amp;rft.pages=2474-2504&amp;rft.date=2016&amp;rft_id=info%3Aarxiv%2F1512.05463&amp;rft_id=info%3Apmid%2F27626963&amp;rft_id=info%3Adoi%2F10.1162%2FNECO_a_00893&amp;rft.aulast=Cui&amp;rft.aufirst=Yuwei&amp;rft.au=Ahmad%2C+Subutai&amp;rft.au=Hawkins%2C+Jeff&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+temporal+memory" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation journal">Ahmad, Subutai; Lavin, Alexander; Purdy, Scott; Agha, Zuha (2017). "Unsupervised real-time anomaly detection for streaming data". <i>Neurocomputing</i>. <b>262</b>: 134–147. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neucom.2017.04.070">10.1016/j.neucom.2017.04.070</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurocomputing&amp;rft.atitle=Unsupervised+real-time+anomaly+detection+for+streaming+data&amp;rft.volume=262&amp;rft.pages=134-147&amp;rft.date=2017&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2017.04.070&amp;rft.aulast=Ahmad&amp;rft.aufirst=Subutai&amp;rft.au=Lavin%2C+Alexander&amp;rft.au=Purdy%2C+Scott&amp;rft.au=Agha%2C+Zuha&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+temporal+memory" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://discourse.numenta.org/t/preliminary-details-about-new-theory-work-on-sensory-motor-inference/697">"Preliminary details about new theory work on sensory-motor inference"</a>. <i>HTM Forum</i>. 2016-06-03.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=HTM+Forum&amp;rft.atitle=Preliminary+details+about+new+theory+work+on+sensory-motor+inference&amp;rft.date=2016-06-03&amp;rft_id=https%3A%2F%2Fdiscourse.numenta.org%2Ft%2Fpreliminary-details-about-new-theory-work-on-sensory-motor-inference%2F697&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+temporal+memory" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=6_wattbWgiU&amp;t=">HTM Retrospective</a></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://web.archive.org/web/20090527174304/http://numenta.com/for-developers/education/general-overview-htm.php">Numenta old documentation</a></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=48r-IeYOvG4">Jeff Hawkins lecture describing cortical learning algorithms</a></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite class="citation journal">Cui, Yuwei; Ahmad, Subutai; Hawkins, Jeff (2017). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5712570">"The HTM Spatial Pooler—A Neocortical Algorithm for Online Sparse Distributed Coding"</a>. <i>Frontiers in Computational Neuroscience</i>. <b>11</b>: 111. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.3389%2Ffncom.2017.00111">10.3389/fncom.2017.00111</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5712570">5712570</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/29238299">29238299</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Frontiers+in+Computational+Neuroscience&amp;rft.atitle=The+HTM+Spatial+Pooler%E2%80%94A+Neocortical+Algorithm+for+Online+Sparse+Distributed+Coding&amp;rft.volume=11&amp;rft.pages=111&amp;rft.date=2017&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5712570&amp;rft_id=info%3Apmid%2F29238299&amp;rft_id=info%3Adoi%2F10.3389%2Ffncom.2017.00111&amp;rft.aulast=Cui&amp;rft.aufirst=Yuwei&amp;rft.au=Ahmad%2C+Subutai&amp;rft.au=Hawkins%2C+Jeff&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5712570&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+temporal+memory" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-Neuronpaper-8"><span class="mw-cite-backlink">^ <a href="#cite_ref-Neuronpaper_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Neuronpaper_8-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.frontiersin.org/articles/10.3389/fncir.2016.00023/full">Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex</a></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://grokstream.com/product/">Grok Product Page</a></span>
</li>
<li id="cite_note-ai.stanford.edu-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-ai.stanford.edu_10-0">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://ai.stanford.edu/~joni/papers/LasersonXRDS2011.pdf">From Neural Networks to Deep Learning: Zeroing in on the Human Brain</a></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hawkins, Jeff; Ahmad, Subutai; Cui, Yuwei (2017). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5661005">"A Theory of How Columns in the Neocortex Enable Learning the Structure of the World"</a>. <i>Frontiers in Neural Circuits</i>. <b>11</b>: 81. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.3389%2Ffncir.2017.00081">10.3389/fncir.2017.00081</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC5661005">5661005</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/29118696">29118696</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Frontiers+in+Neural+Circuits&amp;rft.atitle=A+Theory+of+How+Columns+in+the+Neocortex+Enable+Learning+the+Structure+of+the+World&amp;rft.volume=11&amp;rft.pages=81&amp;rft.date=2017&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5661005&amp;rft_id=info%3Apmid%2F29118696&amp;rft_id=info%3Adoi%2F10.3389%2Ffncir.2017.00081&amp;rft.aulast=Hawkins&amp;rft.aufirst=Jeff&amp;rft.au=Ahmad%2C+Subutai&amp;rft.au=Cui%2C+Yuwei&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5661005&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+temporal+memory" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=yVT7dO_Tf4E">Have We Missed Half of What the Neocortex Does? Allocentric Location as the Basis of Perception</a></span>
</li>
<li id="cite_note-thousandbrains-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-thousandbrains_13-0">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.eurekalert.org/pub_releases/2019-01/kta-np011119.php">"Numenta publishes breakthrough theory for intelligence and cortical computation"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Numenta+publishes+breakthrough+theory+for+intelligence+and+cortical+computation&amp;rft_id=https%3A%2F%2Fwww.eurekalert.org%2Fpub_releases%2F2019-01%2Fkta-np011119.php&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+temporal+memory" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><a href="/wiki/Jeff_Hawkins" title="Jeff Hawkins">Jeff Hawkins</a>, <a href="/wiki/Sandra_Blakeslee" title="Sandra Blakeslee">Sandra Blakeslee</a> <i><a href="/wiki/On_Intelligence" title="On Intelligence">On Intelligence</a></i></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000532">Towards a Mathematical Theory of Cortical Micro-circuits. Dileep George and Jeff Hawkins. PLoS Computational Biology 5(10)</a></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://numenta.org/resources/HTM_CorticalLearningAlgorithms.pdf">HTM Cortical Learning Algorithms</a></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=2841&amp;context=compsci">Hinton, Geoffrey E. "Distributed representations." (1984).</a></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.ijcai.org/Proceedings/91-1/Papers/006.pdf">Plate, Tony. "Holographic Reduced Representations: Convolution Algebra for Compositional Distributed Representations." IJCAI, 1991.</a></span>
</li>
<li id="cite_note-kanerva1988-19"><span class="mw-cite-backlink">^ <a href="#cite_ref-kanerva1988_19-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-kanerva1988_19-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://mitpress.mit.edu/books/sparse-distributed-memory">Kanerva, Pentti. Sparse distributed memory. MIT press, 1988.</a></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://pdfs.semanticscholar.org/9810/4c7fabf6232715ef2bea1f5b3a3425e1c3af.pdf">Snaider, Javier, and Stan Franklin. "Integer sparse distributed memory." Twenty-fifth international flairs conference, 2012.</a></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation journal">Olshausen, Bruno A.; Field, David J. (1997). "Sparse coding with an overcomplete basis set: A strategy employed by V1?". <i>Vision Research</i>. <b>37</b> (23): 3311–3325. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2FS0042-6989%2897%2900169-7">10.1016/S0042-6989(97)00169-7</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/9425546">9425546</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Vision+Research&amp;rft.atitle=Sparse+coding+with+an+overcomplete+basis+set%3A+A+strategy+employed+by+V1%3F&amp;rft.volume=37&amp;rft.issue=23&amp;rft.pages=3311-3325&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1016%2FS0042-6989%2897%2900169-7&amp;rft_id=info%3Apmid%2F9425546&amp;rft.aulast=Olshausen&amp;rft.aufirst=Bruno+A.&amp;rft.au=Field%2C+David+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+temporal+memory" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-nupicSDRs-22"><span class="mw-cite-backlink">^ <a href="#cite_ref-nupicSDRs_22-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-nupicSDRs_22-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1601.00720">Numenta NUPIC  – Sparse Distributed representations</a></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1511.08855">Semantic Folding Theory And its Application in Semantic Fingerprinting by Francisco De Sousa Webber</a></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.2565">Tai Sing Lee, David Mumford "Hierarchical Bayesian Inference in the Visual Cortex"</a>, 2002</span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://dileepgeorge.com/blog/?p=5">Hierarchical Bayesian inference in the visual cortex</a></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://cns.bu.edu/Profiles/Grossberg/GroCisek2007.pdf">Grossberg, S. (2007). Towards a unified theory of neocortex: Laminar cortical circuits for vision and cognition. Technical Report CAS/CNS-TR-2006-008. For Computational Neuroscience: From Neurons to Theory and Back Again, eds: Paul Cisek, Trevor Drew, John Kalaska; Elsevier, Amsterdam, pp. 79–104.</a></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.sciencedirect.com/journal/artificial-intelligence/vol/169/issue/2">ScienceDirect – Artificial Intelligence, Volume 169, Issue 2, Page 103-212 (December 2005)</a></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.scholarpedia.org/article/Neocognitron">Neocognitron at Scholarpedia</a></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=28" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Official">Official</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=29" title="Edit section: Official">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=nBYddmFg4nQ">Cortical Learning Algorithm overview</a> (Accessed May 2013)</li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20111227082100/http://www.numenta.com/htm-overview/education/HTM_CorticalLearningAlgorithms.pdf">HTM Cortical Learning Algorithms</a> (PDF Sept. 2011)</li>
<li><a rel="nofollow" class="external text" href="http://www.numenta.com">Numenta, Inc.</a></li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20110714212402/http://www.numenta.com/htm-overview/education.php">HTM Cortical Learning Algorithms Archive</a></li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20121208143402/http://fora.tv/2009/09/09/Hierarchical_Temporal_Memory_Subutai_Ahmad#fullprogram">Association for Computing Machinery talk from 2009 by Subutai Ahmad from Numenta</a></li>
<li><a rel="nofollow" class="external text" href="http://www.onintelligence.org/forum">OnIntelligence.org Forum</a>, an <a href="/wiki/Internet_forum" title="Internet forum">Internet forum</a> for the discussion of relevant topics, especially relevant being the <a rel="nofollow" class="external text" href="http://www.onintelligence.org/forum/viewforum.php?f=3">Models and Simulation Topics</a> forum.</li>
<li><a rel="nofollow" class="external text" href="http://www.almaden.ibm.com/institute/resources/2006/Almaden%20Institute%20Jeff%20Hawkins.ppt">Hierarchical Temporal Memory</a> (Microsoft PowerPoint presentation)</li>
<li><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=z6r3ekreRzY">Cortical Learning Algorithm Tutorial: CLA Basics</a>, talk about the cortical learning algorithm (CLA) used by the HTM model on <a href="/wiki/YouTube" title="YouTube">YouTube</a></li></ul>
<h3><span class="mw-headline" id="Other">Other</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit&amp;section=30" title="Edit section: Other">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li><a rel="nofollow" class="external text" href="http://bias.csr.unibo.it/maltoni/HTM_TR_v1.0.pdf">Pattern Recognition by Hierarchical Temporal Memory</a> by Davide Maltoni, April 13, 2011</li>
<li><a rel="nofollow" class="external text" href="http://vicarious.com/">Vicarious</a> Startup rooted in HTM by Dileep George</li>
<li><a rel="nofollow" class="external text" href="http://www.gartner.com/research/fellows/fellows_interview_jeff_hawkins_tom_austin.jsp">The Gartner Fellows: Jeff Hawkins Interview</a> by Tom Austin, <i><a href="/wiki/Gartner" title="Gartner">Gartner</a></i>, March 2, 2006</li>
<li><a rel="nofollow" class="external text" href="http://www.cioinsight.com/article2/0,1540,1955963,00.asp">Emerging Tech: Jeff Hawkins reinvents artificial intelligence</a> by Debra D'Agostino and Edward H. Baker, <i>CIO Insight</i>, May 1, 2006</li>
<li><a rel="nofollow" class="external text" href="http://insight.zdnet.co.uk/hardware/emergingtech/0,39020439,39268542,00.htm">"Putting your brain on a microchip"</a> by Stefanie Olsen, <i><a href="/wiki/CNET" title="CNET">CNET News.com</a></i>, May 12, 2006</li>
<li><a rel="nofollow" class="external text" href="https://www.wired.com/wired/archive/15.03/hawkins.html">"The Thinking Machine"</a> by Evan Ratliff, <a href="/wiki/Wired_(magazine)" title="Wired (magazine)">Wired</a>, March 2007</li>
<li><a rel="nofollow" class="external text" href="https://spectrum.ieee.org/apr07/4982">Think like a human</a> by Jeff Hawkins, <a href="/wiki/IEEE_Spectrum" title="IEEE Spectrum">IEEE Spectrum</a>, April 2007</li>
<li><a rel="nofollow" class="external text" href="http://sourceforge.net/projects/neocortex">Neocortex – Memory-Prediction Framework</a> — <a href="/wiki/Open_Source" class="mw-redirect" title="Open Source">Open Source</a> Implementation with <a href="/wiki/GNU_General_Public_License" title="GNU General Public License">GNU General Public License</a></li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20130820231210/http://blog.mohammadzadeh.info/index.php/hierarchical-temporal-memory-related-papers">Hierarchical Temporal Memory related Papers and Books</a></li></ul>
<!-- 
NewPP limit report
Parsed by mw1268
Cached time: 20200210202401
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.420 seconds
Real time usage: 0.576 seconds
Preprocessor visited node count: 1815/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 31317/2097152 bytes
Template argument size: 3492/2097152 bytes
Highest expansion depth: 14/40
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 33159/5000000 bytes
Number of Wikibase entities loaded: 3/400
Lua time usage: 0.208/10.000 seconds
Lua memory usage: 4.69 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  503.827      1 -total
 40.86%  205.861      1 Template:Reflist
 34.28%  172.709      5 Template:Cite_journal
 30.64%  154.386      6 Template:Clarify
 28.39%  143.061      6 Template:Fix-span
 15.44%   77.779      6 Template:Replace
 13.02%   65.588      6 Template:Delink
 10.15%   51.119     12 Template:Category_handler
 10.12%   51.010      1 Template:Use_American_English
  9.71%   48.912      1 Template:Short_description
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:11273721-0!canonical and timestamp 20200210202401 and revision id 938827777
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Hierarchical_temporal_memory&amp;oldid=938827777">https://en.wikipedia.org/w/index.php?title=Hierarchical_temporal_memory&amp;oldid=938827777</a>"</div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Belief_revision" title="Category:Belief revision">Belief revision</a></li><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li><li><a href="/wiki/Category:Deep_learning" title="Category:Deep learning">Deep learning</a></li><li><a href="/wiki/Category:Unsupervised_learning" title="Category:Unsupervised learning">Unsupervised learning</a></li><li><a href="/wiki/Category:Semisupervised_learning" title="Category:Semisupervised learning">Semisupervised learning</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Use_American_English_from_February_2019" title="Category:Use American English from February 2019">Use American English from February 2019</a></li><li><a href="/wiki/Category:All_Wikipedia_articles_written_in_American_English" title="Category:All Wikipedia articles written in American English">All Wikipedia articles written in American English</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_August_2018" title="Category:Wikipedia articles needing clarification from August 2018">Wikipedia articles needing clarification from August 2018</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
    <h2>Navigation menu</h2>
    <div id="mw-head">
        
<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
	<h3 id="p-personal-label">Personal tools</h3>
	<ul >
		
		<li id="pt-anonuserpage">Not logged in</li>
		<li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Hierarchical+temporal+memory" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Hierarchical+temporal+memory" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>
	</ul>
</div>

        <div id="left-navigation">
            <div id="p-namespaces" role="navigation" class="vectorTabs " aria-labelledby="p-namespaces-label">
	<h3 id="p-namespaces-label">Namespaces</h3>
	<ul >
		<li id="ca-nstab-main" class="selected"><a href="/wiki/Hierarchical_temporal_memory" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="/wiki/Talk:Hierarchical_temporal_memory" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></li>
	</ul>
</div>
<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
	<h3 id="p-variants-label">
		<span>Variants</span>
	</h3>
	<ul class="menu" >
		
	</ul>
</div>

        </div>
        <div id="right-navigation">
            <div id="p-views" role="navigation" class="vectorTabs " aria-labelledby="p-views-label">
	<h3 id="p-views-label">Views</h3>
	<ul >
		<li id="ca-view" class="collapsible selected"><a href="/wiki/Hierarchical_temporal_memory">Read</a></li><li id="ca-edit" class="collapsible"><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li>
	</ul>
</div>
<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
	<h3 id="p-cactions-label">
		<span>More</span>
	</h3>
	<ul class="menu" >
		
	</ul>
</div>
<div id="p-search" role="search">
	<h3 >
		<label for="searchInput">Search</label>
	</h3>
	<form action="/w/index.php" id="searchform">
		<div id="simpleSearch">
			<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
			<input type="hidden" value="Special:Search" name="title"/>
			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
		</div>
	</form>
</div>

        </div>
    </div>
    <div id="mw-panel">
        <div id="p-logo" role="banner">
            <a  title="Visit the main page" class="mw-wiki-logo" href="/wiki/Main_Page"></a>
        </div>
        
<div class="portal" role="navigation" id="p-navigation"  aria-labelledby="p-navigation-label">
	<h3  id="p-navigation-label">
		Navigation
	</h3>
	<div class="body">
		<ul><li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-interaction"  aria-labelledby="p-interaction-label">
	<h3  id="p-interaction-label">
		Interaction
	</h3>
	<div class="body">
		<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-tb"  aria-labelledby="p-tb-label">
	<h3  id="p-tb-label">
		Tools
	</h3>
	<div class="body">
		<ul><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Hierarchical_temporal_memory" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Hierarchical_temporal_memory" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;oldid=938827777" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q652594" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Hierarchical_temporal_memory&amp;id=938827777" title="Information on how to cite this page">Cite this page</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-coll-print_export"  aria-labelledby="p-coll-print_export-label">
	<h3  id="p-coll-print_export-label">
		Print/export
	</h3>
	<div class="body">
		<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Hierarchical+temporal+memory">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Hierarchical+temporal+memory&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Hierarchical_temporal_memory&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-lang"  aria-labelledby="p-lang-label">
	<h3  id="p-lang-label">
		Languages
	</h3>
	<div class="body">
		<ul><li class="interlanguage-link interwiki-bg"><a href="https://bg.wikipedia.org/wiki/%D0%99%D0%B5%D1%80%D0%B0%D1%80%D1%85%D0%B8%D1%87%D0%BD%D0%B0_%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D0%B0_%D0%BF%D0%B0%D0%BC%D0%B5%D1%82" title="Йерархична временна памет – Bulgarian" lang="bg" hreflang="bg" class="interlanguage-link-target">Български</a></li><li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/Hierarchischer_Temporalspeicher" title="Hierarchischer Temporalspeicher – German" lang="de" hreflang="de" class="interlanguage-link-target">Deutsch</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/M%C3%A9moire_temporelle_et_hi%C3%A9rarchique" title="Mémoire temporelle et hiérarchique – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li><li class="interlanguage-link interwiki-pl"><a href="https://pl.wikipedia.org/wiki/Hierarchiczna_pami%C4%99%C4%87" title="Hierarchiczna pamięć – Polish" lang="pl" hreflang="pl" class="interlanguage-link-target">Polski</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/%D0%98%D0%B5%D1%80%D0%B0%D1%80%D1%85%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%BF%D0%B0%D0%BC%D1%8F%D1%82%D1%8C" title="Иерархическая временная память – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target">Русский</a></li></ul>
		<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q652594#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
	</div>
</div>

    </div>
</div>


<div id="footer" role="contentinfo" >
	<ul id="footer-info" class="">
		<li id="footer-info-lastmod"> This page was last edited on 2 February 2020, at 16:36<span class="anonymous-show">&#160;(UTC)</span>.</li>
		<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
	</ul>
	<ul id="footer-places" class="">
		<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
		<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
		<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
		<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/v2/#/en.wikipedia.org">Statistics</a></li>
		<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
		<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Hierarchical_temporal_memory&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a></li>
		<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a></li>
	</ul>
	<div style="clear: both;"></div>
</div>


<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.420","walltime":"0.576","ppvisitednodes":{"value":1815,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":31317,"limit":2097152},"templateargumentsize":{"value":3492,"limit":2097152},"expansiondepth":{"value":14,"limit":40},"expensivefunctioncount":{"value":5,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":33159,"limit":5000000},"entityaccesscount":{"value":3,"limit":400},"timingprofile":["100.00%  503.827      1 -total"," 40.86%  205.861      1 Template:Reflist"," 34.28%  172.709      5 Template:Cite_journal"," 30.64%  154.386      6 Template:Clarify"," 28.39%  143.061      6 Template:Fix-span"," 15.44%   77.779      6 Template:Replace"," 13.02%   65.588      6 Template:Delink"," 10.15%   51.119     12 Template:Category_handler"," 10.12%   51.010      1 Template:Use_American_English","  9.71%   48.912      1 Template:Short_description"]},"scribunto":{"limitreport-timeusage":{"value":"0.208","limit":"10.000"},"limitreport-memusage":{"value":4912947,"limit":52428800}},"cachereport":{"origin":"mw1268","timestamp":"20200210202401","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Hierarchical temporal memory","url":"https:\/\/en.wikipedia.org\/wiki\/Hierarchical_temporal_memory","sameAs":"http:\/\/www.wikidata.org\/entity\/Q652594","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q652594","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2007-05-17T15:00:49Z","dateModified":"2020-02-02T16:36:15Z","headline":"biological theory of intelligence"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":117,"wgHostname":"mw1247"});});</script></body></html>
