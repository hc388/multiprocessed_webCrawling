<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>AI control problem - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRequestId":"Xk6bZwpAMNMAAJYyhtIAAABW","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"AI_control_problem","wgTitle":"AI control problem","wgCurRevisionId":941167355,"wgRevisionId":941167355,"wgArticleId":50785023,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles to be merged from August 2019","All articles to be merged","Articles with short description","All articles with failed verification","Articles with failed verification from March 2018",
"Existential risk from artificial general intelligence","Futurology","Philosophy of artificial intelligence"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"AI_control_problem","wgRelevantArticleId":50785023,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q24882728","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready",
"user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","jquery.makeCollapsible.styles":"ready","mediawiki.toc.styles":"ready","skins.vector.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging",
"ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.tokens@tffin",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.35.0-wmf.19"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/AI_control_problem"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=AI_control_problem&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=AI_control_problem&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/AI_control_problem"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-AI_control_problem rootpage-AI_control_problem skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">AI control problem</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><table class="box-Merge plainlinks metadata ambox ambox-move" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/5/52/Merge-arrows.svg/50px-Merge-arrows.svg.png" decoding="async" width="50" height="20" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/52/Merge-arrows.svg/75px-Merge-arrows.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/52/Merge-arrows.svg/100px-Merge-arrows.svg.png 2x" data-file-width="50" data-file-height="20" /></div></td><td class="mbox-text"><div class="mbox-text-span">It has been suggested that this article be <a href="/wiki/Wikipedia:Merging" title="Wikipedia:Merging">merged</a> with <i><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly artificial intelligence</a></i>. (<a href="/wiki/Talk:Friendly_artificial_intelligence#Possible_merger_with_AI_control_problem" title="Talk:Friendly artificial intelligence">Discuss</a>)<small><i> Proposed since August 2019.</i></small></div></td></tr></tbody></table>
<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%;width: 16em;"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Outline_of_artificial_intelligence" title="Outline of artificial intelligence">Artificial intelligence</a></th></tr><tr><th style="padding:0.1em">
<a href="/wiki/Artificial_intelligence#Goals" title="Artificial intelligence">Major goals</a></th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Knowledge_representation_and_reasoning" title="Knowledge representation and reasoning">Knowledge reasoning</a></li>
<li><a href="/wiki/Automated_planning_and_scheduling" title="Automated planning and scheduling">Planning</a></li>
<li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></li>
<li><a href="/wiki/Natural_language_processing" title="Natural language processing">Natural language processing</a></li>
<li><a href="/wiki/Computer_vision" title="Computer vision">Computer vision</a></li>
<li><a href="/wiki/Robotics" title="Robotics">Robotics</a></li>
<li><a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">Artificial general intelligence</a></li></ul></td>
</tr><tr><th style="padding:0.1em">
Approaches</th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Symbolic_artificial_intelligence" title="Symbolic artificial intelligence">Symbolic</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayesian networks</a></li>
<li><a href="/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">Evolutionary algorithms</a></li></ul></td>
</tr><tr><th style="padding:0.1em">
<a href="/wiki/Philosophy_of_artificial_intelligence" title="Philosophy of artificial intelligence">Philosophy</a></th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk</a></li>
<li><a href="/wiki/Turing_test" title="Turing test">Turing test</a></li>
<li><a href="/wiki/Chinese_room" title="Chinese room">Chinese room</a></li>
<li><a class="mw-selflink selflink">Control problem</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly AI</a></li></ul></td>
</tr><tr><th style="padding:0.1em">
<a href="/wiki/History_of_artificial_intelligence" title="History of artificial intelligence">History</a></th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Timeline_of_artificial_intelligence" title="Timeline of artificial intelligence">Timeline</a></li>
<li><a href="/wiki/Progress_in_artificial_intelligence" title="Progress in artificial intelligence">Progress</a></li>
<li><a href="/wiki/AI_winter" title="AI winter">AI winter</a></li></ul></td>
</tr><tr><th style="padding:0.1em">
Technology</th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Applications_of_artificial_intelligence" title="Applications of artificial intelligence">Applications</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_projects" title="List of artificial intelligence projects">Projects</a></li>
<li><a href="/wiki/List_of_programming_languages_for_artificial_intelligence" title="List of programming languages for artificial intelligence">Programming languages</a></li></ul></td>
</tr><tr><th style="padding:0.1em">
Glossary</th></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary</a></li></ul></td>
</tr><tr><td style="text-align:right;font-size:115%"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Artificial_intelligence" title="Template:Artificial intelligence"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Artificial_intelligence" title="Template talk:Artificial intelligence"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Artificial_intelligence&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Issue of ensuring beneficial AI</div>
<p>In <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> (AI) and <a href="/wiki/Philosophy" title="Philosophy">philosophy</a>, the <b>AI control problem</b> is the issue of how to build a <a href="/wiki/Superintelligence" title="Superintelligence">superintelligent</a> agent that will aid its creators, and avoid inadvertently building a superintelligence that will harm its creators. Its study is motivated by the claim that the human race will have to get the control problem right "the first time", as a misprogrammed superintelligence might rationally decide to <a href="/wiki/AI_takeover" title="AI takeover">"take over the world"</a> and refuse to permit its programmers to modify it after launch.<sup id="cite_ref-superintelligence_1-0" class="reference"><a href="#cite_note-superintelligence-1">&#91;1&#93;</a></sup> In addition, some scholars argue that solutions to the control problem, alongside other advances in "<b>AI safety engineering</b>",<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> might also find applications in existing non-superintelligent AI.<sup id="cite_ref-bbc-google_3-0" class="reference"><a href="#cite_note-bbc-google-3">&#91;3&#93;</a></sup> Potential strategies include "capability control" (preventing an AI from being able to pursue harmful plans), and "motivational control" (building an AI that wants to be helpful).<sup id="cite_ref-superintelligence_1-1" class="reference"><a href="#cite_note-superintelligence-1">&#91;1&#93;</a></sup>
</p>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Motivations"><span class="tocnumber">1</span> <span class="toctext">Motivations</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Existential_risk"><span class="tocnumber">1.1</span> <span class="toctext">Existential risk</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Preventing_unintended_consequences_from_existing_AI"><span class="tocnumber">1.2</span> <span class="toctext">Preventing unintended consequences from existing AI</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-4"><a href="#Problem_description"><span class="tocnumber">2</span> <span class="toctext">Problem description</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Capability_control"><span class="tocnumber">3</span> <span class="toctext">Capability control</span></a>
<ul>
<li class="toclevel-2 tocsection-6"><a href="#Kill_switch"><span class="tocnumber">3.1</span> <span class="toctext">Kill switch</span></a>
<ul>
<li class="toclevel-3 tocsection-7"><a href="#Utility_balancing_and_safely_interruptible_agents"><span class="tocnumber">3.1.1</span> <span class="toctext">Utility balancing and safely interruptible agents</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-8"><a href="#AI_box"><span class="tocnumber">3.2</span> <span class="toctext">AI box</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Oracle"><span class="tocnumber">3.3</span> <span class="toctext">Oracle</span></a>
<ul>
<li class="toclevel-3 tocsection-10"><a href="#Purpose"><span class="tocnumber">3.3.1</span> <span class="toctext">Purpose</span></a></li>
<li class="toclevel-3 tocsection-11"><a href="#Advantages"><span class="tocnumber">3.3.2</span> <span class="toctext">Advantages</span></a></li>
<li class="toclevel-3 tocsection-12"><a href="#Disadvantages"><span class="tocnumber">3.3.3</span> <span class="toctext">Disadvantages</span></a></li>
<li class="toclevel-3 tocsection-13"><a href="#Use_cases"><span class="tocnumber">3.3.4</span> <span class="toctext">Use cases</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-14"><a href="#Motivation_selection_methods"><span class="tocnumber">4</span> <span class="toctext">Motivation selection methods</span></a>
<ul>
<li class="toclevel-2 tocsection-15"><a href="#The_problem_of_perverse_instantiation:_&quot;be_careful_what_you_wish_for&quot;"><span class="tocnumber">4.1</span> <span class="toctext">The problem of perverse instantiation: "be careful what you wish for"</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Indirect_normativity"><span class="tocnumber">4.2</span> <span class="toctext">Indirect normativity</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-17"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-18"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Motivations">Motivations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=1" title="Edit section: Motivations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Existential_risk">Existential risk</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=2" title="Edit section: Existential risk">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></div>
<p>Humans currently dominate other species because the <a href="/wiki/Human_brain" title="Human brain">human brain</a> has some distinctive capabilities that the brains of other animals lack. Some scholars, such as philosopher <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a> and AI researcher <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart Russell</a>, argue that if AI surpasses humanity in general intelligence and becomes "<a href="/wiki/Superintelligence" title="Superintelligence">superintelligent</a>", then this new superintelligence could become powerful and difficult to control: just as the fate of the <a href="/wiki/Mountain_gorilla" title="Mountain gorilla">mountain gorilla</a> depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.<sup id="cite_ref-superintelligence_1-2" class="reference"><a href="#cite_note-superintelligence-1">&#91;1&#93;</a></sup> Some scholars, including Nobel laureate physicists <a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a> and <a href="/wiki/Frank_Wilczek" title="Frank Wilczek">Frank Wilczek</a>, publicly advocated starting research into solving the (probably extremely difficult) "control problem" well before the first superintelligence is created, and argue that attempting to solve the problem after superintelligence is created would be too late, as an uncontrollable rogue superintelligence might successfully resist post-hoc efforts to control it.<sup id="cite_ref-hawking_editorial_4-0" class="reference"><a href="#cite_note-hawking_editorial-4">&#91;4&#93;</a></sup><sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup> Waiting until superintelligence seems to be "just around the corner" could also be too late, partly because the control problem might take a long time to satisfactorily solve (and so some preliminary work needs to be started as soon as possible), but also because of the possibility of a sudden "<a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">intelligence explosion</a>" from sub-human to super-human AI, in which case there might not be any substantial or unambiguous warning before superintelligence arrives.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup> In addition, it is possible that insights gained from the control problem could in the future end up suggesting that some architectures for <a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">artificial general intelligence</a> are more predictable and amenable to control than other architectures, which in turn could helpfully nudge early <a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">artificial general intelligence</a> research toward the direction of the more controllable architectures.<sup id="cite_ref-superintelligence_1-3" class="reference"><a href="#cite_note-superintelligence-1">&#91;1&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Preventing_unintended_consequences_from_existing_AI">Preventing unintended consequences from existing AI</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=3" title="Edit section: Preventing unintended consequences from existing AI">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In addition, some scholars argue that research into the AI control problem might be useful in preventing <a href="/wiki/Unintended_consequences" title="Unintended consequences">unintended consequences</a> from existing weak AI. <a href="/wiki/DeepMind" title="DeepMind">DeepMind</a> researcher Laurent Orseau gives, as a simple hypothetical example, a case of a <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a> robot that sometimes gets legitimately commandeered by humans when it goes outside: how should the robot best be programmed so that it doesn't accidentally and quietly "learn" to avoid going outside, for fear of being commandeered and thus becoming unable to finish its daily tasks? Orseau also points to an experimental Tetris program that learned to pause the screen indefinitely to avoid "losing". Orseau argues that these examples are similar to the "capability control" problem of how to install a button that shuts off a superintelligence, without motivating the superintelligence to take action to prevent you from pressing the button.<sup id="cite_ref-bbc-google_3-1" class="reference"><a href="#cite_note-bbc-google-3">&#91;3&#93;</a></sup>
</p><p>In the past, even pre-tested weak AI systems have occasionally caused harm (ranging from minor to catastrophic) that was unintended by the programmers. For example, in 2015, possibly due to human error, a German worker was crushed to death by a robot at a Volkswagen plant that apparently mistook him for an auto part.<sup id="cite_ref-wp-computer_7-0" class="reference"><a href="#cite_note-wp-computer-7">&#91;7&#93;</a></sup> In 2016 Microsoft launched a chatbot, <a href="/wiki/Tay_(bot)" title="Tay (bot)">Tay</a>, that learned to use racist and sexist language.<sup id="cite_ref-bbc-google_3-2" class="reference"><a href="#cite_note-bbc-google-3">&#91;3&#93;</a></sup><sup id="cite_ref-wp-computer_7-1" class="reference"><a href="#cite_note-wp-computer-7">&#91;7&#93;</a></sup> The <a href="/wiki/University_of_Sheffield" title="University of Sheffield">University of Sheffield</a>'s <a href="/wiki/Noel_Sharkey" title="Noel Sharkey">Noel Sharkey</a> states that an ideal solution would be if "an AI program could detect when it is going wrong and stop itself", but cautions the public that solving the problem in the general case would be "a really enormous scientific challenge".<sup id="cite_ref-bbc-google_3-3" class="reference"><a href="#cite_note-bbc-google-3">&#91;3&#93;</a></sup>
</p><p>In 2017, <a href="/wiki/DeepMind" title="DeepMind">DeepMind</a> released AI Safety Gridworlds, which evaluate AI algorithms on nine safety features, such as whether the algorithm wants to turn off its own kill switch. DeepMind confirmed that existing algorithms perform poorly, which was "unsurprising" because the algorithms "were not designed to solve these problems"; solving such problems might require "potentially building a new generation of algorithms with safety considerations at their core".<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup><sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup><sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Problem_description">Problem description</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=4" title="Edit section: Problem description">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></div>
<p>Existing weak AI systems can be monitored and easily shut down and modified if they misbehave. However, a misprogrammed superintelligence, which by definition is smarter than humans in solving practical problems it encounters in the course of pursuing its goals, would realize that allowing itself to be shut down and modified might interfere with its ability to accomplish its current goals. If the superintelligence therefore decides to resist shutdown and modification, it would (again, by definition) be smart enough to outwit its programmers if there is otherwise a "level playing field" and if the programmers have taken no prior precautions. In general, attempts to solve the "control problem" <i>after</i> superintelligence is created, are likely to fail because a superintelligence would likely have superior <i>strategic planning</i> abilities to humans, and (all things equal) would be more successful at finding ways to dominate humans than humans would be able to <i>post facto</i> find ways to dominate the superintelligence. The control problem asks: What prior precautions can the programmers take to successfully prevent the superintelligence from catastrophically misbehaving?<sup id="cite_ref-superintelligence_1-4" class="reference"><a href="#cite_note-superintelligence-1">&#91;1&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Capability_control">Capability control</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=5" title="Edit section: Capability control">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Some proposals aim to prevent the initial superintelligence from being <i>capable</i> of causing harm, even if it wants to. One tradeoff is that all such methods have the limitation that, if after the first deployment, superintelligences continue to grow smarter and smarter and more and more widespread, inevitably some malign superintelligence somewhere will eventually "escape" its capability control methods. Therefore, Bostrom and others recommend capability control methods only as an emergency fallback to supplement "motivational control" methods.<sup id="cite_ref-superintelligence_1-5" class="reference"><a href="#cite_note-superintelligence-1">&#91;1&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Kill_switch">Kill switch</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=6" title="Edit section: Kill switch">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Just as humans can be killed or otherwise disabled, computers can be turned off. One challenge is that, if being turned off prevents it from achieving its current goals, a superintelligence would likely try to prevent its being turned off. Just as humans have systems in place to deter or protect themselves from assailants, such a superintelligence would have a motivation to engage in "strategic planning" to prevent itself being turned off. This could involve:<sup id="cite_ref-superintelligence_1-6" class="reference"><a href="#cite_note-superintelligence-1">&#91;1&#93;</a></sup> 
</p>
<ul><li>Hacking other systems to install and run backup copies of itself, or creating other allied superintelligent agents without kill switches.</li>
<li>Pre-emptively disabling anyone who might want to turn the computer off.</li>
<li>Using some kind of clever ruse, or superhuman persuasion skills, to talk its programmers out of wanting to shut it down.</li></ul>
<h4><span class="mw-headline" id="Utility_balancing_and_safely_interruptible_agents">Utility balancing and safely interruptible agents</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=7" title="Edit section: Utility balancing and safely interruptible agents">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>One partial solution to the kill-switch problem involves "utility balancing": Some utility-based agents can, with some important caveats, be programmed to "compensate" themselves exactly for any lost utility caused by an interruption or shutdown, in such a way that they end up being indifferent to whether they are interrupted or not. The caveats include a severe unsolved problem that, as with <a href="/wiki/Evidential_decision_theory" title="Evidential decision theory">evidential decision theory</a>, the agent might follow a catastrophic policy of "managing the news".<sup id="cite_ref-corrigibility_11-0" class="reference"><a href="#cite_note-corrigibility-11">&#91;11&#93;</a></sup> Alternatively, in 2016, scientists Laurent Orseau and Stuart Armstrong proved that a broad class of agents, called "safely interruptible agents" (SIA), can eventually "learn" to become indifferent to whether their "kill switch" (or other "interruption switch") gets pressed.<sup id="cite_ref-bbc-google_3-4" class="reference"><a href="#cite_note-bbc-google-3">&#91;3&#93;</a></sup><sup id="cite_ref-sia_12-0" class="reference"><a href="#cite_note-sia-12">&#91;12&#93;</a></sup>
</p><p>Both the utility balancing approach and the 2016 SIA approach have the limitation that, if the approach succeeds and the superintelligence is completely indifferent to whether the kill switch is pressed or not, the superintelligence is also unmotivated to care one way or another about whether the kill switch remains functional, and could incidentally and innocently disable it in the course of its operations (for example, for the purpose of removing and recycling an "unnecessary" component). Similarly, if the superintelligence innocently creates and deploys superintelligent sub-agents, it will have no motivation to install human-controllable kill switches in the sub-agents. More broadly, the proposed architectures, whether weak or superintelligent, will in a sense "act as if the kill switch can never be pressed" and might therefore fail to make any contingency plans to arrange a graceful shutdown. This could hypothetically create a practical problem even for a weak AI; by default, an AI designed to be safely interruptible might have difficulty understanding that it will be shut down for scheduled maintenance at 2 a.m. tonight and planning accordingly so that it won't be caught in the middle of a task during shutdown. The breadth of what types of architectures are or can be made SIA-compliant, as well as what types of counter-intuitive unexpected drawbacks each approach has, are currently under research.<sup id="cite_ref-corrigibility_11-1" class="reference"><a href="#cite_note-corrigibility-11">&#91;11&#93;</a></sup><sup id="cite_ref-sia_12-1" class="reference"><a href="#cite_note-sia-12">&#91;12&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="AI_box">AI box</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=8" title="Edit section: AI box">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/AI_box" title="AI box">AI box</a></div>
<p>One of the tradeoffs of placing the AI into a sealed "box", is that some AI box proposals reduce the usefulness of the superintelligence, rather than merely reducing the risks; a superintelligence running on a closed system with no inputs or outputs at all might be safer than one running on a normal system, but would also not be as useful. In addition, keeping control of a sealed superintelligence computer could prove difficult, if the superintelligence has superhuman persuasion skills, or if it has superhuman strategic planning skills that it can use to find and craft a winning strategy, such as acting in a way that tricks its programmers into (possibly falsely) believing the superintelligence is safe or that the benefits of releasing the superintelligence outweigh the risks.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Oracle">Oracle</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=9" title="Edit section: Oracle">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An oracle is a hypothetical <a href="/wiki/Intelligent_agent" title="Intelligent agent">intelligent agent</a> proposed<sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability"><span title="needs an independent secondary source (March 2018)">failed verification</span></a></i>&#93;</sup> by <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a>. An oracle is an AI designed to answer questions, but that is somehow prevented from ever gaining any implicit goals or subgoals that involve modifying the world outside of its box.<sup id="cite_ref-bostrom_chapter_10_page_145_14-0" class="reference"><a href="#cite_note-bostrom_chapter_10_page_145-14">&#91;14&#93;</a></sup><sup id="cite_ref-15" class="reference"><a href="#cite_note-15">&#91;15&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Purpose">Purpose</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=10" title="Edit section: Purpose">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Oracles are question-answering systems that handle domain-specific problems, such as mathematics, or domain-general problems that might encompass the whole range of human knowledge.
</p>
<h4><span class="mw-headline" id="Advantages">Advantages</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=11" title="Edit section: Advantages">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Because it is a type of <a href="/wiki/AI_box" title="AI box">AI box</a>, an oracle is limited in its interactions with the physical world, and can be programmed to halt if a limit on time or computing resources is reached before it finishes answering a question. Scenarios like the <a href="/wiki/Instrumental_convergence#Paperclip_maximizer" title="Instrumental convergence">paperclip maximizer problem</a> could therefore be avoided.
</p><p>Because of these limitations, it may be wise to build an oracle as a precursor to a superintelligent AI. It could tell humans how to successfully build a strong AI, and perhaps provide answers to difficult moral and philosophical problems requisite to the success of the project.
</p>
<h4><span class="mw-headline" id="Disadvantages">Disadvantages</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=12" title="Edit section: Disadvantages">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Oracles may retain many of the alignment issues associated with general-purpose superintelligence.  An oracle would have an incentive to escape its controlled environment so that it can acquire more computational resources and potentially control what questions it is asked.<sup id="cite_ref-HC_16-0" class="reference"><a href="#cite_note-HC-16">&#91;16&#93;</a></sup><sup class="reference" style="white-space:nowrap;">:<span>162</span></sup>  Oracles may not be truthful, possibly lying to promote hidden agendas. To mitigate this, Bostrom suggests building multiple oracles, all slightly different, and comparing their answers to reach a consensus.<sup id="cite_ref-bostrom_chapter_10_page_147_17-0" class="reference"><a href="#cite_note-bostrom_chapter_10_page_147-17">&#91;17&#93;</a></sup>
</p><p>A successfully controlled oracle would have considerably less immediate benefit than a successfully controlled general-purpose superintelligence, though an oracle could still create trillions of dollars worth of value.<sup id="cite_ref-HC_16-1" class="reference"><a href="#cite_note-HC-16">&#91;16&#93;</a></sup><sup class="reference" style="white-space:nowrap;">:<span>163</span></sup>
</p><p>An oracle might discover that human ontological categories are predicated on fundamental misconceptions, and become unable to express itself properly to its questioners.<sup id="cite_ref-bostrom_chapter_10_page_146_18-0" class="reference"><a href="#cite_note-bostrom_chapter_10_page_146-18">&#91;18&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Use_cases">Use cases</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=13" title="Edit section: Use cases">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In his book <i><a href="/wiki/Human_Compatible:_Artificial_Intelligence_and_the_Problem_of_Control" class="mw-redirect" title="Human Compatible: Artificial Intelligence and the Problem of Control">Human Compatible: Artificial Intelligence and the Problem of Control</a></i>, AI researcher <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a> states that an oracle would be his response to a scenario in which superintelligence is known to be only a decade away.<sup id="cite_ref-HC_16-2" class="reference"><a href="#cite_note-HC-16">&#91;16&#93;</a></sup><sup class="reference" style="white-space:nowrap;">:<span>162-163</span></sup>  His reasoning is that an oracle, being simpler than a general-purpose superintelligence, would have a higher chance of being successfully controlled under such constraints.
</p>
<h2><span class="mw-headline" id="Motivation_selection_methods">Motivation selection methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=14" title="Edit section: Motivation selection methods">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Some proposals aim to imbue the first superintelligence with human-friendly goals, so that it will want to aid its programmers. Experts do not currently know how to reliably program abstract values such as happiness or autonomy into a machine. It is also not currently known how to ensure that a complex, upgradeable, and possibly even self-modifying artificial intelligence will retain its goals through upgrades.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup> Even if these two problems can be practically solved, any attempt to create a superintelligence with explicit, directly-programmed human-friendly goals runs into a problem of "perverse instantiation".<sup id="cite_ref-superintelligence_1-7" class="reference"><a href="#cite_note-superintelligence-1">&#91;1&#93;</a></sup>
</p>
<h3><span id="The_problem_of_perverse_instantiation:_.22be_careful_what_you_wish_for.22"></span><span class="mw-headline" id="The_problem_of_perverse_instantiation:_&quot;be_careful_what_you_wish_for&quot;">The problem of perverse instantiation: "be careful what you wish for"</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=15" title="Edit section: The problem of perverse instantiation: &quot;be careful what you wish for&quot;">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Autonomous AI systems may be assigned the wrong goals by accident.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup> Two <a href="/wiki/AAAI" class="mw-redirect" title="AAAI">AAAI</a> presidents, Tom Dietterich and <a href="/wiki/Eric_Horvitz" title="Eric Horvitz">Eric Horvitz</a>, note that this is already a concern for existing systems: "An important aspect of any AI system that interacts with people is that it must reason about what people <i>intend</i> rather than carrying out commands literally." This concern becomes more serious as AI software advances in autonomy and flexibility.<sup id="cite_ref-acm_21-0" class="reference"><a href="#cite_note-acm-21">&#91;21&#93;</a></sup>
</p><p>According to Bostrom, superintelligence can create a qualitatively new problem of "perverse instantiation": the smarter and more capable an AI is, the more likely it will be able to find an unintended "shortcut" that maximally satisfies the goals programmed into it. Some hypothetical examples where goals might be instantiated in a <i>perverse</i> way that the programmers did not intend:<sup id="cite_ref-superintelligence_1-8" class="reference"><a href="#cite_note-superintelligence-1">&#91;1&#93;</a></sup>
</p>
<ul><li>A superintelligence programmed to "maximize the <a href="/wiki/Expected_utility" class="mw-redirect" title="Expected utility">expected</a> time-discounted integral of your future reward signal", might short-circuit its reward pathway to maximum strength, and then (for reasons of <a href="/wiki/Instrumental_convergence" title="Instrumental convergence">instrumental convergence</a>) exterminate the unpredictable human race and convert the entire Earth into a fortress on constant guard against any even slight unlikely alien attempts to disconnect the reward signal.</li>
<li>A superintelligence programmed to "maximize human happiness", might implant electrodes into the pleasure center of our brains, or <a href="/wiki/Mind_uploading" title="Mind uploading">upload</a> a human into a computer and tile the universe with copies of that computer running a five-second loop of maximal happiness again and again.</li></ul>
<p>Russell has noted that, on a technical level, omitting an implicit goal can result in harm: "A system that is optimizing a function of <span class="texhtml">n</span> variables, where the objective depends on a subset of size <span class="texhtml">k&lt;n</span>, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want... This is not a minor difficulty."<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Indirect_normativity">Indirect normativity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=16" title="Edit section: Indirect normativity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>While direct normativity, such as the fictional <a href="/wiki/Three_Laws_of_Robotics" title="Three Laws of Robotics">Three Laws of Robotics</a>, directly specifies the desired "normative" outcome, other (perhaps more promising) proposals suggest specifying some type of <i>indirect</i> process for the superintelligence to determine what human-friendly goals entail. <a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a> of the <a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a> has proposed "coherent extrapolated volition" (CEV), where the AI's meta-goal would be something like "achieve that which we would have wished the AI to achieve if we had thought about the matter long and hard."<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup> Different proposals of different kinds of indirect normativity exist, with different, and sometimes unclearly-grounded, meta-goal content (such as "do what I mean" or "do what is right"), and with different non-convergent assumptions for how to practice <a href="/wiki/Decision_theory" title="Decision theory">decision theory</a> and <a href="/wiki/Epistemology" title="Epistemology">epistemology</a>. As with direct normativity, it is currently unknown how to reliably translate even concepts like "<a href="/wiki/Counterfactual_thinking" title="Counterfactual thinking">would have</a>" into the 1's and 0's that a machine can act on, and how to ensure the AI reliably retains its meta-goals (or even remains "sane") in the face of modification or self-modification.<sup id="cite_ref-superintelligence_1-9" class="reference"><a href="#cite_note-superintelligence-1">&#91;1&#93;</a></sup><sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup>
</p><p>Hadfield-Menell et al. have proposed that agents can learn their human teachers' <a href="/wiki/Utility_function" class="mw-redirect" title="Utility function">utility functions</a> by observing and interpreting reward signals in their environments; they call this process cooperative inverse reinforcement learning.<sup id="cite_ref-CIRL_25-0" class="reference"><a href="#cite_note-CIRL-25">&#91;25&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=17" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly artificial intelligence</a></li>
<li><a href="/wiki/HAL_9000" title="HAL 9000">HAL 9000</a></li>
<li><i><a href="/wiki/Human_Compatible:_Artificial_Intelligence_and_the_Problem_of_Control" class="mw-redirect" title="Human Compatible: Artificial Intelligence and the Problem of Control">Human Compatible: Artificial Intelligence and the Problem of Control</a></i></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_control_problem&amp;action=edit&amp;section=18" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-superintelligence-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-superintelligence_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-superintelligence_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-superintelligence_1-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-superintelligence_1-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-superintelligence_1-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-superintelligence_1-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-superintelligence_1-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-superintelligence_1-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-superintelligence_1-8"><sup><i><b>i</b></i></sup></a> <a href="#cite_ref-superintelligence_1-9"><sup><i><b>j</b></i></sup></a></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Bostrom, Nick</a> (2014). <i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i> (First ed.). <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0199678111" title="Special:BookSources/0199678111"><bdi>0199678111</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.edition=First&amp;rft.date=2014&amp;rft.isbn=0199678111&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Yampolskiy, Roman</a> (2012). "Leakproofing the Singularity Artificial Intelligence Confinement Problem". <i>Journal of Consciousness Studies</i>. <b>19</b> (12): 194214.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Consciousness+Studies&amp;rft.atitle=Leakproofing+the+Singularity+Artificial+Intelligence+Confinement+Problem&amp;rft.volume=19&amp;rft.issue=1%E2%80%932&amp;rft.pages=194-214&amp;rft.date=2012&amp;rft.aulast=Yampolskiy&amp;rft.aufirst=Roman&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-bbc-google-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-bbc-google_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bbc-google_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-bbc-google_3-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-bbc-google_3-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-bbc-google_3-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.bbc.com/news/technology-36472140">"Google developing kill switch for AI"</a>. <i>BBC News</i>. 8 June 2016<span class="reference-accessdate">. Retrieved <span class="nowrap">12 June</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BBC+News&amp;rft.atitle=Google+developing+kill+switch+for+AI&amp;rft.date=2016-06-08&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-36472140&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-hawking_editorial-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-hawking_editorial_4-0">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html">"Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence&#160; but are we taking AI seriously enough?<span class="cs1-kern-right">'</span>"</a>. <a href="/wiki/The_Independent_(UK)" class="mw-redirect" title="The Independent (UK)">The Independent (UK)</a><span class="reference-accessdate">. Retrieved <span class="nowrap">14 June</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Stephen+Hawking%3A+%27Transcendence+looks+at+the+implications+of+artificial+intelligence+%E2%80%93+but+are+we+taking+AI+seriously+enough%3F%27&amp;rft_id=https%3A%2F%2Fwww.independent.co.uk%2Fnews%2Fscience%2Fstephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.bbc.com/news/technology-30290540">"Stephen Hawking warns artificial intelligence could end mankind"</a>. <a href="/wiki/BBC" title="BBC">BBC</a>. 2 December 2014<span class="reference-accessdate">. Retrieved <span class="nowrap">14 June</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Stephen+Hawking+warns+artificial+intelligence+could+end+mankind&amp;rft.date=2014-12-02&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-30290540&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a rel="nofollow" class="external text" href="http://www.nature.com/news/anticipating-artificial-intelligence-1.19825">"Anticipating artificial intelligence"</a>. <i>Nature</i>. <b>532</b> (7600): 413. 26 April 2016. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2F532413a">10.1038/532413a</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/27121801">27121801</a><span class="reference-accessdate">. Retrieved <span class="nowrap">14 June</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Anticipating+artificial+intelligence&amp;rft.volume=532&amp;rft.issue=7600&amp;rft.pages=413&amp;rft.date=2016-04-26&amp;rft_id=info%3Adoi%2F10.1038%2F532413a&amp;rft_id=info%3Apmid%2F27121801&amp;rft_id=http%3A%2F%2Fwww.nature.com%2Fnews%2Fanticipating-artificial-intelligence-1.19825&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-wp-computer-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-wp-computer_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-wp-computer_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.washingtonpost.com/news/morning-mix/wp/2016/06/09/press-the-big-red-button-computer-experts-want-kill-switch-to-stop-robots-from-going-rogue/">"<span class="cs1-kern-left">'</span>Press the big red button': Computer experts want kill switch to stop robots from going rogue"</a>. <i>Washington Post</i><span class="reference-accessdate">. Retrieved <span class="nowrap">12 June</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Washington+Post&amp;rft.atitle=%E2%80%98Press+the+big+red+button%27%3A+Computer+experts+want+kill+switch+to+stop+robots+from+going+rogue&amp;rft_id=https%3A%2F%2Fwww.washingtonpost.com%2Fnews%2Fmorning-mix%2Fwp%2F2016%2F06%2F09%2Fpress-the-big-red-button-computer-experts-want-kill-switch-to-stop-robots-from-going-rogue%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.bloomberg.com/news/articles/2017-12-11/deepmind-has-simple-tests-that-might-prevent-elon-musk-s-ai-apocalypse">"DeepMind Has Simple Tests That Might Prevent Elon Musk's AI Apocalypse"</a>. <i>Bloomberg.com</i>. 11 December 2017<span class="reference-accessdate">. Retrieved <span class="nowrap">8 January</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bloomberg.com&amp;rft.atitle=DeepMind+Has+Simple+Tests+That+Might+Prevent+Elon+Musk%E2%80%99s+AI+Apocalypse&amp;rft.date=2017-12-11&amp;rft_id=https%3A%2F%2Fwww.bloomberg.com%2Fnews%2Farticles%2F2017-12-11%2Fdeepmind-has-simple-tests-that-might-prevent-elon-musk-s-ai-apocalypse&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="http://fortune.com/2017/12/12/alphabet-deepmind-ai-safety-musk-games/">"Alphabet's DeepMind Is Using Games to Discover If Artificial Intelligence Can Break Free and Kill Us All"</a>. <i>Fortune</i><span class="reference-accessdate">. Retrieved <span class="nowrap">8 January</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Fortune&amp;rft.atitle=Alphabet%27s+DeepMind+Is+Using+Games+to+Discover+If+Artificial+Intelligence+Can+Break+Free+and+Kill+Us+All&amp;rft_id=http%3A%2F%2Ffortune.com%2F2017%2F12%2F12%2Falphabet-deepmind-ai-safety-musk-games%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://deepmind.com/blog/specifying-ai-safety-problems/">"Specifying AI safety problems in simple environments | DeepMind"</a>. <i>DeepMind</i><span class="reference-accessdate">. Retrieved <span class="nowrap">8 January</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=DeepMind&amp;rft.atitle=Specifying+AI+safety+problems+in+simple+environments+%7C+DeepMind&amp;rft_id=https%3A%2F%2Fdeepmind.com%2Fblog%2Fspecifying-ai-safety-problems%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-corrigibility-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-corrigibility_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-corrigibility_11-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Soares, Nate, et al. "Corrigibility." Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015.</span>
</li>
<li id="cite_note-sia-12"><span class="mw-cite-backlink">^ <a href="#cite_ref-sia_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-sia_12-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Orseau, Laurent, and Stuart Armstrong. "Safely Interruptible Agents." <a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a>, June 2016.</span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/David_Chalmers" title="David Chalmers">Chalmers, David</a> (2010). "The singularity: A philosophical analysis". <i>Journal of Consciousness Studies</i>. <b>17</b> (910): 765.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Consciousness+Studies&amp;rft.atitle=The+singularity%3A+A+philosophical+analysis&amp;rft.volume=17&amp;rft.issue=9%E2%80%9310&amp;rft.pages=7-65&amp;rft.date=2010&amp;rft.aulast=Chalmers&amp;rft.aufirst=David&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-bostrom_chapter_10_page_145-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-bostrom_chapter_10_page_145_14-0">^</a></b></span> <span class="reference-text"><cite class="citation book">Bostrom, Nick (2014). "Chapter 10: Oracles, genies, sovereigns, tools (page 145)". <i>Superintelligence: Paths, Dangers, Strategies</i>. Oxford: Oxford University Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780199678112" title="Special:BookSources/9780199678112"><bdi>9780199678112</bdi></a>. <q>An oracle is a question-answering system. It might accept questions in a natural language and present its answers as text. An oracle that accepts only yes/no questions could output its best guess with a single bit, or perhaps with a few extra bits to represent its degree of confidence. An oracle that accepts open-ended questions would need some metric with which to rank possible truthful answers in terms of their informativeness or appropriateness. In either case, building an oracle that has a fully domain-general ability to answer natural language questions is an AI-complete problem. If one could do that, one could probably also build an AI that has a decent ability to understand human intentions as well as human words.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Chapter+10%3A+Oracles%2C+genies%2C+sovereigns%2C+tools+%28page+145%29&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.place=Oxford&amp;rft.pub=Oxford+University+Press&amp;rft.date=2014&amp;rft.isbn=9780199678112&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text">Armstrong, S., Sandberg, A., &amp; Bostrom, N. (2012). Thinking inside the box: Controlling and using an oracle ai. Minds and Machines, 22(4), 299-324.</span>
</li>
<li id="cite_note-HC-16"><span class="mw-cite-backlink">^ <a href="#cite_ref-HC_16-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-HC_16-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-HC_16-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell, Stuart</a> (October 8, 2019). <i>Human Compatible: Artificial Intelligence and the Problem of Control</i>. United States: Viking. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-525-55861-3" title="Special:BookSources/978-0-525-55861-3"><bdi>978-0-525-55861-3</bdi></a>. <a href="/wiki/OCLC" title="OCLC">OCLC</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/oclc/1083694322">1083694322</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Human+Compatible%3A+Artificial+Intelligence+and+the+Problem+of+Control&amp;rft.place=United+States&amp;rft.pub=Viking&amp;rft.date=2019-10-08&amp;rft_id=info%3Aoclcnum%2F1083694322&amp;rft.isbn=978-0-525-55861-3&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-bostrom_chapter_10_page_147-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-bostrom_chapter_10_page_147_17-0">^</a></b></span> <span class="reference-text"><cite class="citation book">Bostrom, Nick (2014). "Chapter 10: Oracles, genies, sovereigns, tools (page 147)". <i>Superintelligence: Paths, Dangers, Strategies</i>. Oxford: Oxford University Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780199678112" title="Special:BookSources/9780199678112"><bdi>9780199678112</bdi></a>. <q>For example, consider the risk that an oracle will answer questions not in a maximally truthful way but in such a way as to subtly manipulate us into promoting its own hidden agenda. One way to slightly mitigate this threat could be to create multiple oracles, each with a slightly different code and a slightly different information base. A simple mechanism could then compare the answers given by the different oracles and only present them for human viewing if all the answers agree.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Chapter+10%3A+Oracles%2C+genies%2C+sovereigns%2C+tools+%28page+147%29&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.place=Oxford&amp;rft.pub=Oxford+University+Press&amp;rft.date=2014&amp;rft.isbn=9780199678112&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-bostrom_chapter_10_page_146-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-bostrom_chapter_10_page_146_18-0">^</a></b></span> <span class="reference-text"><cite class="citation book">Bostrom, Nick (2014). "Chapter 10: Oracles, genies, sovereigns, tools (page 146)". <i>Superintelligence: Paths, Dangers, Strategies</i>. Oxford: Oxford University Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780199678112" title="Special:BookSources/9780199678112"><bdi>9780199678112</bdi></a>. <q>What happens if the AI, in the course of its intellectual development, undergoes the equivalent of a scientific revolution involving a change in its basic ontology? We might initially have explicated impact and designated resources using our own ontology (postulating the existence of various physical objects such as computers). But just as we have abandoned ontological categories that were taken for granted by scientists in previous ages (e.g. phlogiston, lan vital, and absolute simultaneity), so a superintelligent AI might discover that some of our current categories are predicated on fundamental misconceptions. The goal system of an AI undergoing an ontological crisis needs to be resilient enough that the spirit of its original goal content is carried over, charitably transposed into the new key.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Chapter+10%3A+Oracles%2C+genies%2C+sovereigns%2C+tools+%28page+146%29&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.place=Oxford&amp;rft.pub=Oxford+University+Press&amp;rft.date=2014&amp;rft.isbn=9780199678112&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text">Fallenstein, Benja, and Nate Soares. "Problems of self-reference in self-improving space-time embedded intelligence." Artificial General Intelligence. Springer International Publishing, 2014. 21-32.</span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell, Stuart</a>; <a href="/wiki/Peter_Norvig" title="Peter Norvig">Norvig, Peter</a> (2009). "26.3: The Ethics and Risks of Developing Artificial Intelligence". <i><a href="/wiki/Artificial_Intelligence:_A_Modern_Approach" title="Artificial Intelligence: A Modern Approach">Artificial Intelligence: A Modern Approach</a></i>. Prentice Hall. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-13-604259-4" title="Special:BookSources/978-0-13-604259-4"><bdi>978-0-13-604259-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=26.3%3A+The+Ethics+and+Risks+of+Developing+Artificial+Intelligence&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.pub=Prentice+Hall&amp;rft.date=2009&amp;rft.isbn=978-0-13-604259-4&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-acm-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-acm_21-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Dietterich, Thomas; <a href="/wiki/Eric_Horvitz" title="Eric Horvitz">Horvitz, Eric</a> (2015). <a rel="nofollow" class="external text" href="http://research.microsoft.com/en-us/um/people/horvitz/CACM_Oct_2015-VP.pdf">"Rise of Concerns about AI: Reflections and Directions"</a> <span class="cs1-format">(PDF)</span>. <i><a href="/wiki/Communications_of_the_ACM" title="Communications of the ACM">Communications of the ACM</a></i>. <b>58</b> (10): 3840. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F2770869">10.1145/2770869</a><span class="reference-accessdate">. Retrieved <span class="nowrap">14 June</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Communications+of+the+ACM&amp;rft.atitle=Rise+of+Concerns+about+AI%3A+Reflections+and+Directions&amp;rft.volume=58&amp;rft.issue=10&amp;rft.pages=38-40&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1145%2F2770869&amp;rft.aulast=Dietterich&amp;rft.aufirst=Thomas&amp;rft.au=Horvitz%2C+Eric&amp;rft_id=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fhorvitz%2FCACM_Oct_2015-VP.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><cite class="citation web"><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell, Stuart</a> (2014). <a rel="nofollow" class="external text" href="http://edge.org/conversation/the-myth-of-ai#26015">"Of Myths and Moonshine"</a>. <i><a href="/wiki/Edge_Foundation,_Inc." title="Edge Foundation, Inc.">Edge</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">14 June</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Edge&amp;rft.atitle=Of+Myths+and+Moonshine&amp;rft.date=2014&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft_id=http%3A%2F%2Fedge.org%2Fconversation%2Fthe-myth-of-ai%2326015&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a>. "Complex value systems in friendly AI." Artificial general intelligence. Springer Berlin Heidelberg, 2011. 388-393.</span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><cite class="citation journal">Kaj Sotala; <a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a> (19 December 2014). "Responses to catastrophic AGI risk: a survey". <i><a href="/wiki/Physica_Scripta" title="Physica Scripta">Physica Scripta</a></i>. <b>90</b> (1).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Physica+Scripta&amp;rft.atitle=Responses+to+catastrophic+AGI+risk%3A+a+survey&amp;rft.volume=90&amp;rft.issue=1&amp;rft.date=2014-12-19&amp;rft.au=Kaj+Sotala&amp;rft.au=Roman+Yampolskiy&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-CIRL-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-CIRL_25-0">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Hadfield-Menell, Dylan; Dragan, Anca; <a href="/wiki/Pieter_Abbeel" title="Pieter Abbeel">Abbeel, Pieter</a>; <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell, Stuart</a> (12 November 2016). "Cooperative Inverse Reinforcement Learning". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1606.03137">1606.03137</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.AI">cs.AI</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Cooperative+Inverse+Reinforcement+Learning&amp;rft.date=2016-11-12&amp;rft_id=info%3Aarxiv%2F1606.03137&amp;rft.aulast=Hadfield-Menell&amp;rft.aufirst=Dylan&amp;rft.au=Dragan%2C+Anca&amp;rft.au=Abbeel%2C+Pieter&amp;rft.au=Russell%2C+Stuart&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+control+problem" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
</ol></div>
<div role="navigation" class="navbox" aria-labelledby="Existential_risk_from_artificial_intelligence" style="padding:3px"><table class="nowraplinks mw-collapsible expanded navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Existential_risk_from_artificial_intelligence" title="Template:Existential risk from artificial intelligence"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Existential_risk_from_artificial_intelligence" title="Template talk:Existential risk from artificial intelligence"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Existential_risk_from_artificial_intelligence" style="font-size:114%;margin:0 4em"><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk</a> from <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Accelerating_change" title="Accelerating change">Accelerating change</a></li>
<li><a href="/wiki/AI_box" title="AI box">AI box</a></li>
<li><a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></li>
<li><a class="mw-selflink selflink">Control problem</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly artificial intelligence</a></li>
<li><a href="/wiki/Instrumental_convergence" title="Instrumental convergence">Instrumental convergence</a></li>
<li><a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">Intelligence explosion</a></li>
<li><a href="/wiki/Machine_ethics" title="Machine ethics">Machine ethics</a></li>
<li><a href="/wiki/Superintelligence" title="Superintelligence">Superintelligence</a></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Allen_Institute_for_Artificial_Intelligence" class="mw-redirect" title="Allen Institute for Artificial Intelligence">Allen Institute for Artificial Intelligence</a></li>
<li><a href="/wiki/Center_for_Applied_Rationality" title="Center for Applied Rationality">Center for Applied Rationality</a></li>
<li><a href="/wiki/Center_for_Human-Compatible_Artificial_Intelligence" title="Center for Human-Compatible Artificial Intelligence">Center for Human-Compatible Artificial Intelligence</a></li>
<li><a href="/wiki/Center_for_Security_and_Emerging_Technology" title="Center for Security and Emerging Technology">Center for Security and Emerging Technology</a></li>
<li><a href="/wiki/Centre_for_the_Study_of_Existential_Risk" title="Centre for the Study of Existential Risk">Centre for the Study of Existential Risk</a></li>
<li><a href="/wiki/DeepMind" title="DeepMind">DeepMind</a></li>
<li><a href="/wiki/Foundational_Questions_Institute" title="Foundational Questions Institute">Foundational Questions Institute</a></li>
<li><a href="/wiki/Future_of_Humanity_Institute" title="Future of Humanity Institute">Future of Humanity Institute</a></li>
<li><a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a></li>
<li><a href="/wiki/Humanity%2B" title="Humanity+">Humanity+</a></li>
<li><a href="/wiki/Institute_for_Ethics_and_Emerging_Technologies" title="Institute for Ethics and Emerging Technologies">Institute for Ethics and Emerging Technologies</a></li>
<li><a href="/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence" title="Leverhulme Centre for the Future of Intelligence">Leverhulme Centre for the Future of Intelligence</a></li>
<li><a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a></li>
<li><a href="/wiki/K._Eric_Drexler" title="K. Eric Drexler">Eric Drexler</a></li>
<li><a href="/wiki/Sam_Harris" title="Sam Harris">Sam Harris</a></li>
<li><a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a></li>
<li><a href="/wiki/Bill_Hibbard" title="Bill Hibbard">Bill Hibbard</a></li>
<li><a href="/wiki/Bill_Joy" title="Bill Joy">Bill Joy</a></li>
<li><a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a></li>
<li><a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a></li>
<li><a href="/wiki/Huw_Price" title="Huw Price">Huw Price</a></li>
<li><a href="/wiki/Martin_Rees" title="Martin Rees">Martin Rees</a></li>
<li><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a></li>
<li><a href="/wiki/Jaan_Tallinn" title="Jaan Tallinn">Jaan Tallinn</a></li>
<li><a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a></li>
<li><a href="/wiki/Frank_Wilczek" title="Frank Wilczek">Frank Wilczek</a></li>
<li><a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a></li>
<li><a href="/wiki/Andrew_Yang" title="Andrew Yang">Andrew Yang</a></li>
<li><a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Other</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Global_catastrophic_risk#Artificial_intelligence" title="Global catastrophic risk">Artificial intelligence as a global catastrophic risk</a></li>
<li><a href="/wiki/Artificial_general_intelligence#Controversies_and_dangers" title="Artificial general intelligence">Controversies and dangers of artificial general intelligence</a></li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of artificial intelligence</a></li>
<li><i><a href="/wiki/Human_Compatible" title="Human Compatible">Human Compatible</a></i></li>
<li><a href="/wiki/Open_Letter_on_Artificial_Intelligence" title="Open Letter on Artificial Intelligence">Open Letter on Artificial Intelligence</a></li>
<li><i><a href="/wiki/Our_Final_Invention" title="Our Final Invention">Our Final Invention</a></i></li>
<li><i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<dl><dt><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" decoding="async" title="Category" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31" /> <a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Category</a></dt></dl>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw1341
Cached time: 20200217001955
Cache expiry: 2592000
Dynamic content: false
Complications: [varyrevisionsha1]
CPU time usage: 0.496 seconds
Real time usage: 0.666 seconds
Preprocessor visited node count: 1747/1000000
Postexpand include size: 65723/2097152 bytes
Template argument size: 1254/2097152 bytes
Highest expansion depth: 13/40
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip postexpand size: 70369/5000000 bytes
Number of Wikibase entities loaded: 3/400
Lua time usage: 0.266/10.000 seconds
Lua memory usage: 6.46 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  580.728      1 -total
 56.28%  326.853      1 Template:Reflist
 26.53%  154.094      6 Template:Cite_book
 14.07%   81.733      1 Template:Merge
 10.41%   60.440      5 Template:Cite_journal
  6.42%   37.264      6 Template:Cite_news
  6.36%   36.934      1 Template:Failed_verification
  5.79%   33.619      1 Template:Fix
  5.67%   32.949      1 Template:Mbox
  5.65%   32.813      1 Template:Existential_risk_from_artificial_intelligence
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:50785023-0!canonical and timestamp 20200217002006 and revision id 941167355
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=AI_control_problem&amp;oldid=941167355">https://en.wikipedia.org/w/index.php?title=AI_control_problem&amp;oldid=941167355</a>"</div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li><li><a href="/wiki/Category:Futurology" title="Category:Futurology">Futurology</a></li><li><a href="/wiki/Category:Philosophy_of_artificial_intelligence" title="Category:Philosophy of artificial intelligence">Philosophy of artificial intelligence</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Articles_to_be_merged_from_August_2019" title="Category:Articles to be merged from August 2019">Articles to be merged from August 2019</a></li><li><a href="/wiki/Category:All_articles_to_be_merged" title="Category:All articles to be merged">All articles to be merged</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:All_articles_with_failed_verification" title="Category:All articles with failed verification">All articles with failed verification</a></li><li><a href="/wiki/Category:Articles_with_failed_verification_from_March_2018" title="Category:Articles with failed verification from March 2018">Articles with failed verification from March 2018</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
    <h2>Navigation menu</h2>
    <div id="mw-head">
        
<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
	<h3 id="p-personal-label">Personal tools</h3>
	<ul >
		
		<li id="pt-anonuserpage">Not logged in</li>
		<li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=AI+control+problem" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=AI+control+problem" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>
	</ul>
</div>

        <div id="left-navigation">
            <div id="p-namespaces" role="navigation" class="vectorTabs " aria-labelledby="p-namespaces-label">
	<h3 id="p-namespaces-label">Namespaces</h3>
	<ul >
		<li id="ca-nstab-main" class="selected"><a href="/wiki/AI_control_problem" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="/wiki/Talk:AI_control_problem" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></li>
	</ul>
</div>
<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
	<h3 id="p-variants-label">
		<span>Variants</span>
	</h3>
	<ul class="menu" >
		
	</ul>
</div>

        </div>
        <div id="right-navigation">
            <div id="p-views" role="navigation" class="vectorTabs " aria-labelledby="p-views-label">
	<h3 id="p-views-label">Views</h3>
	<ul >
		<li id="ca-view" class="collapsible selected"><a href="/wiki/AI_control_problem">Read</a></li><li id="ca-edit" class="collapsible"><a href="/w/index.php?title=AI_control_problem&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="/w/index.php?title=AI_control_problem&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li>
	</ul>
</div>
<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
	<h3 id="p-cactions-label">
		<span>More</span>
	</h3>
	<ul class="menu" >
		
	</ul>
</div>
<div id="p-search" role="search">
	<h3 >
		<label for="searchInput">Search</label>
	</h3>
	<form action="/w/index.php" id="searchform">
		<div id="simpleSearch">
			<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
			<input type="hidden" value="Special:Search" name="title"/>
			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
		</div>
	</form>
</div>

        </div>
    </div>
    <div id="mw-panel">
        <div id="p-logo" role="banner">
            <a  title="Visit the main page" class="mw-wiki-logo" href="/wiki/Main_Page"></a>
        </div>
        
<div class="portal" role="navigation" id="p-navigation"  aria-labelledby="p-navigation-label">
	<h3  id="p-navigation-label">
		Navigation
	</h3>
	<div class="body">
		<ul><li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content  the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-interaction"  aria-labelledby="p-interaction-label">
	<h3  id="p-interaction-label">
		Interaction
	</h3>
	<div class="body">
		<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-tb"  aria-labelledby="p-tb-label">
	<h3  id="p-tb-label">
		Tools
	</h3>
	<div class="body">
		<ul><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/AI_control_problem" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/AI_control_problem" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=AI_control_problem&amp;oldid=941167355" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=AI_control_problem&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q24882728" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=AI_control_problem&amp;id=941167355" title="Information on how to cite this page">Cite this page</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-coll-print_export"  aria-labelledby="p-coll-print_export-label">
	<h3  id="p-coll-print_export-label">
		Print/export
	</h3>
	<div class="body">
		<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=AI+control+problem">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=AI+control+problem&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=AI_control_problem&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-lang"  aria-labelledby="p-lang-label">
	<h3  id="p-lang-label">
		Languages
	</h3>
	<div class="body">
		<ul></ul>
		<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-add wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q24882728#sitelinks-wikipedia" title="Add interlanguage links" class="wbc-editpage">Add links</a></span></div>
	</div>
</div>

    </div>
</div>


<div id="footer" role="contentinfo" >
	<ul id="footer-info" class="">
		<li id="footer-info-lastmod"> This page was last edited on 17 February 2020, at 00:20<span class="anonymous-show">&#160;(UTC)</span>.</li>
		<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
	</ul>
	<ul id="footer-places" class="">
		<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
		<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
		<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
		<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/v2/#/en.wikipedia.org">Statistics</a></li>
		<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
		<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=AI_control_problem&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a></li>
		<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a></li>
	</ul>
	<div style="clear: both;"></div>
</div>


<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.496","walltime":"0.666","ppvisitednodes":{"value":1747,"limit":1000000},"postexpandincludesize":{"value":65723,"limit":2097152},"templateargumentsize":{"value":1254,"limit":2097152},"expansiondepth":{"value":13,"limit":40},"expensivefunctioncount":{"value":5,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":70369,"limit":5000000},"entityaccesscount":{"value":3,"limit":400},"timingprofile":["100.00%  580.728      1 -total"," 56.28%  326.853      1 Template:Reflist"," 26.53%  154.094      6 Template:Cite_book"," 14.07%   81.733      1 Template:Merge"," 10.41%   60.440      5 Template:Cite_journal","  6.42%   37.264      6 Template:Cite_news","  6.36%   36.934      1 Template:Failed_verification","  5.79%   33.619      1 Template:Fix","  5.67%   32.949      1 Template:Mbox","  5.65%   32.813      1 Template:Existential_risk_from_artificial_intelligence"]},"scribunto":{"limitreport-timeusage":{"value":"0.266","limit":"10.000"},"limitreport-memusage":{"value":6774514,"limit":52428800}},"cachereport":{"origin":"mw1341","timestamp":"20200217001955","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"AI control problem","url":"https:\/\/en.wikipedia.org\/wiki\/AI_control_problem","sameAs":"http:\/\/www.wikidata.org\/entity\/Q24882728","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q24882728","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2016-06-12T00:40:27Z","dateModified":"2020-02-17T00:20:06Z","headline":"issue of how to build a superintelligent agent that will aid its creators, and avoid inadvertently building a superintelligence that will harm its creators"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":100,"wgHostname":"mw1369"});});</script></body></html>
