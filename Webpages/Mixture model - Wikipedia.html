<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Mixture model - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRequestId":"Xk8fcApAML8AAg0cbRgAAADY","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Mixture_model","wgTitle":"Mixture model","wgCurRevisionId":926043903,"wgRevisionId":926043903,"wgArticleId":871681,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles with incomplete citations from November 2012","All Wikipedia articles needing clarification","Wikipedia articles needing clarification from March 2008",
"Articles lacking in-text citations from November 2010","All articles lacking in-text citations","Cluster analysis","Latent variable models","Probabilistic models","Machine learning"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Mixture_model","wgRelevantArticleId":871681,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgRedirectedFrom":"Gaussian_mixture_model","wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgInternalRedirectTargetUrl":"/wiki/Mixture_model#Gaussian_mixture_model",
"wgWikibaseItemId":"Q2260434","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.math.styles":"ready","ext.cite.styles":"ready","ext.tmh.thumbnail.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","skins.vector.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["mediawiki.action.view.redirect","ext.math.scripts","ext.cite.ux-enhancements","mw.MediaWikiPlayer.loader","mw.PopUpMediaTransform","mw.TMHGalleryHook.js","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice",
"ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.tokens@tffin",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.tmh.thumbnail.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.35.0-wmf.20"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Mixture_model&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Mixture_model&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Mixture_model rootpage-Mixture_model skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">Mixture model</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"><span class="mw-redirectedfrom">&#160;&#160;(Redirected from <a href="/w/index.php?title=Gaussian_mixture_model&amp;redirect=no" class="mw-redirect" title="Gaussian mixture model">Gaussian mixture model</a>)</span></div>
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div role="note" class="hatnote navigation-not-searchable">Not to be confused with <a href="/wiki/Mixed_model" title="Mixed model">mixed model</a>.</div>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Mixture_distribution" title="Mixture distribution">Mixture distribution</a></div>
<p>In <a href="/wiki/Statistics" title="Statistics">statistics</a>, a <b>mixture model</b> is a <a href="/wiki/Probabilistic_model" class="mw-redirect" title="Probabilistic model">probabilistic model</a> for representing the presence of <a href="/wiki/Subpopulation" class="mw-redirect" title="Subpopulation">subpopulations</a> within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the <a href="/wiki/Mixture_distribution" title="Mixture distribution">mixture distribution</a> that represents the <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> of observations in the overall population. However, while problems associated with "mixture distributions" relate to deriving the properties of the overall population from those of the sub-populations, "mixture models" are used to make <a href="/wiki/Statistical_inference" title="Statistical inference">statistical inferences</a> about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.
</p><p>Some ways of implementing mixture models involve steps that attribute postulated sub-population-identities to individual observations (or weights towards such sub-populations), in which case these can be regarded as types of <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a> or <a href="/wiki/Cluster_analysis" title="Cluster analysis">clustering</a> procedures. However, not all inference procedures involve such steps.
</p><p>Mixture models should not be confused with models for <a href="/wiki/Compositional_data" title="Compositional data">compositional data</a>, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the <a href="/wiki/Measure_(mathematics)" title="Measure (mathematics)">total size</a> reading population has been normalized to 1.
</p>
<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Structure"><span class="tocnumber">1</span> <span class="toctext">Structure</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#General_mixture_model"><span class="tocnumber">1.1</span> <span class="toctext">General mixture model</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Specific_examples"><span class="tocnumber">1.2</span> <span class="toctext">Specific examples</span></a>
<ul>
<li class="toclevel-3 tocsection-4"><a href="#Gaussian_mixture_model"><span class="tocnumber">1.2.1</span> <span class="toctext">Gaussian mixture model</span></a></li>
<li class="toclevel-3 tocsection-5"><a href="#Multivariate_Gaussian_mixture_model"><span class="tocnumber">1.2.2</span> <span class="toctext">Multivariate Gaussian mixture model</span></a></li>
<li class="toclevel-3 tocsection-6"><a href="#Categorical_mixture_model"><span class="tocnumber">1.2.3</span> <span class="toctext">Categorical mixture model</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-7"><a href="#Examples"><span class="tocnumber">2</span> <span class="toctext">Examples</span></a>
<ul>
<li class="toclevel-2 tocsection-8"><a href="#A_financial_model"><span class="tocnumber">2.1</span> <span class="toctext">A financial model</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#House_prices"><span class="tocnumber">2.2</span> <span class="toctext">House prices</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#Topics_in_a_document"><span class="tocnumber">2.3</span> <span class="toctext">Topics in a document</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Handwriting_recognition"><span class="tocnumber">2.4</span> <span class="toctext">Handwriting recognition</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Assessing_projectile_accuracy_(a.k.a._circular_error_probable,_CEP)"><span class="tocnumber">2.5</span> <span class="toctext">Assessing projectile accuracy (a.k.a. circular error probable, CEP)</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="#Direct_and_indirect_applications"><span class="tocnumber">2.6</span> <span class="toctext">Direct and indirect applications</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#Predictive_Maintenance"><span class="tocnumber">2.7</span> <span class="toctext">Predictive Maintenance</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Fuzzy_image_segmentation"><span class="tocnumber">2.8</span> <span class="toctext">Fuzzy image segmentation</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Point_set_registration"><span class="tocnumber">2.9</span> <span class="toctext">Point set registration</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-17"><a href="#Identifiability"><span class="tocnumber">3</span> <span class="toctext">Identifiability</span></a>
<ul>
<li class="toclevel-2 tocsection-18"><a href="#Example"><span class="tocnumber">3.1</span> <span class="toctext">Example</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="#Definition"><span class="tocnumber">3.2</span> <span class="toctext">Definition</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-20"><a href="#Parameter_estimation_and_system_identification"><span class="tocnumber">4</span> <span class="toctext">Parameter estimation and system identification</span></a>
<ul>
<li class="toclevel-2 tocsection-21"><a href="#Expectation_maximization_(EM)"><span class="tocnumber">4.1</span> <span class="toctext">Expectation maximization (EM)</span></a>
<ul>
<li class="toclevel-3 tocsection-22"><a href="#The_expectation_step"><span class="tocnumber">4.1.1</span> <span class="toctext">The expectation step</span></a></li>
<li class="toclevel-3 tocsection-23"><a href="#The_maximization_step"><span class="tocnumber">4.1.2</span> <span class="toctext">The maximization step</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-24"><a href="#Markov_chain_Monte_Carlo"><span class="tocnumber">4.2</span> <span class="toctext">Markov chain Monte Carlo</span></a></li>
<li class="toclevel-2 tocsection-25"><a href="#Moment_matching"><span class="tocnumber">4.3</span> <span class="toctext">Moment matching</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="#Spectral_method"><span class="tocnumber">4.4</span> <span class="toctext">Spectral method</span></a></li>
<li class="toclevel-2 tocsection-27"><a href="#Graphical_Methods"><span class="tocnumber">4.5</span> <span class="toctext">Graphical Methods</span></a></li>
<li class="toclevel-2 tocsection-28"><a href="#Other_methods"><span class="tocnumber">4.6</span> <span class="toctext">Other methods</span></a></li>
<li class="toclevel-2 tocsection-29"><a href="#A_simulation"><span class="tocnumber">4.7</span> <span class="toctext">A simulation</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-30"><a href="#Extensions"><span class="tocnumber">5</span> <span class="toctext">Extensions</span></a></li>
<li class="toclevel-1 tocsection-31"><a href="#History"><span class="tocnumber">6</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-32"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a>
<ul>
<li class="toclevel-2 tocsection-33"><a href="#Mixture"><span class="tocnumber">7.1</span> <span class="toctext">Mixture</span></a></li>
<li class="toclevel-2 tocsection-34"><a href="#Hierarchical_models"><span class="tocnumber">7.2</span> <span class="toctext">Hierarchical models</span></a></li>
<li class="toclevel-2 tocsection-35"><a href="#Outlier_detection"><span class="tocnumber">7.3</span> <span class="toctext">Outlier detection</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-36"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-37"><a href="#Further_reading"><span class="tocnumber">9</span> <span class="toctext">Further reading</span></a>
<ul>
<li class="toclevel-2 tocsection-38"><a href="#Books_on_mixture_models"><span class="tocnumber">9.1</span> <span class="toctext">Books on mixture models</span></a></li>
<li class="toclevel-2 tocsection-39"><a href="#Application_of_Gaussian_mixture_models"><span class="tocnumber">9.2</span> <span class="toctext">Application of Gaussian mixture models</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-40"><a href="#External_links"><span class="tocnumber">10</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Structure">Structure</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=1" title="Edit section: Structure">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="General_mixture_model">General mixture model</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=2" title="Edit section: General mixture model">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A typical finite-dimensional mixture model is a <a href="/wiki/Hierarchical_Bayes_model" class="mw-redirect" title="Hierarchical Bayes model">hierarchical model</a> consisting of the following components:
</p>
<ul><li><i>N</i> random variables that are observed, each distributed according to a mixture of <i>K</i> components, with the components belonging to the same <a href="/wiki/Parametric_family" title="Parametric family">parametric family</a> of distributions (e.g., all <a href="/wiki/Normal_distribution" title="Normal distribution">normal</a>, all <a href="/wiki/Zipf%27s_law" title="Zipf&#39;s law">Zipfian</a>, etc.) but with different parameters</li>
<li><i>N</i> random <a href="/wiki/Latent_variable" title="Latent variable">latent variables</a> specifying the identity of the mixture component of each observation, each distributed according to a <i>K</i>-dimensional <a href="/wiki/Categorical_distribution" title="Categorical distribution">categorical distribution</a></li>
<li>A set of <i>K</i> mixture weights, which are probabilities that sum to 1.</li>
<li>A set of <i>K</i> parameters, each specifying the parameter of the corresponding mixture component.  In many cases, each "parameter" is actually a set of parameters.  For example, if the mixture components are <a href="/wiki/Gaussian_distribution" class="mw-redirect" title="Gaussian distribution">Gaussian distributions</a>, there will be a <a href="/wiki/Mean" title="Mean">mean</a> and <a href="/wiki/Variance" title="Variance">variance</a> for each component. If the mixture components are <a href="/wiki/Categorical_distribution" title="Categorical distribution">categorical distributions</a> (e.g., when each observation is a token from a finite alphabet of size <i>V</i>), there will be a vector of <i>V</i> probabilities summing to 1.</li></ul>
<p>In addition, in a <a href="/wiki/Bayesian_inference" title="Bayesian inference">Bayesian setting</a>, the mixture weights and parameters will themselves be random variables, and <a href="/wiki/Prior_distribution" class="mw-redirect" title="Prior distribution">prior distributions</a> will be placed over the variables.  In such a case, the weights are typically viewed as a <i>K</i>-dimensional random vector drawn from a <a href="/wiki/Dirichlet_distribution" title="Dirichlet distribution">Dirichlet distribution</a> (the <a href="/wiki/Conjugate_prior" title="Conjugate prior">conjugate prior</a> of the categorical distribution), and the parameters will be distributed according to their respective conjugate priors.
</p><p>Mathematically, a basic parametric mixture model can be described as follows:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\begin{array}{lcl}K&amp;=&amp;{\text{number of mixture components}}\\N&amp;=&amp;{\text{number of observations}}\\\theta _{i=1\dots K}&amp;=&amp;{\text{parameter of distribution of observation associated with component }}i\\\phi _{i=1\dots K}&amp;=&amp;{\text{mixture weight, i.e., prior probability of a particular component }}i\\{\boldsymbol {\phi }}&amp;=&amp;K{\text{-dimensional vector composed of all the individual }}\phi _{1\dots K}{\text{; must sum to 1}}\\z_{i=1\dots N}&amp;=&amp;{\text{component of observation }}i\\x_{i=1\dots N}&amp;=&amp;{\text{observation }}i\\F(x|\theta )&amp;=&amp;{\text{probability distribution of an observation, parametrized on }}\theta \\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}|z_{i=1\dots N}&amp;\sim &amp;F(\theta _{z_{i}})\end{array}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="left center left" rowspacing="4pt" columnspacing="1em">
            <mtr>
              <mtd>
                <mi>K</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>number of mixture components</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>N</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>number of observations</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03B8;<!-- θ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>parameter of distribution of observation associated with component&#xA0;</mtext>
                </mrow>
                <mi>i</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03D5;<!-- ϕ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>mixture weight, i.e., prior probability of a particular component&#xA0;</mtext>
                </mrow>
                <mi>i</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mi>K</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>-dimensional vector composed of all the individual&#xA0;</mtext>
                </mrow>
                <msub>
                  <mi>&#x03D5;<!-- ϕ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>; must sum to 1</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>component of observation&#xA0;</mtext>
                </mrow>
                <mi>i</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>observation&#xA0;</mtext>
                </mrow>
                <mi>i</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>F</mi>
                <mo stretchy="false">(</mo>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">|</mo>
                </mrow>
                <mi>&#x03B8;<!-- θ --></mi>
                <mo stretchy="false">)</mo>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>probability distribution of an observation, parametrized on&#xA0;</mtext>
                </mrow>
                <mi>&#x03B8;<!-- θ --></mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mi>Categorical</mi>
                <mo>&#x2061;<!-- ⁡ --></mo>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">|</mo>
                </mrow>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mi>F</mi>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>&#x03B8;<!-- θ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msub>
                      <mi>z</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>i</mi>
                      </mrow>
                    </msub>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{array}{lcl}K&amp;=&amp;{\text{number of mixture components}}\\N&amp;=&amp;{\text{number of observations}}\\\theta _{i=1\dots K}&amp;=&amp;{\text{parameter of distribution of observation associated with component }}i\\\phi _{i=1\dots K}&amp;=&amp;{\text{mixture weight, i.e., prior probability of a particular component }}i\\{\boldsymbol {\phi }}&amp;=&amp;K{\text{-dimensional vector composed of all the individual }}\phi _{1\dots K}{\text{; must sum to 1}}\\z_{i=1\dots N}&amp;=&amp;{\text{component of observation }}i\\x_{i=1\dots N}&amp;=&amp;{\text{observation }}i\\F(x|\theta )&amp;=&amp;{\text{probability distribution of an observation, parametrized on }}\theta \\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}|z_{i=1\dots N}&amp;\sim &amp;F(\theta _{z_{i}})\end{array}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7ca7990c1a320c9fb6295fc6fde881f81890a64c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -15.671ex; width:96.789ex; height:32.509ex;" alt="{\displaystyle {\begin{array}{lcl}K&amp;=&amp;{\text{number of mixture components}}\\N&amp;=&amp;{\text{number of observations}}\\\theta _{i=1\dots K}&amp;=&amp;{\text{parameter of distribution of observation associated with component }}i\\\phi _{i=1\dots K}&amp;=&amp;{\text{mixture weight, i.e., prior probability of a particular component }}i\\{\boldsymbol {\phi }}&amp;=&amp;K{\text{-dimensional vector composed of all the individual }}\phi _{1\dots K}{\text{; must sum to 1}}\\z_{i=1\dots N}&amp;=&amp;{\text{component of observation }}i\\x_{i=1\dots N}&amp;=&amp;{\text{observation }}i\\F(x|\theta )&amp;=&amp;{\text{probability distribution of an observation, parametrized on }}\theta \\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}|z_{i=1\dots N}&amp;\sim &amp;F(\theta _{z_{i}})\end{array}}}"/></span></dd></dl>
<p>In a Bayesian setting, all parameters are associated with random variables, as follows:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\begin{array}{lcl}K,N&amp;=&amp;{\text{as above}}\\\theta _{i=1\dots K},\phi _{i=1\dots K},{\boldsymbol {\phi }}&amp;=&amp;{\text{as above}}\\z_{i=1\dots N},x_{i=1\dots N},F(x|\theta )&amp;=&amp;{\text{as above}}\\\alpha &amp;=&amp;{\text{shared hyperparameter for component parameters}}\\\beta &amp;=&amp;{\text{shared hyperparameter for mixture weights}}\\H(\theta |\alpha )&amp;=&amp;{\text{prior probability distribution of component parameters, parametrized on }}\alpha \\\theta _{i=1\dots K}&amp;\sim &amp;H(\theta |\alpha )\\{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Symmetric-Dirichlet} _{K}(\beta )\\z_{i=1\dots N}|{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}|z_{i=1\dots N},\theta _{i=1\dots K}&amp;\sim &amp;F(\theta _{z_{i}})\end{array}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="left center left" rowspacing="4pt" columnspacing="1em">
            <mtr>
              <mtd>
                <mi>K</mi>
                <mo>,</mo>
                <mi>N</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>as above</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03B8;<!-- θ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
                <mo>,</mo>
                <msub>
                  <mi>&#x03D5;<!-- ϕ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
                <mo>,</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>as above</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
                <mo>,</mo>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
                <mo>,</mo>
                <mi>F</mi>
                <mo stretchy="false">(</mo>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">|</mo>
                </mrow>
                <mi>&#x03B8;<!-- θ --></mi>
                <mo stretchy="false">)</mo>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>as above</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>&#x03B1;<!-- α --></mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>shared hyperparameter for component parameters</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>&#x03B2;<!-- β --></mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>shared hyperparameter for mixture weights</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>H</mi>
                <mo stretchy="false">(</mo>
                <mi>&#x03B8;<!-- θ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">|</mo>
                </mrow>
                <mi>&#x03B1;<!-- α --></mi>
                <mo stretchy="false">)</mo>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>prior probability distribution of component parameters, parametrized on&#xA0;</mtext>
                </mrow>
                <mi>&#x03B1;<!-- α --></mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03B8;<!-- θ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mi>H</mi>
                <mo stretchy="false">(</mo>
                <mi>&#x03B8;<!-- θ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">|</mo>
                </mrow>
                <mi>&#x03B1;<!-- α --></mi>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <msub>
                  <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
                    <mi mathvariant="normal">S</mi>
                    <mi mathvariant="normal">y</mi>
                    <mi mathvariant="normal">m</mi>
                    <mi mathvariant="normal">m</mi>
                    <mi mathvariant="normal">e</mi>
                    <mi mathvariant="normal">t</mi>
                    <mi mathvariant="normal">r</mi>
                    <mi mathvariant="normal">i</mi>
                    <mi mathvariant="normal">c</mi>
                    <mtext>-</mtext>
                    <mi mathvariant="normal">D</mi>
                    <mi mathvariant="normal">i</mi>
                    <mi mathvariant="normal">r</mi>
                    <mi mathvariant="normal">i</mi>
                    <mi mathvariant="normal">c</mi>
                    <mi mathvariant="normal">h</mi>
                    <mi mathvariant="normal">l</mi>
                    <mi mathvariant="normal">e</mi>
                    <mi mathvariant="normal">t</mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>K</mi>
                  </mrow>
                </msub>
                <mo>&#x2061;<!-- ⁡ --></mo>
                <mo stretchy="false">(</mo>
                <mi>&#x03B2;<!-- β --></mi>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">|</mo>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mi>Categorical</mi>
                <mo>&#x2061;<!-- ⁡ --></mo>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">|</mo>
                </mrow>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
                <mo>,</mo>
                <msub>
                  <mi>&#x03B8;<!-- θ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mi>F</mi>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>&#x03B8;<!-- θ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msub>
                      <mi>z</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>i</mi>
                      </mrow>
                    </msub>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{array}{lcl}K,N&amp;=&amp;{\text{as above}}\\\theta _{i=1\dots K},\phi _{i=1\dots K},{\boldsymbol {\phi }}&amp;=&amp;{\text{as above}}\\z_{i=1\dots N},x_{i=1\dots N},F(x|\theta )&amp;=&amp;{\text{as above}}\\\alpha &amp;=&amp;{\text{shared hyperparameter for component parameters}}\\\beta &amp;=&amp;{\text{shared hyperparameter for mixture weights}}\\H(\theta |\alpha )&amp;=&amp;{\text{prior probability distribution of component parameters, parametrized on }}\alpha \\\theta _{i=1\dots K}&amp;\sim &amp;H(\theta |\alpha )\\{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Symmetric-Dirichlet} _{K}(\beta )\\z_{i=1\dots N}|{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}|z_{i=1\dots N},\theta _{i=1\dots K}&amp;\sim &amp;F(\theta _{z_{i}})\end{array}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7bdc75e1d0a124bc684307724e2635f53b88be8d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -15.838ex; width:106.062ex; height:32.843ex;" alt="{\displaystyle {\begin{array}{lcl}K,N&amp;=&amp;{\text{as above}}\\\theta _{i=1\dots K},\phi _{i=1\dots K},{\boldsymbol {\phi }}&amp;=&amp;{\text{as above}}\\z_{i=1\dots N},x_{i=1\dots N},F(x|\theta )&amp;=&amp;{\text{as above}}\\\alpha &amp;=&amp;{\text{shared hyperparameter for component parameters}}\\\beta &amp;=&amp;{\text{shared hyperparameter for mixture weights}}\\H(\theta |\alpha )&amp;=&amp;{\text{prior probability distribution of component parameters, parametrized on }}\alpha \\\theta _{i=1\dots K}&amp;\sim &amp;H(\theta |\alpha )\\{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Symmetric-Dirichlet} _{K}(\beta )\\z_{i=1\dots N}|{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}|z_{i=1\dots N},\theta _{i=1\dots K}&amp;\sim &amp;F(\theta _{z_{i}})\end{array}}}"/></span></dd></dl>
<p>This characterization uses <i>F</i> and <i>H</i> to describe arbitrary distributions over observations and parameters, respectively.  Typically <i>H</i> will be the <a href="/wiki/Conjugate_prior" title="Conjugate prior">conjugate prior</a> of <i>F</i>.  The two most common choices of <i>F</i> are <a href="/wiki/Gaussian_distribution" class="mw-redirect" title="Gaussian distribution">Gaussian</a> aka "<a href="/wiki/Normal_distribution" title="Normal distribution">normal</a>" (for real-valued observations) and <a href="/wiki/Categorical_distribution" title="Categorical distribution">categorical</a> (for discrete observations).  Other common possibilities for the distribution of the mixture components are:
</p>
<ul><li><a href="/wiki/Binomial_distribution" title="Binomial distribution">Binomial distribution</a>, for the number of "positive occurrences" (e.g., successes, yes votes, etc.) given a fixed number of total occurrences</li>
<li><a href="/wiki/Multinomial_distribution" title="Multinomial distribution">Multinomial distribution</a>, similar to the binomial distribution, but for counts of multi-way occurrences (e.g., yes/no/maybe in a survey)</li>
<li><a href="/wiki/Negative_binomial_distribution" title="Negative binomial distribution">Negative binomial distribution</a>, for binomial-type observations but where the quantity of interest is the number of failures before a given number of successes occurs</li>
<li><a href="/wiki/Poisson_distribution" title="Poisson distribution">Poisson distribution</a>, for the number of occurrences of an event in a given period of time, for an event that is characterized by a fixed rate of occurrence</li>
<li><a href="/wiki/Exponential_distribution" title="Exponential distribution">Exponential distribution</a>, for the time before the next event occurs, for an event that is characterized by a fixed rate of occurrence</li>
<li><a href="/wiki/Log-normal_distribution" title="Log-normal distribution">Log-normal distribution</a>, for positive real numbers that are assumed to grow exponentially, such as incomes or prices</li>
<li><a href="/wiki/Multivariate_normal_distribution" title="Multivariate normal distribution">Multivariate normal distribution</a> (aka <a href="/wiki/Multivariate_Gaussian_distribution" class="mw-redirect" title="Multivariate Gaussian distribution">multivariate Gaussian distribution</a>), for vectors of correlated outcomes that are individually Gaussian-distributed</li>
<li><a href="/w/index.php?title=Multivariate_Student%27s-t_distribution&amp;action=edit&amp;redlink=1" class="new" title="Multivariate Student&#39;s-t distribution (page does not exist)">Multivariate Student's-t distribution</a> (aka <a href="/wiki/Multivariate_t-distribution" title="Multivariate t-distribution">multivariate t-distribution</a>), for vectors of heavy-tailed correlated outcomes<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup></li>
<li>A vector of <a href="/wiki/Bernoulli_distribution" title="Bernoulli distribution">Bernoulli</a>-distributed values, corresponding, e.g., to a black-and-white image, with each value representing a pixel; see the handwriting-recognition example below</li></ul>
<h3><span class="mw-headline" id="Specific_examples">Specific examples</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=3" title="Edit section: Specific examples">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Gaussian_mixture_model">Gaussian mixture model</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=4" title="Edit section: Gaussian mixture model">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<div class="thumb tright"><div class="thumbinner" style="width:252px;"><a href="/wiki/File:Nonbayesian-gaussian-mixture.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Nonbayesian-gaussian-mixture.svg/250px-Nonbayesian-gaussian-mixture.svg.png" decoding="async" width="250" height="207" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Nonbayesian-gaussian-mixture.svg/375px-Nonbayesian-gaussian-mixture.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Nonbayesian-gaussian-mixture.svg/500px-Nonbayesian-gaussian-mixture.svg.png 2x" data-file-width="186" data-file-height="154" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Nonbayesian-gaussian-mixture.svg" class="internal" title="Enlarge"></a></div>Non-Bayesian Gaussian mixture model using <a href="/wiki/Plate_notation" title="Plate notation">plate notation</a>.  Smaller squares indicate fixed parameters; larger circles indicate random variables.  Filled-in shapes indicate known values.  The indication [K] means a vector of size <i>K</i>.</div></div></div>
<p>A typical non-Bayesian <a href="/wiki/Gaussian_distribution" class="mw-redirect" title="Gaussian distribution">Gaussian</a> mixture model looks like this:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\begin{array}{lcl}K,N&amp;=&amp;{\text{as above}}\\\phi _{i=1\dots K},{\boldsymbol {\phi }}&amp;=&amp;{\text{as above}}\\z_{i=1\dots N},x_{i=1\dots N}&amp;=&amp;{\text{as above}}\\\theta _{i=1\dots K}&amp;=&amp;\{\mu _{i=1\dots K},\sigma _{i=1\dots K}^{2}\}\\\mu _{i=1\dots K}&amp;=&amp;{\text{mean of component }}i\\\sigma _{i=1\dots K}^{2}&amp;=&amp;{\text{variance of component }}i\\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\mathcal {N}}(\mu _{z_{i}},\sigma _{z_{i}}^{2})\end{array}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="left center left" rowspacing="4pt" columnspacing="1em">
            <mtr>
              <mtd>
                <mi>K</mi>
                <mo>,</mo>
                <mi>N</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>as above</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03D5;<!-- ϕ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
                <mo>,</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>as above</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
                <mo>,</mo>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>as above</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03B8;<!-- θ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mo fence="false" stretchy="false">{</mo>
                <msub>
                  <mi>&#x03BC;<!-- μ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
                <mo>,</mo>
                <msubsup>
                  <mi>&#x03C3;<!-- σ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msubsup>
                <mo fence="false" stretchy="false">}</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03BC;<!-- μ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>mean of component&#xA0;</mtext>
                </mrow>
                <mi>i</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msubsup>
                  <mi>&#x03C3;<!-- σ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msubsup>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>variance of component&#xA0;</mtext>
                </mrow>
                <mi>i</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mi>Categorical</mi>
                <mo>&#x2061;<!-- ⁡ --></mo>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
                  </mrow>
                </mrow>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>&#x03BC;<!-- μ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msub>
                      <mi>z</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>i</mi>
                      </mrow>
                    </msub>
                  </mrow>
                </msub>
                <mo>,</mo>
                <msubsup>
                  <mi>&#x03C3;<!-- σ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msub>
                      <mi>z</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>i</mi>
                      </mrow>
                    </msub>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msubsup>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{array}{lcl}K,N&amp;=&amp;{\text{as above}}\\\phi _{i=1\dots K},{\boldsymbol {\phi }}&amp;=&amp;{\text{as above}}\\z_{i=1\dots N},x_{i=1\dots N}&amp;=&amp;{\text{as above}}\\\theta _{i=1\dots K}&amp;=&amp;\{\mu _{i=1\dots K},\sigma _{i=1\dots K}^{2}\}\\\mu _{i=1\dots K}&amp;=&amp;{\text{mean of component }}i\\\sigma _{i=1\dots K}^{2}&amp;=&amp;{\text{variance of component }}i\\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\mathcal {N}}(\mu _{z_{i}},\sigma _{z_{i}}^{2})\end{array}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6f54869aea02dad65150187b4cfb0bf6687dc8eb" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -12.511ex; margin-bottom: -0.327ex; width:47.214ex; height:26.843ex;" alt="{\begin{array}{lcl}K,N&amp;=&amp;{\text{as above}}\\\phi _{i=1\dots K},{\boldsymbol {\phi }}&amp;=&amp;{\text{as above}}\\z_{i=1\dots N},x_{i=1\dots N}&amp;=&amp;{\text{as above}}\\\theta _{i=1\dots K}&amp;=&amp;\{\mu _{i=1\dots K},\sigma _{i=1\dots K}^{2}\}\\\mu _{i=1\dots K}&amp;=&amp;{\text{mean of component }}i\\\sigma _{i=1\dots K}^{2}&amp;=&amp;{\text{variance of component }}i\\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\mathcal {N}}(\mu _{z_{i}},\sigma _{z_{i}}^{2})\end{array}}"/></span></dd></dl>
<div style="clear:both;"></div>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="/wiki/File:Bayesian-gaussian-mixture.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Bayesian-gaussian-mixture.svg/300px-Bayesian-gaussian-mixture.svg.png" decoding="async" width="300" height="283" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Bayesian-gaussian-mixture.svg/450px-Bayesian-gaussian-mixture.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/28/Bayesian-gaussian-mixture.svg/600px-Bayesian-gaussian-mixture.svg.png 2x" data-file-width="232" data-file-height="219" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Bayesian-gaussian-mixture.svg" class="internal" title="Enlarge"></a></div>Bayesian Gaussian mixture model using <a href="/wiki/Plate_notation" title="Plate notation">plate notation</a>.  Smaller squares indicate fixed parameters; larger circles indicate random variables.  Filled-in shapes indicate known values.  The indication [K] means a vector of size <i>K</i>.</div></div></div>
<p>A Bayesian version of a <a href="/wiki/Gaussian_distribution" class="mw-redirect" title="Gaussian distribution">Gaussian</a> mixture model is as follows:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\begin{array}{lcl}K,N&amp;=&amp;{\text{as above}}\\\phi _{i=1\dots K},{\boldsymbol {\phi }}&amp;=&amp;{\text{as above}}\\z_{i=1\dots N},x_{i=1\dots N}&amp;=&amp;{\text{as above}}\\\theta _{i=1\dots K}&amp;=&amp;\{\mu _{i=1\dots K},\sigma _{i=1\dots K}^{2}\}\\\mu _{i=1\dots K}&amp;=&amp;{\text{mean of component }}i\\\sigma _{i=1\dots K}^{2}&amp;=&amp;{\text{variance of component }}i\\\mu _{0},\lambda ,\nu ,\sigma _{0}^{2}&amp;=&amp;{\text{shared hyperparameters}}\\\mu _{i=1\dots K}&amp;\sim &amp;{\mathcal {N}}(\mu _{0},\lambda \sigma _{i}^{2})\\\sigma _{i=1\dots K}^{2}&amp;\sim &amp;\operatorname {Inverse-Gamma} (\nu ,\sigma _{0}^{2})\\{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Symmetric-Dirichlet} _{K}(\beta )\\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\mathcal {N}}(\mu _{z_{i}},\sigma _{z_{i}}^{2})\end{array}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="left center left" rowspacing="4pt" columnspacing="1em">
            <mtr>
              <mtd>
                <mi>K</mi>
                <mo>,</mo>
                <mi>N</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>as above</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03D5;<!-- ϕ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
                <mo>,</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>as above</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
                <mo>,</mo>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>as above</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03B8;<!-- θ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mo fence="false" stretchy="false">{</mo>
                <msub>
                  <mi>&#x03BC;<!-- μ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
                <mo>,</mo>
                <msubsup>
                  <mi>&#x03C3;<!-- σ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msubsup>
                <mo fence="false" stretchy="false">}</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03BC;<!-- μ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>mean of component&#xA0;</mtext>
                </mrow>
                <mi>i</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msubsup>
                  <mi>&#x03C3;<!-- σ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msubsup>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>variance of component&#xA0;</mtext>
                </mrow>
                <mi>i</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03BC;<!-- μ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>0</mn>
                  </mrow>
                </msub>
                <mo>,</mo>
                <mi>&#x03BB;<!-- λ --></mi>
                <mo>,</mo>
                <mi>&#x03BD;<!-- ν --></mi>
                <mo>,</mo>
                <msubsup>
                  <mi>&#x03C3;<!-- σ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>0</mn>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msubsup>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>shared hyperparameters</mtext>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>&#x03BC;<!-- μ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
                  </mrow>
                </mrow>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>&#x03BC;<!-- μ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>0</mn>
                  </mrow>
                </msub>
                <mo>,</mo>
                <mi>&#x03BB;<!-- λ --></mi>
                <msubsup>
                  <mi>&#x03C3;<!-- σ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msubsup>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msubsup>
                  <mi>&#x03C3;<!-- σ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msubsup>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
                  <mi mathvariant="normal">I</mi>
                  <mi mathvariant="normal">n</mi>
                  <mi mathvariant="normal">v</mi>
                  <mi mathvariant="normal">e</mi>
                  <mi mathvariant="normal">r</mi>
                  <mi mathvariant="normal">s</mi>
                  <mi mathvariant="normal">e</mi>
                  <mtext>-</mtext>
                  <mi mathvariant="normal">G</mi>
                  <mi mathvariant="normal">a</mi>
                  <mi mathvariant="normal">m</mi>
                  <mi mathvariant="normal">m</mi>
                  <mi mathvariant="normal">a</mi>
                </mrow>
                <mo>&#x2061;<!-- ⁡ --></mo>
                <mo stretchy="false">(</mo>
                <mi>&#x03BD;<!-- ν --></mi>
                <mo>,</mo>
                <msubsup>
                  <mi>&#x03C3;<!-- σ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>0</mn>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msubsup>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <msub>
                  <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
                    <mi mathvariant="normal">S</mi>
                    <mi mathvariant="normal">y</mi>
                    <mi mathvariant="normal">m</mi>
                    <mi mathvariant="normal">m</mi>
                    <mi mathvariant="normal">e</mi>
                    <mi mathvariant="normal">t</mi>
                    <mi mathvariant="normal">r</mi>
                    <mi mathvariant="normal">i</mi>
                    <mi mathvariant="normal">c</mi>
                    <mtext>-</mtext>
                    <mi mathvariant="normal">D</mi>
                    <mi mathvariant="normal">i</mi>
                    <mi mathvariant="normal">r</mi>
                    <mi mathvariant="normal">i</mi>
                    <mi mathvariant="normal">c</mi>
                    <mi mathvariant="normal">h</mi>
                    <mi mathvariant="normal">l</mi>
                    <mi mathvariant="normal">e</mi>
                    <mi mathvariant="normal">t</mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>K</mi>
                  </mrow>
                </msub>
                <mo>&#x2061;<!-- ⁡ --></mo>
                <mo stretchy="false">(</mo>
                <mi>&#x03B2;<!-- β --></mi>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mi>Categorical</mi>
                <mo>&#x2061;<!-- ⁡ --></mo>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
                  </mrow>
                </mrow>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>&#x03BC;<!-- μ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msub>
                      <mi>z</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>i</mi>
                      </mrow>
                    </msub>
                  </mrow>
                </msub>
                <mo>,</mo>
                <msubsup>
                  <mi>&#x03C3;<!-- σ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msub>
                      <mi>z</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>i</mi>
                      </mrow>
                    </msub>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msubsup>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{array}{lcl}K,N&amp;=&amp;{\text{as above}}\\\phi _{i=1\dots K},{\boldsymbol {\phi }}&amp;=&amp;{\text{as above}}\\z_{i=1\dots N},x_{i=1\dots N}&amp;=&amp;{\text{as above}}\\\theta _{i=1\dots K}&amp;=&amp;\{\mu _{i=1\dots K},\sigma _{i=1\dots K}^{2}\}\\\mu _{i=1\dots K}&amp;=&amp;{\text{mean of component }}i\\\sigma _{i=1\dots K}^{2}&amp;=&amp;{\text{variance of component }}i\\\mu _{0},\lambda ,\nu ,\sigma _{0}^{2}&amp;=&amp;{\text{shared hyperparameters}}\\\mu _{i=1\dots K}&amp;\sim &amp;{\mathcal {N}}(\mu _{0},\lambda \sigma _{i}^{2})\\\sigma _{i=1\dots K}^{2}&amp;\sim &amp;\operatorname {Inverse-Gamma} (\nu ,\sigma _{0}^{2})\\{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Symmetric-Dirichlet} _{K}(\beta )\\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\mathcal {N}}(\mu _{z_{i}},\sigma _{z_{i}}^{2})\end{array}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bc2297cfa94e7a3fdabb337a37719131632b7183" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -19.838ex; width:48.638ex; height:40.843ex;" alt="{\begin{array}{lcl}K,N&amp;=&amp;{\text{as above}}\\\phi _{i=1\dots K},{\boldsymbol {\phi }}&amp;=&amp;{\text{as above}}\\z_{i=1\dots N},x_{i=1\dots N}&amp;=&amp;{\text{as above}}\\\theta _{i=1\dots K}&amp;=&amp;\{\mu _{i=1\dots K},\sigma _{i=1\dots K}^{2}\}\\\mu _{i=1\dots K}&amp;=&amp;{\text{mean of component }}i\\\sigma _{i=1\dots K}^{2}&amp;=&amp;{\text{variance of component }}i\\\mu _{0},\lambda ,\nu ,\sigma _{0}^{2}&amp;=&amp;{\text{shared hyperparameters}}\\\mu _{i=1\dots K}&amp;\sim &amp;{\mathcal {N}}(\mu _{0},\lambda \sigma _{i}^{2})\\\sigma _{i=1\dots K}^{2}&amp;\sim &amp;\operatorname {Inverse-Gamma} (\nu ,\sigma _{0}^{2})\\{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Symmetric-Dirichlet} _{K}(\beta )\\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\mathcal {N}}(\mu _{z_{i}},\sigma _{z_{i}}^{2})\end{array}}"/></span></dd></dl>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><div id="mwe_player_0" class="PopUpMediaTransform" style="width:220px;" videopayload="&lt;div class=&quot;mediaContainer&quot; style=&quot;width:432px&quot;&gt;&lt;video id=&quot;mwe_player_1&quot; poster=&quot;//upload.wikimedia.org/wikipedia/commons/thumb/f/f5/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm/432px--Parameter_estimation_process_infinite_Gaussian_mixture_model.webm.jpg&quot; controls=&quot;&quot; preload=&quot;none&quot; autoplay=&quot;&quot; style=&quot;width:432px;height:288px&quot; class=&quot;kskin&quot; data-durationhint=&quot;50&quot; data-startoffset=&quot;0&quot; data-mwtitle=&quot;Parameter_estimation_process_infinite_Gaussian_mixture_model.webm&quot; data-mwprovider=&quot;wikimediacommons&quot;&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/f/f5/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm#t=0,00:00:49&quot; type=&quot;video/webm; codecs=&amp;quot;vp9&amp;quot;&quot; data-title=&quot;Original WebM file, 432 × 288 (20 kbps)&quot; data-shorttitle=&quot;WebM source&quot; data-width=&quot;432&quot; data-height=&quot;288&quot; data-bandwidth=&quot;19915&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/f/f5/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm.120p.vp9.webm#t=0,00:00:49&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;Lowest bandwidth VP9 (120P)&quot; data-shorttitle=&quot;VP9 120P&quot; data-transcodekey=&quot;120p.vp9.webm&quot; data-width=&quot;180&quot; data-height=&quot;120&quot; data-bandwidth=&quot;5832&quot; data-framerate=&quot;0&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/f/f5/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm.160p.webm#t=0,00:00:49&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;Low bandwidth WebM (160P)&quot; data-shorttitle=&quot;WebM 160P&quot; data-transcodekey=&quot;160p.webm&quot; data-width=&quot;240&quot; data-height=&quot;160&quot; data-bandwidth=&quot;21264&quot; data-framerate=&quot;0&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/f/f5/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm.180p.vp9.webm#t=0,00:00:49&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;Low bandwidth VP9 (180P)&quot; data-shorttitle=&quot;VP9 180P&quot; data-transcodekey=&quot;180p.vp9.webm&quot; data-width=&quot;270&quot; data-height=&quot;180&quot; data-bandwidth=&quot;7768&quot; data-framerate=&quot;0&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/f/f5/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm.240p.vp9.webm#t=0,00:00:49&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;Small VP9 (240P)&quot; data-shorttitle=&quot;VP9 240P&quot; data-transcodekey=&quot;240p.vp9.webm&quot; data-width=&quot;360&quot; data-height=&quot;240&quot; data-bandwidth=&quot;9536&quot; data-framerate=&quot;0&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/f/f5/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm.240p.webm#t=0,00:00:49&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;Small WebM (240P)&quot; data-shorttitle=&quot;WebM 240P&quot; data-transcodekey=&quot;240p.webm&quot; data-width=&quot;360&quot; data-height=&quot;240&quot; data-bandwidth=&quot;31480&quot; data-framerate=&quot;0&quot;/&gt;&lt;/video&gt;&lt;/div&gt;"><img alt="File:Parameter estimation process infinite Gaussian mixture model.webm" style="width:220px;height:147px" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/f5/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm/220px--Parameter_estimation_process_infinite_Gaussian_mixture_model.webm.jpg" /><a href="//upload.wikimedia.org/wikipedia/commons/f/f5/Parameter_estimation_process_infinite_Gaussian_mixture_model.webm" title="Play media" target="new"><span class="play-btn-large"><span class="mw-tmh-playtext">Play media</span></span></a></div>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Parameter_estimation_process_infinite_Gaussian_mixture_model.webm" class="internal" title="Enlarge"></a></div>Animation of the clustering process for one-dimensional data using a Bayesian Gaussian mixture model where normal distributions are drawn from a <a href="/wiki/Dirichlet_process" title="Dirichlet process">Dirichlet process</a>. The histograms of the clusters are shown in different colours. During the parameter estimation process, new clusters are created and grow on the data. The legend shows the cluster colours and the number of datapoints assigned to each cluster.</div></div></div>
<h4><span class="mw-headline" id="Multivariate_Gaussian_mixture_model">Multivariate Gaussian mixture model</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=5" title="Edit section: Multivariate Gaussian mixture model">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>A Bayesian Gaussian mixture model is commonly extended to fit a vector of unknown parameters (denoted in bold), or multivariate normal distributions.  In a multivariate distribution (i.e. one modelling a vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {x}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">x</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {x}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/606b7680d510560a505937143775ea80fa958051" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.532ex; height:1.676ex;" alt="{\boldsymbol {x}}"/></span> with <i>N</i> random variables) one may model a vector of parameters (such as several observations of a signal or patches within an image) using a Gaussian mixture model prior distribution on the vector of estimates given by
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle p({\boldsymbol {\theta }})=\sum _{i=1}^{K}\phi _{i}{\mathcal {N}}({\boldsymbol {\mu _{i},\Sigma _{i}}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>p</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>K</mi>
          </mrow>
        </munderover>
        <msub>
          <mi>&#x03D5;<!-- ϕ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi mathvariant="bold-italic">&#x03BC;<!-- μ --></mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold-italic">i</mi>
            </mrow>
          </msub>
          <mo mathvariant="bold">,</mo>
          <msub>
            <mi mathvariant="bold">&#x03A3;<!-- Σ --></mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold-italic">i</mi>
            </mrow>
          </msub>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p({\boldsymbol {\theta }})=\sum _{i=1}^{K}\phi _{i}{\mathcal {N}}({\boldsymbol {\mu _{i},\Sigma _{i}}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f13843df7f69545e27b06c4b59f1d8fe9690ce1" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; margin-left: -0.089ex; width:23.987ex; height:7.343ex;" alt="p({\boldsymbol {\theta }})=\sum _{i=1}^{K}\phi _{i}{\mathcal {N}}({\boldsymbol {\mu _{i},\Sigma _{i}}})"/></span></dd></dl>
<p>where the <i>i<sup>th</sup></i> vector component is characterized by normal distributions with weights <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \phi _{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>&#x03D5;<!-- ϕ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \phi _{i}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0182dbf29b54844c92fd9b0311778a02a38398ec" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.185ex; height:2.509ex;" alt="\phi _{i}"/></span>, means <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\mu _{i}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi mathvariant="bold-italic">&#x03BC;<!-- μ --></mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold-italic">i</mi>
            </mrow>
          </msub>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\mu _{i}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6017c4615b12b54442ca446414a5e292aec45ccf" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.544ex; height:2.009ex;" alt="{\boldsymbol {\mu _{i}}}"/></span> and covariance matrices <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\Sigma _{i}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi mathvariant="bold">&#x03A3;<!-- Σ --></mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold-italic">i</mi>
            </mrow>
          </msub>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\Sigma _{i}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f7b784fd01f800ebfcd4e97299f985ecb2704e0e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.829ex; height:2.509ex;" alt="{\boldsymbol {\Sigma _{i}}}"/></span>.  To incorporate this prior into a Bayesian estimation, the prior is multiplied with the known distribution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle p({\boldsymbol {x|\theta }})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>p</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">|</mo>
          </mrow>
          <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p({\boldsymbol {x|\theta }})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1234f137a9961b079f0347666d7b8e94d64d7385" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:6.553ex; height:2.843ex;" alt="p({\boldsymbol {x|\theta }})"/></span> of the data <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {x}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">x</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {x}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/606b7680d510560a505937143775ea80fa958051" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.532ex; height:1.676ex;" alt="{\boldsymbol {x}}"/></span> conditioned on the parameters <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\theta }}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\theta }}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/33b025a6bf54ec02e65c871dc3e5897c921419cf" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.306ex; height:2.176ex;" alt="{\boldsymbol {\theta }}"/></span> to be estimated.  With this formulation, the <a href="/wiki/Posterior_probability" title="Posterior probability">posterior distribution</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle p({\boldsymbol {\theta |x}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>p</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">|</mo>
          </mrow>
          <mi mathvariant="bold-italic">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p({\boldsymbol {\theta |x}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/69054073f2eb297b74a9260420160378e0f63ddb" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:6.553ex; height:2.843ex;" alt="p({\boldsymbol {\theta |x}})"/></span> is <i>also</i> a Gaussian mixture model of the form 
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle p({\boldsymbol {\theta |x}})=\sum _{i=1}^{K}{\tilde {\phi _{i}}}{\mathcal {N}}({\boldsymbol {{\tilde {\mu _{i}}},{\tilde {\Sigma _{i}}}}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>p</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">|</mo>
          </mrow>
          <mi mathvariant="bold-italic">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>K</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <msub>
                <mi>&#x03D5;<!-- ϕ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
              <mo stretchy="false">&#x007E;<!-- ~ --></mo>
            </mover>
          </mrow>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mover>
                <msub>
                  <mi mathvariant="bold-italic">&#x03BC;<!-- μ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="bold-italic">i</mi>
                  </mrow>
                </msub>
                <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>
              </mover>
            </mrow>
          </mrow>
          <mo mathvariant="bold">,</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mover>
                <msub>
                  <mi mathvariant="bold">&#x03A3;<!-- Σ --></mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="bold-italic">i</mi>
                  </mrow>
                </msub>
                <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>
              </mover>
            </mrow>
          </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p({\boldsymbol {\theta |x}})=\sum _{i=1}^{K}{\tilde {\phi _{i}}}{\mathcal {N}}({\boldsymbol {{\tilde {\mu _{i}}},{\tilde {\Sigma _{i}}}}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/219ce489c2b3b4edd99b7f0fa2d1cedfe1ec29ad" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; margin-left: -0.089ex; width:26.165ex; height:7.343ex;" alt="p({\boldsymbol {\theta |x}})=\sum _{i=1}^{K}{\tilde {\phi _{i}}}{\mathcal {N}}({\boldsymbol {{\tilde {\mu _{i}}},{\tilde {\Sigma _{i}}}}})"/></span></dd></dl>
<p>with new parameters <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\tilde {\phi _{i}}},{\boldsymbol {\tilde {\mu _{i}}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <msub>
                <mi>&#x03D5;<!-- ϕ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
              <mo stretchy="false">&#x007E;<!-- ~ --></mo>
            </mover>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <msub>
                <mi mathvariant="bold-italic">&#x03BC;<!-- μ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">i</mi>
                </mrow>
              </msub>
              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>
            </mover>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\tilde {\phi _{i}}},{\boldsymbol {\tilde {\mu _{i}}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/77e711d41a66ce4304a6a98e2868c761c417724f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:5.763ex; height:3.009ex;" alt="{\tilde {\phi _{i}}},{\boldsymbol {\tilde {\mu _{i}}}}"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\tilde {\Sigma _{i}}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <msub>
                <mi mathvariant="bold">&#x03A3;<!-- Σ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">i</mi>
                </mrow>
              </msub>
              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>
            </mover>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\tilde {\Sigma _{i}}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/63ea4596ed235ebd3b4af43360bc333d6a0594e0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.829ex; height:3.176ex;" alt="{\boldsymbol {\tilde {\Sigma _{i}}}}"/></span> that are updated using the <a href="/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">EM algorithm</a>.
<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> Although EM-based parameter updates are well-established, providing the initial estimates for these parameters is currently an area of active research.  Note that this formulation yields a closed-form solution to the complete posterior distribution.  Estimations of the random variable <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\theta }}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\theta }}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/33b025a6bf54ec02e65c871dc3e5897c921419cf" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.306ex; height:2.176ex;" alt="{\boldsymbol {\theta }}"/></span> may be obtained via one of several estimators, such as the mean or maximum of the posterior distribution.
</p><p>Such distributions are useful for assuming patch-wise shapes of images and clusters, for example.  In the case of image representation, each Gaussian may be tilted, expanded, and warped according to the covariance matrices <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\Sigma _{i}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi mathvariant="bold">&#x03A3;<!-- Σ --></mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold-italic">i</mi>
            </mrow>
          </msub>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\Sigma _{i}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f7b784fd01f800ebfcd4e97299f985ecb2704e0e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.829ex; height:2.509ex;" alt="{\boldsymbol {\Sigma _{i}}}"/></span>.  One Gaussian distribution of the set is fit to each patch (usually of size 8x8 pixels) in the image.  Notably, any distribution of points around a cluster (see <a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a>) may be accurately given enough Gaussian components, but scarcely over <i>K</i>=20 components are needed to accurately model a given image distribution or cluster of data.
</p>
<h4><span class="mw-headline" id="Categorical_mixture_model">Categorical mixture model</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=6" title="Edit section: Categorical mixture model">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<div class="thumb tright"><div class="thumbinner" style="width:252px;"><a href="/wiki/File:Nonbayesian-categorical-mixture.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Nonbayesian-categorical-mixture.svg/250px-Nonbayesian-categorical-mixture.svg.png" decoding="async" width="250" height="219" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Nonbayesian-categorical-mixture.svg/375px-Nonbayesian-categorical-mixture.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Nonbayesian-categorical-mixture.svg/500px-Nonbayesian-categorical-mixture.svg.png 2x" data-file-width="178" data-file-height="156" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Nonbayesian-categorical-mixture.svg" class="internal" title="Enlarge"></a></div>Non-Bayesian categorical mixture model using <a href="/wiki/Plate_notation" title="Plate notation">plate notation</a>.  Smaller squares indicate fixed parameters; larger circles indicate random variables.  Filled-in shapes indicate known values.  The indication [K] means a vector of size <i>K</i>; likewise for [V].</div></div></div>
<p>A typical non-Bayesian mixture model with <a href="/wiki/Categorical_distribution" title="Categorical distribution">categorical</a> observations looks like this:
</p>
<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle K,N:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
        <mo>,</mo>
        <mi>N</mi>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K,N:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/99458f5e86c26e5116bbd38b5785dd3281ae08e7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:6.456ex; height:2.509ex;" alt="K,N:"/></span> as above</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \phi _{i=1\dots K},{\boldsymbol {\phi }}:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>&#x03D5;<!-- ϕ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>K</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
        </mrow>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \phi _{i=1\dots K},{\boldsymbol {\phi }}:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2a1f5a04a02e7da2f8822778b7484ad4322b555b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:11.653ex; height:2.509ex;" alt="\phi _{i=1\dots K},{\boldsymbol {\phi }}:"/></span> as above</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle z_{i=1\dots N},x_{i=1\dots N}:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>z</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>N</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>N</mi>
          </mrow>
        </msub>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle z_{i=1\dots N},x_{i=1\dots N}:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/30917979248e96e82a84c435cc04612c1b224c31" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:17.307ex; height:2.009ex;" alt="z_{i=1\dots N},x_{i=1\dots N}:"/></span> as above</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>V</mi>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a6bf66ce0eadf19346f6c9b07f4dfab82b9d77dc" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:3.079ex; height:2.176ex;" alt="V:"/></span> dimension of categorical observations, e.g., size of word vocabulary</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \theta _{i=1\dots K,j=1\dots V}:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>K</mi>
            <mo>,</mo>
            <mi>j</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>V</mi>
          </mrow>
        </msub>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \theta _{i=1\dots K,j=1\dots V}:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7bef275f5676205a98adc208bb4b70cf8f8dc94d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:15.094ex; height:2.843ex;" alt="\theta _{i=1\dots K,j=1\dots V}:"/></span> probability for component <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle i}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;" alt="i"/></span> of observing item <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="j"/></span></li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\theta }}_{i=1\dots K}:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>K</mi>
          </mrow>
        </msub>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\theta }}_{i=1\dots K}:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0ee85fdb36bef2ee33bd0d093c84c548b125eb32" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:8.885ex; height:2.509ex;" alt="{\boldsymbol {\theta }}_{i=1\dots K}:"/></span> vector of dimension <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>V</mi>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V,}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ace9595e3ce66fdec7e9d30202626accd676b11e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.434ex; height:2.509ex;" alt="V,"/></span> composed of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \theta _{i,1\dots V};}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>,</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>V</mi>
          </mrow>
        </msub>
        <mo>;</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \theta _{i,1\dots V};}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ddea5657393f6bcf15a05e1b57cdb2359d10dea" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:7.006ex; height:2.843ex;" alt="\theta _{i,1\dots V};"/></span> must sum to 1</li></ul>
<p>The random variables:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\begin{array}{lcl}z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\text{Categorical}}({\boldsymbol {\theta }}_{z_{i}})\end{array}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="left center left" rowspacing="4pt" columnspacing="1em">
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mi>Categorical</mi>
                <mo>&#x2061;<!-- ⁡ --></mo>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>Categorical</mtext>
                </mrow>
                <mo stretchy="false">(</mo>
                <msub>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msub>
                      <mi>z</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>i</mi>
                      </mrow>
                    </msub>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{array}{lcl}z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\text{Categorical}}({\boldsymbol {\theta }}_{z_{i}})\end{array}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b5f72f9dc7972cb21c5e9c07c4326d4dc8e63004" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.671ex; width:31.06ex; height:6.509ex;" alt="{\begin{array}{lcl}z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\text{Categorical}}({\boldsymbol {\theta }}_{z_{i}})\end{array}}"/></span></dd></dl>
<p><br />
</p>
<div style="clear:both;"></div>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="/wiki/File:Bayesian-categorical-mixture.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Bayesian-categorical-mixture.svg/300px-Bayesian-categorical-mixture.svg.png" decoding="async" width="300" height="274" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Bayesian-categorical-mixture.svg/450px-Bayesian-categorical-mixture.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Bayesian-categorical-mixture.svg/600px-Bayesian-categorical-mixture.svg.png 2x" data-file-width="231" data-file-height="211" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Bayesian-categorical-mixture.svg" class="internal" title="Enlarge"></a></div>Bayesian categorical mixture model using <a href="/wiki/Plate_notation" title="Plate notation">plate notation</a>.  Smaller squares indicate fixed parameters; larger circles indicate random variables.  Filled-in shapes indicate known values.  The indication [K] means a vector of size <i>K</i>; likewise for [V].</div></div></div>
<p>A typical Bayesian mixture model with <a href="/wiki/Categorical_distribution" title="Categorical distribution">categorical</a> observations looks like this:
</p>
<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle K,N:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
        <mo>,</mo>
        <mi>N</mi>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K,N:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/99458f5e86c26e5116bbd38b5785dd3281ae08e7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:6.456ex; height:2.509ex;" alt="K,N:"/></span> as above</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \phi _{i=1\dots K},{\boldsymbol {\phi }}:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>&#x03D5;<!-- ϕ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>K</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
        </mrow>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \phi _{i=1\dots K},{\boldsymbol {\phi }}:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2a1f5a04a02e7da2f8822778b7484ad4322b555b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:11.653ex; height:2.509ex;" alt="\phi _{i=1\dots K},{\boldsymbol {\phi }}:"/></span> as above</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle z_{i=1\dots N},x_{i=1\dots N}:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>z</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>N</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>N</mi>
          </mrow>
        </msub>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle z_{i=1\dots N},x_{i=1\dots N}:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/30917979248e96e82a84c435cc04612c1b224c31" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:17.307ex; height:2.009ex;" alt="z_{i=1\dots N},x_{i=1\dots N}:"/></span> as above</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>V</mi>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a6bf66ce0eadf19346f6c9b07f4dfab82b9d77dc" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:3.079ex; height:2.176ex;" alt="V:"/></span> dimension of categorical observations, e.g., size of word vocabulary</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \theta _{i=1\dots K,j=1\dots V}:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>K</mi>
            <mo>,</mo>
            <mi>j</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>V</mi>
          </mrow>
        </msub>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \theta _{i=1\dots K,j=1\dots V}:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7bef275f5676205a98adc208bb4b70cf8f8dc94d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:15.094ex; height:2.843ex;" alt="\theta _{i=1\dots K,j=1\dots V}:"/></span> probability for component <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle i}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:0.802ex; height:2.176ex;" alt="i"/></span> of observing item <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="j"/></span></li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\theta }}_{i=1\dots K}:}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>K</mi>
          </mrow>
        </msub>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\theta }}_{i=1\dots K}:}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0ee85fdb36bef2ee33bd0d093c84c548b125eb32" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:8.885ex; height:2.509ex;" alt="{\boldsymbol {\theta }}_{i=1\dots K}:"/></span> vector of dimension <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>V</mi>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V,}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ace9595e3ce66fdec7e9d30202626accd676b11e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.434ex; height:2.509ex;" alt="V,"/></span> composed of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \theta _{i,1\dots V};}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>,</mo>
            <mn>1</mn>
            <mo>&#x2026;<!-- … --></mo>
            <mi>V</mi>
          </mrow>
        </msub>
        <mo>;</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \theta _{i,1\dots V};}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ddea5657393f6bcf15a05e1b57cdb2359d10dea" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:7.006ex; height:2.843ex;" alt="\theta _{i,1\dots V};"/></span> must sum to 1</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \alpha :}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03B1;<!-- α --></mi>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \alpha :}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3dc633c1cfa93fe91f681273fcebb66d206e66bd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.78ex; height:1.676ex;" alt="\alpha :"/></span> shared concentration hyperparameter of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\theta }}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\theta }}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/33b025a6bf54ec02e65c871dc3e5897c921419cf" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.306ex; height:2.176ex;" alt="{\boldsymbol {\theta }}"/></span> for each component</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \beta :}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03B2;<!-- β --></mi>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \beta :}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/97cc108ed0f06f749a1bb2fa78cfa626a7d5eb28" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.624ex; height:2.509ex;" alt="\beta :"/></span> concentration hyperparameter of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\phi }}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\phi }}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2162da9f005868f9129e85261efe96e619fffdab" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.655ex; height:2.509ex;" alt="{\boldsymbol {\phi }}"/></span></li></ul>
<p>The random variables:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\begin{array}{lcl}{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Symmetric-Dirichlet} _{K}(\beta )\\{\boldsymbol {\theta }}_{i=1\dots K}&amp;\sim &amp;{\text{Symmetric-Dirichlet}}_{V}(\alpha )\\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\text{Categorical}}({\boldsymbol {\theta }}_{z_{i}})\end{array}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="left center left" rowspacing="4pt" columnspacing="1em">
            <mtr>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <msub>
                  <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
                    <mi mathvariant="normal">S</mi>
                    <mi mathvariant="normal">y</mi>
                    <mi mathvariant="normal">m</mi>
                    <mi mathvariant="normal">m</mi>
                    <mi mathvariant="normal">e</mi>
                    <mi mathvariant="normal">t</mi>
                    <mi mathvariant="normal">r</mi>
                    <mi mathvariant="normal">i</mi>
                    <mi mathvariant="normal">c</mi>
                    <mtext>-</mtext>
                    <mi mathvariant="normal">D</mi>
                    <mi mathvariant="normal">i</mi>
                    <mi mathvariant="normal">r</mi>
                    <mi mathvariant="normal">i</mi>
                    <mi mathvariant="normal">c</mi>
                    <mi mathvariant="normal">h</mi>
                    <mi mathvariant="normal">l</mi>
                    <mi mathvariant="normal">e</mi>
                    <mi mathvariant="normal">t</mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>K</mi>
                  </mrow>
                </msub>
                <mo>&#x2061;<!-- ⁡ --></mo>
                <mo stretchy="false">(</mo>
                <mi>&#x03B2;<!-- β --></mi>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>K</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <msub>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>Symmetric-Dirichlet</mtext>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>V</mi>
                  </mrow>
                </msub>
                <mo stretchy="false">(</mo>
                <mi>&#x03B1;<!-- α --></mi>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mi>Categorical</mi>
                <mo>&#x2061;<!-- ⁡ --></mo>
                <mo stretchy="false">(</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold-italic">&#x03D5;<!-- ϕ --></mi>
                </mrow>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                    <mo>&#x2026;<!-- … --></mo>
                    <mi>N</mi>
                  </mrow>
                </msub>
              </mtd>
              <mtd>
                <mo>&#x223C;<!-- ∼ --></mo>
              </mtd>
              <mtd>
                <mrow class="MJX-TeXAtom-ORD">
                  <mtext>Categorical</mtext>
                </mrow>
                <mo stretchy="false">(</mo>
                <msub>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="bold-italic">&#x03B8;<!-- θ --></mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msub>
                      <mi>z</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>i</mi>
                      </mrow>
                    </msub>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\begin{array}{lcl}{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Symmetric-Dirichlet} _{K}(\beta )\\{\boldsymbol {\theta }}_{i=1\dots K}&amp;\sim &amp;{\text{Symmetric-Dirichlet}}_{V}(\alpha )\\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\text{Categorical}}({\boldsymbol {\theta }}_{z_{i}})\end{array}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4b703bb83590855b47bc8b9e4cbc8df0d20d4a6d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -6.005ex; width:40.238ex; height:13.176ex;" alt="{\begin{array}{lcl}{\boldsymbol {\phi }}&amp;\sim &amp;\operatorname {Symmetric-Dirichlet} _{K}(\beta )\\{\boldsymbol {\theta }}_{i=1\dots K}&amp;\sim &amp;{\text{Symmetric-Dirichlet}}_{V}(\alpha )\\z_{i=1\dots N}&amp;\sim &amp;\operatorname {Categorical} ({\boldsymbol {\phi }})\\x_{i=1\dots N}&amp;\sim &amp;{\text{Categorical}}({\boldsymbol {\theta }}_{z_{i}})\end{array}}"/></span></dd></dl>
<p><br />
</p>
<h2><span class="mw-headline" id="Examples">Examples</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=7" title="Edit section: Examples">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="A_financial_model">A financial model</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=8" title="Edit section: A financial model">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:252px;"><a href="/wiki/File:Normal_distribution_pdf.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/250px-Normal_distribution_pdf.png" decoding="async" width="250" height="188" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/375px-Normal_distribution_pdf.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Normal_distribution_pdf.png/500px-Normal_distribution_pdf.png 2x" data-file-width="1300" data-file-height="975" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Normal_distribution_pdf.png" class="internal" title="Enlarge"></a></div>The <a href="/wiki/Normal_distribution" title="Normal distribution">normal distribution</a> is plotted using different means and variances</div></div></div>
<p>Financial returns often behave differently in normal situations and during crisis times. A mixture model<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup> for return data seems reasonable. Sometimes the model used is a <a href="/wiki/Jump-diffusion_model" class="mw-redirect" title="Jump-diffusion model">jump-diffusion model</a>, or as a mixture of two normal distributions. See <a href="/wiki/Financial_economics#Challenges_and_criticism" title="Financial economics">Financial economics#Challenges and criticism</a> for further context.
</p>
<h3><span class="mw-headline" id="House_prices">House prices</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=9" title="Edit section: House prices">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Assume that we observe the prices of <i>N</i> different houses.  Different types of houses in different neighborhoods will have vastly different prices, but the price of a particular type of house in a particular neighborhood (e.g., three-bedroom house in moderately upscale neighborhood) will tend to cluster fairly closely around the mean.  One possible model of such prices would be to assume that the prices are accurately described by a mixture model with <i>K</i> different components, each distributed as a <a href="/wiki/Normal_distribution" title="Normal distribution">normal distribution</a> with unknown mean and variance, with each component specifying a particular combination of house type/neighborhood.  Fitting this model to observed prices, e.g., using the <a href="/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">expectation-maximization algorithm</a>, would tend to cluster the prices according to house type/neighborhood and reveal the spread of prices in each type/neighborhood. (Note that for values such as prices or incomes that are guaranteed to be positive and which tend to grow <a href="/wiki/Exponential_growth" title="Exponential growth">exponentially</a>, a <a href="/wiki/Log-normal_distribution" title="Log-normal distribution">log-normal distribution</a> might actually be a better model than a normal distribution.)
</p>
<h3><span class="mw-headline" id="Topics_in_a_document">Topics in a document</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=10" title="Edit section: Topics in a document">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Assume that a document is composed of <i>N</i> different words from a total vocabulary of size <i>V</i>, where each word corresponds to one of <i>K</i> possible topics.  The distribution of such words could be modelled as a mixture of <i>K</i> different <i>V</i>-dimensional <a href="/wiki/Categorical_distribution" title="Categorical distribution">categorical distributions</a>.  A model of this sort is commonly termed a <a href="/wiki/Topic_model" title="Topic model">topic model</a>.  Note that <a href="/wiki/Expectation_maximization" class="mw-redirect" title="Expectation maximization">expectation maximization</a> applied to such a model will typically fail to produce realistic results, due (among other things) to the <a href="/wiki/Overfitting" title="Overfitting">excessive number of parameters</a>.  Some sorts of additional assumptions are typically necessary to get good results.  Typically two sorts of additional components are added to the model:
</p>
<ol><li>A <a href="/wiki/Prior_distribution" class="mw-redirect" title="Prior distribution">prior distribution</a> is placed over the parameters describing the topic distributions, using a <a href="/wiki/Dirichlet_distribution" title="Dirichlet distribution">Dirichlet distribution</a> with a <a href="/wiki/Concentration_parameter" title="Concentration parameter">concentration parameter</a> that is set significantly below 1, so as to encourage sparse distributions (where only a small number of words have significantly non-zero probabilities).</li>
<li>Some sort of additional constraint is placed over the topic identities of words, to take advantage of natural clustering.</li></ol>
<dl><dd><ul><li>For example, a <a href="/wiki/Markov_chain" title="Markov chain">Markov chain</a> could be placed on the topic identities (i.e., the latent variables specifying the mixture component of each observation), corresponding to the fact that nearby words belong to similar topics. (This results in a <a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">hidden Markov model</a>, specifically one where a <a href="/wiki/Prior_distribution" class="mw-redirect" title="Prior distribution">prior distribution</a> is placed over state transitions that favors transitions that stay in the same state.)</li>
<li>Another possibility is the <a href="/wiki/Latent_Dirichlet_allocation" title="Latent Dirichlet allocation">latent Dirichlet allocation</a> model, which divides up the words into <i>D</i> different documents and assumes that in each document only a small number of topics occur with any frequency.</li></ul></dd></dl>
<h3><span class="mw-headline" id="Handwriting_recognition">Handwriting recognition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=11" title="Edit section: Handwriting recognition">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The following example is based on an example in <a href="/wiki/Christopher_M._Bishop" class="mw-redirect" title="Christopher M. Bishop">Christopher M. Bishop</a>, <i>Pattern Recognition and Machine Learning</i>.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup>
</p><p>Imagine that we are given an <i>N</i>×<i>N</i> black-and-white image that is known to be a scan of a hand-written digit between 0 and 9, but we don't know which digit is written.  We can create a mixture model with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle K=10}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
        <mo>=</mo>
        <mn>10</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K=10}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4f5caa2945733b3b3207347fdf814714cefce3fb" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:7.489ex; height:2.176ex;" alt="K=10"/></span> different components, where each component is a vector of size <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle N^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>N</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle N^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fe131b76af8a2bc86e01b14a7ba843db69c1a164" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:3.177ex; height:2.676ex;" alt="N^{2}"/></span> of <a href="/wiki/Bernoulli_distribution" title="Bernoulli distribution">Bernoulli distributions</a> (one per pixel).  Such a model can be trained with the <a href="/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">expectation-maximization algorithm</a> on an unlabeled set of hand-written digits, and will effectively cluster the images according to the digit being written.  The same model could then be used to recognize the digit of another image simply by holding the parameters constant, computing the probability of the new image for each possible digit (a trivial calculation), and returning the digit that generated the highest probability.
</p>
<h3><span id="Assessing_projectile_accuracy_.28a.k.a._circular_error_probable.2C_CEP.29"></span><span class="mw-headline" id="Assessing_projectile_accuracy_(a.k.a._circular_error_probable,_CEP)">Assessing projectile accuracy (a.k.a. circular error probable, CEP)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=12" title="Edit section: Assessing projectile accuracy (a.k.a. circular error probable, CEP)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Mixture models apply in the problem of directing multiple projectiles at a target (as in air, land, or sea defense applications), where the physical and/or statistical characteristics of the projectiles differ within the multiple projectiles. An example might be shots from multiple munitions types or shots from multiple locations directed at one target. The combination of projectile types may be characterized as a Gaussian mixture model.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup> Further, a well-known measure of accuracy for a group of projectiles is the <a href="/wiki/Circular_error_probable" title="Circular error probable">circular error probable</a> (CEP), which is the number <i>R</i> such that, on average, half of the group of projectiles falls within the circle of radius <i>R</i> about the target point. The mixture model can be used to determine (or estimate) the value <i>R</i>. The mixture model properly captures the different types of projectiles.
</p>
<h3><span class="mw-headline" id="Direct_and_indirect_applications">Direct and indirect applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=13" title="Edit section: Direct and indirect applications">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The financial example above is one direct application of the mixture model, a situation in which we assume an underlying mechanism so that each observation belongs to one of some number of different sources or categories. This underlying mechanism may or may not, however, be observable. In this form of mixture, each of the sources is described by a component probability density function, and its mixture weight is the probability that an observation comes from this component.
</p><p>In an indirect application of the mixture model we do not assume such a mechanism. The mixture model is simply used for its mathematical flexibilities. For example, a mixture of two <a href="/wiki/Normal_distribution" title="Normal distribution">normal distributions</a> with different means may result in a density with two <a href="/wiki/Mode_(statistics)" title="Mode (statistics)">modes</a>, which is not modeled by standard parametric distributions. Another example is given by the possibility of mixture distributions to model fatter tails than the basic Gaussian ones, so as to be a candidate for modeling more extreme events. When combined with <a href="/w/index.php?title=Dynamical_consistency&amp;action=edit&amp;redlink=1" class="new" title="Dynamical consistency (page does not exist)">dynamical consistency</a>, this approach has been applied to <a href="/wiki/Financial_derivatives" class="mw-redirect" title="Financial derivatives">financial derivatives</a> valuation in presence of the <a href="/wiki/Volatility_smile" title="Volatility smile">volatility smile</a> in the context of <a href="/wiki/Local_volatility" title="Local volatility">local volatility</a> models. This defines our application.
</p>
<h3><span class="mw-headline" id="Predictive_Maintenance">Predictive Maintenance</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=14" title="Edit section: Predictive Maintenance">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The mixture model-based clustering is also predominantly used in identifying the state of the machine in <a href="/wiki/Predictive_maintenance" title="Predictive maintenance">predictive maintenance</a>. Density plots are used to analyze the density of high dimensional features. If multi-model densities are observed, then it is assumed that a finite set of densities are formed by a finite set of normal mixtures. A multivariate Gaussian mixture model is used to cluster the feature data into k number of groups where k represents each state of the machine. The machine state can be a normal state, power off state, or faulty state.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup> Each formed cluster can be diagnosed using techniques such as spectral analysis. In the recent years, this has also been widely used in other areas such as early fault detection.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Fuzzy_image_segmentation">Fuzzy image segmentation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=15" title="Edit section: Fuzzy image segmentation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Movie.gif" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Movie.gif/220px-Movie.gif" decoding="async" width="220" height="165" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Movie.gif/330px-Movie.gif 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/64/Movie.gif/440px-Movie.gif 2x" data-file-width="1120" data-file-height="840" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Movie.gif" class="internal" title="Enlarge"></a></div>An example of Gaussian Mixture in image segmentation with grey histogram</div></div></div>
<p>In image processing and computer vision, traditional <a href="/wiki/Image_segmentation" title="Image segmentation">image segmentation</a> models often assign to one <a href="/wiki/Pixel" title="Pixel">pixel</a> only one exclusive pattern. In fuzzy or soft segmentation, any pattern can have certain "ownership" over any single pixel. If the patterns are Gaussian, fuzzy segmentation naturally results in Gaussian mixtures. Combined with other analytic or geometric tools (e.g., phase transitions over diffusive boundaries), such spatially regularized mixture models could lead to more realistic and computationally efficient segmentation methods.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Point_set_registration">Point set registration</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=16" title="Edit section: Point set registration">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Probabilistic mixture models such as <a href="/wiki/Gaussian_mixture_model" class="mw-redirect" title="Gaussian mixture model">Gaussian mixture models</a> (GMM) are used to resolve <a href="/wiki/Point_set_registration" title="Point set registration">point set registration</a> problems in image processing and computer vision fields. For pair-wise <a href="/wiki/Point_set_registration" title="Point set registration">point set registration</a>, one point set is regarded as the centroids of mixture models, and the other point set is regarded as data points (observations). State-of-the-art methods are e.g. <a href="/wiki/Point_set_registration#Point_set_registration_algorithms#Coherent_point_drift" title="Point set registration">coherent point drift</a> (CPD)<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup> 
and <a href="/wiki/Student%27s_t-distribution" title="Student&#39;s t-distribution">Student's t-distribution</a> mixture models (TMM).<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup> 
The result of recent research demonstrate the superiorty of hybrid mixture models<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup> 
(e.g. combining Student's t-Distritubtion and Watson distribution/<a href="/wiki/Bingham_distribution" title="Bingham distribution">Bingham distribution</a> to model spatial positions and axes orientations separately) compare to CPD and TMM, in terms of inherent robustness, accuracy and discriminative capacity.
</p>
<h2><span class="mw-headline" id="Identifiability">Identifiability</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=17" title="Edit section: Identifiability">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Identifiability refers to the existence of a unique characterization for any one of the models in the class (family) being considered. Estimation procedures may not be well-defined and asymptotic theory may not hold if a model is not identifiable.
</p>
<h3><span class="mw-headline" id="Example">Example</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=18" title="Edit section: Example">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Let <i>J</i> be the class of all binomial distributions with <span class="nowrap"><i>n</i> = 2</span>. Then a mixture of two members of <i>J</i> would have
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle p_{0}=\pi (1-\theta _{1})^{2}+(1-\pi )(1-\theta _{2})^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>0</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <mi>&#x03C0;<!-- π --></mi>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mi>&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>+</mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <mi>&#x03C0;<!-- π --></mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mi>&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p_{0}=\pi (1-\theta _{1})^{2}+(1-\pi )(1-\theta _{2})^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0df30123ed812d4807bd9df463c0c3ff79be6e1e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:34.75ex; height:3.176ex;" alt="p_{0}=\pi (1-\theta _{1})^{2}+(1-\pi )(1-\theta _{2})^{2}"/></span></dd>
<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle p_{1}=2\pi \theta _{1}(1-\theta _{1})+2(1-\pi )\theta _{2}(1-\theta _{2})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>=</mo>
        <mn>2</mn>
        <mi>&#x03C0;<!-- π --></mi>
        <msub>
          <mi>&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mi>&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mn>2</mn>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <mi>&#x03C0;<!-- π --></mi>
        <mo stretchy="false">)</mo>
        <msub>
          <mi>&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mi>&#x03B8;<!-- θ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p_{1}=2\pi \theta _{1}(1-\theta _{1})+2(1-\pi )\theta _{2}(1-\theta _{2})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/30b87e1545253b72011f46a70b35d651a6e92eea" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:39.256ex; height:2.843ex;" alt="p_{1}=2\pi \theta _{1}(1-\theta _{1})+2(1-\pi )\theta _{2}(1-\theta _{2})"/></span></dd></dl>
<p>and <span class="nowrap"><i>p</i><sub>2</sub> = 1 − <i>p</i><sub>0</sub> − <i>p</i><sub>1</sub></span>. Clearly, given <i>p</i><sub>0</sub> and <i>p</i><sub>1</sub>, it is not possible to determine the above mixture model uniquely, as there are three parameters (<i>π</i>, <i>θ</i><sub>1</sub>, <i>θ</i><sub>2</sub>) to be determined.
</p>
<h3><span class="mw-headline" id="Definition">Definition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=19" title="Edit section: Definition">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Consider a mixture of parametric distributions of the same class. Let
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle J=\{f(\cdot ;\theta ):\theta \in \Omega \}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>J</mi>
        <mo>=</mo>
        <mo fence="false" stretchy="false">{</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo>;</mo>
        <mi>&#x03B8;<!-- θ --></mi>
        <mo stretchy="false">)</mo>
        <mo>:</mo>
        <mi>&#x03B8;<!-- θ --></mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi mathvariant="normal">&#x03A9;<!-- Ω --></mi>
        <mo fence="false" stretchy="false">}</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle J=\{f(\cdot ;\theta ):\theta \in \Omega \}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d0f03a42f1db94c520d3fbc66e583beed2a363be" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:20.3ex; height:2.843ex;" alt="J=\{f(\cdot ;\theta ):\theta \in \Omega \}"/></span></dd></dl>
<p>be the class of all component distributions. Then the <a href="/wiki/Convex_hull" title="Convex hull">convex hull</a> <i>K</i> of <i>J</i> defines the class of all finite mixture of distributions in <i>J</i>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle K=\left\{p(\cdot ):p(\cdot )=\sum _{i=1}^{n}a_{i}f_{i}(\cdot ;\theta _{i}),a_{i}&gt;0,\sum _{i=1}^{n}a_{i}=1,f_{i}(\cdot ;\theta _{i})\in J\ \forall i,n\right\}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
        <mo>=</mo>
        <mrow>
          <mo>{</mo>
          <mrow>
            <mi>p</mi>
            <mo stretchy="false">(</mo>
            <mo>&#x22C5;<!-- ⋅ --></mo>
            <mo stretchy="false">)</mo>
            <mo>:</mo>
            <mi>p</mi>
            <mo stretchy="false">(</mo>
            <mo>&#x22C5;<!-- ⋅ --></mo>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>n</mi>
              </mrow>
            </munderover>
            <msub>
              <mi>a</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <msub>
              <mi>f</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo stretchy="false">(</mo>
            <mo>&#x22C5;<!-- ⋅ --></mo>
            <mo>;</mo>
            <msub>
              <mi>&#x03B8;<!-- θ --></mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo stretchy="false">)</mo>
            <mo>,</mo>
            <msub>
              <mi>a</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo>&gt;</mo>
            <mn>0</mn>
            <mo>,</mo>
            <munderover>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>n</mi>
              </mrow>
            </munderover>
            <msub>
              <mi>a</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo>=</mo>
            <mn>1</mn>
            <mo>,</mo>
            <msub>
              <mi>f</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo stretchy="false">(</mo>
            <mo>&#x22C5;<!-- ⋅ --></mo>
            <mo>;</mo>
            <msub>
              <mi>&#x03B8;<!-- θ --></mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo stretchy="false">)</mo>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>J</mi>
            <mtext>&#xA0;</mtext>
            <mi mathvariant="normal">&#x2200;<!-- ∀ --></mi>
            <mi>i</mi>
            <mo>,</mo>
            <mi>n</mi>
          </mrow>
          <mo>}</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K=\left\{p(\cdot ):p(\cdot )=\sum _{i=1}^{n}a_{i}f_{i}(\cdot ;\theta _{i}),a_{i}&gt;0,\sum _{i=1}^{n}a_{i}=1,f_{i}(\cdot ;\theta _{i})\in J\ \forall i,n\right\}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c31c1445ef803dab36338850f095f2b655129692" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.171ex; width:70.448ex; height:7.509ex;" alt="K=\left\{p(\cdot ):p(\cdot )=\sum _{i=1}^{n}a_{i}f_{i}(\cdot ;\theta _{i}),a_{i}&gt;0,\sum _{i=1}^{n}a_{i}=1,f_{i}(\cdot ;\theta _{i})\in J\ \forall i,n\right\}"/></span></dd></dl>
<p><i>K</i> is said to be identifiable if all its members are unique, that is, given two members <i>p</i> and <span class="nowrap"><i>p′</i></span> in <i>K</i>, being mixtures of <i>k</i> distributions and <span class="nowrap"><i>k′</i></span> distributions respectively in <i>J</i>, we have <span class="nowrap"><i>p = p′</i></span> if and only if, first of all, <span class="nowrap"><i>k = k′</i></span> and secondly we can reorder the summations such that <span class="nowrap"><i>a<sub>i</sub> = a<sub>i</sub></i>′</span> and <span class="nowrap"><i>ƒ<sub>i</sub> = ƒ<sub>i</sub></i>′</span> for all <i>i</i>.
</p>
<h2><span class="mw-headline" id="Parameter_estimation_and_system_identification">Parameter estimation and system identification</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=20" title="Edit section: Parameter estimation and system identification">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Parametric mixture models are often used when we know the distribution <i>Y</i> and we can sample from <i>X</i>, but we would like to determine the <i>a<sub>i</sub></i> and <i>θ<sub>i</sub></i> values.  Such situations can arise in studies in which we sample from a population that is composed of several distinct subpopulations.
</p><p>It is common to think of probability mixture modeling as a missing data problem.  One way to understand this is to assume that the data points under consideration have "membership" in one of the distributions we are using to model the data.  When we start, this membership is unknown, or missing.  The job of estimation is to devise appropriate parameters for the model functions we choose, with the connection to the data points being represented as their membership in the individual model distributions.
</p><p>A variety of approaches to the problem of mixture decomposition have been proposed, many of which focus on maximum likelihood methods such as <a href="/wiki/Expectation_maximization" class="mw-redirect" title="Expectation maximization">expectation maximization</a> (EM) or maximum <i>a posteriori</i> estimation (MAP).  Generally these methods consider separately the questions of system identification and parameter estimation; methods to determine the number and functional form of components within a mixture are distinguished from methods to estimate the corresponding parameter values.  Some notable departures are the graphical methods as outlined in Tarter and Lock<sup id="cite_ref-tart_12-0" class="reference"><a href="#cite_note-tart-12">&#91;12&#93;</a></sup> and more recently <a href="/wiki/Minimum_message_length" title="Minimum message length">minimum message length</a> (MML) techniques such as Figueiredo and Jain<sup id="cite_ref-Jain_13-0" class="reference"><a href="#cite_note-Jain-13">&#91;13&#93;</a></sup> and to some extent the moment matching pattern analysis routines suggested by McWilliam and Loh (2009).<sup id="cite_ref-mcwilli_14-0" class="reference"><a href="#cite_note-mcwilli-14">&#91;14&#93;</a></sup>
</p>
<h3><span id="Expectation_maximization_.28EM.29"></span><span class="mw-headline" id="Expectation_maximization_(EM)">Expectation maximization (EM)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=21" title="Edit section: Expectation maximization (EM)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">Expectation maximization</a> (EM) is seemingly the most popular technique used to determine the parameters of a mixture with an <i>a priori</i> given number of components. This is a particular way of implementing <a href="/wiki/Maximum_likelihood" class="mw-redirect" title="Maximum likelihood">maximum likelihood</a> estimation for this problem. EM is of particular appeal for finite normal mixtures where closed-form expressions are possible such as in the following iterative algorithm by Dempster <i>et al.</i> (1977)<sup id="cite_ref-dempster1977_15-0" class="reference"><a href="#cite_note-dempster1977-15">&#91;15&#93;</a></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w_{s}^{(j+1)}={\frac {1}{N}}\sum _{t=1}^{N}h_{s}^{(j)}(t)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>s</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>j</mi>
            <mo>+</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mi>N</mi>
          </mfrac>
        </mrow>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>N</mi>
          </mrow>
        </munderover>
        <msubsup>
          <mi>h</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>s</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>j</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{s}^{(j+1)}={\frac {1}{N}}\sum _{t=1}^{N}h_{s}^{(j)}(t)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e3b3594cc4e64e49c4ba357e9669fb44c3076af5" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:22.258ex; height:7.343ex;" alt="w_{s}^{(j+1)}={\frac {1}{N}}\sum _{t=1}^{N}h_{s}^{(j)}(t)"/></span></dd>
<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mu _{s}^{(j+1)}={\frac {\sum _{t=1}^{N}h_{s}^{(j)}(t)x^{(t)}}{\sum _{t=1}^{N}h_{s}^{(j)}(t)}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>&#x03BC;<!-- μ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>s</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>j</mi>
            <mo>+</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <munderover>
                <mo>&#x2211;<!-- ∑ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>t</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>N</mi>
                </mrow>
              </munderover>
              <msubsup>
                <mi>h</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>s</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mo stretchy="false">(</mo>
              <mi>t</mi>
              <mo stretchy="false">)</mo>
              <msup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>t</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
            </mrow>
            <mrow>
              <munderover>
                <mo>&#x2211;<!-- ∑ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>t</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>N</mi>
                </mrow>
              </munderover>
              <msubsup>
                <mi>h</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>s</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mo stretchy="false">(</mo>
              <mi>t</mi>
              <mo stretchy="false">)</mo>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mu _{s}^{(j+1)}={\frac {\sum _{t=1}^{N}h_{s}^{(j)}(t)x^{(t)}}{\sum _{t=1}^{N}h_{s}^{(j)}(t)}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3aeda49e95d7437ef5798fe9e1edd14ffaa404a0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.505ex; width:25.005ex; height:8.176ex;" alt="\mu _{s}^{(j+1)}={\frac {\sum _{t=1}^{N}h_{s}^{(j)}(t)x^{(t)}}{\sum _{t=1}^{N}h_{s}^{(j)}(t)}}"/></span></dd>
<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \Sigma _{s}^{(j+1)}={\frac {\sum _{t=1}^{N}h_{s}^{(j)}(t)[x^{(t)}-\mu _{s}^{(j+1)}][x^{(t)}-\mu _{s}^{(j+1)}]^{\top }}{\sum _{t=1}^{N}h_{s}^{(j)}(t)}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>s</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>j</mi>
            <mo>+</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <munderover>
                <mo>&#x2211;<!-- ∑ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>t</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>N</mi>
                </mrow>
              </munderover>
              <msubsup>
                <mi>h</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>s</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mo stretchy="false">(</mo>
              <mi>t</mi>
              <mo stretchy="false">)</mo>
              <mo stretchy="false">[</mo>
              <msup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>t</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>&#x2212;<!-- − --></mo>
              <msubsup>
                <mi>&#x03BC;<!-- μ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>s</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mo stretchy="false">]</mo>
              <mo stretchy="false">[</mo>
              <msup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>t</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>&#x2212;<!-- − --></mo>
              <msubsup>
                <mi>&#x03BC;<!-- μ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>s</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <msup>
                <mo stretchy="false">]</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="normal">&#x22A4;<!-- ⊤ --></mi>
                </mrow>
              </msup>
            </mrow>
            <mrow>
              <munderover>
                <mo>&#x2211;<!-- ∑ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>t</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>N</mi>
                </mrow>
              </munderover>
              <msubsup>
                <mi>h</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>s</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mo stretchy="false">(</mo>
              <mi>t</mi>
              <mo stretchy="false">)</mo>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Sigma _{s}^{(j+1)}={\frac {\sum _{t=1}^{N}h_{s}^{(j)}(t)[x^{(t)}-\mu _{s}^{(j+1)}][x^{(t)}-\mu _{s}^{(j+1)}]^{\top }}{\sum _{t=1}^{N}h_{s}^{(j)}(t)}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0198bb195ea56a45f53c21b0c1f4322c126063d4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.505ex; width:49.878ex; height:8.176ex;" alt="\Sigma _{s}^{(j+1)}={\frac {\sum _{t=1}^{N}h_{s}^{(j)}(t)[x^{(t)}-\mu _{s}^{(j+1)}][x^{(t)}-\mu _{s}^{(j+1)}]^{\top }}{\sum _{t=1}^{N}h_{s}^{(j)}(t)}}"/></span></dd></dl>
<p>with the posterior probabilities
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle h_{s}^{(j)}(t)={\frac {w_{s}^{(j)}p_{s}(x^{(t)};\mu _{s}^{(j)},\Sigma _{s}^{(j)})}{\sum _{i=1}^{n}w_{i}^{(j)}p_{i}(x^{(t)};\mu _{i}^{(j)},\Sigma _{i}^{(j)})}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>h</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>s</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>j</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <msubsup>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>s</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <msub>
                <mi>p</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>s</mi>
                </mrow>
              </msub>
              <mo stretchy="false">(</mo>
              <msup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>t</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>;</mo>
              <msubsup>
                <mi>&#x03BC;<!-- μ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>s</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mo>,</mo>
              <msubsup>
                <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>s</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mo stretchy="false">)</mo>
            </mrow>
            <mrow>
              <munderover>
                <mo>&#x2211;<!-- ∑ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>n</mi>
                </mrow>
              </munderover>
              <msubsup>
                <mi>w</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <msub>
                <mi>p</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
              <mo stretchy="false">(</mo>
              <msup>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>t</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>;</mo>
              <msubsup>
                <mi>&#x03BC;<!-- μ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mo>,</mo>
              <msubsup>
                <mi mathvariant="normal">&#x03A3;<!-- Σ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>j</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mo stretchy="false">)</mo>
            </mrow>
          </mfrac>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle h_{s}^{(j)}(t)={\frac {w_{s}^{(j)}p_{s}(x^{(t)};\mu _{s}^{(j)},\Sigma _{s}^{(j)})}{\sum _{i=1}^{n}w_{i}^{(j)}p_{i}(x^{(t)};\mu _{i}^{(j)},\Sigma _{i}^{(j)})}}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6348a43581944bc3f4c527fd3aea50ec25317e47" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.505ex; width:37.092ex; height:8.009ex;" alt="h_{s}^{(j)}(t)={\frac {w_{s}^{(j)}p_{s}(x^{(t)};\mu _{s}^{(j)},\Sigma _{s}^{(j)})}{\sum _{i=1}^{n}w_{i}^{(j)}p_{i}(x^{(t)};\mu _{i}^{(j)},\Sigma _{i}^{(j)})}}."/></span></dd></dl>
<p>Thus on the basis of the current estimate for the parameters, the <a href="/wiki/Conditional_probability" title="Conditional probability">conditional probability</a> for a given observation <i>x</i><sup>(<i>t</i>)</sup> being generated from state <i>s</i> is determined for each <span class="nowrap"><i>t</i> = 1, …, <i>N</i></span>&#160;; <i>N</i> being the sample size.  The parameters are then updated such that the new component weights correspond to the average conditional probability and each component mean and covariance is the component specific weighted average of the mean and covariance of the entire sample.
</p><p>Dempster<sup id="cite_ref-dempster1977_15-1" class="reference"><a href="#cite_note-dempster1977-15">&#91;15&#93;</a></sup> also showed that each successive EM iteration will not decrease the likelihood, a property not shared by other gradient based maximization techniques.  Moreover, EM naturally embeds within it constraints on the probability vector, and for sufficiently large sample sizes positive definiteness of the covariance iterates.  This is a key advantage since explicitly constrained methods incur extra computational costs to check and maintain appropriate values. Theoretically EM is a first-order algorithm and as such converges slowly to a fixed-point solution. Redner and Walker (1984)<sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citing_sources#What_information_to_include" title="Wikipedia:Citing sources"><span title="A complete citation is needed (November 2012)">full citation needed</span></a></i>&#93;</sup> make this point arguing in favour of superlinear and second order Newton and quasi-Newton methods and reporting slow convergence in EM on the basis of their empirical tests.  They do concede that convergence in likelihood was rapid even if convergence in the parameter values themselves was not.  The relative merits of EM and other algorithms vis-à-vis convergence have been discussed in other literature.<sup id="cite_ref-XuJordam_16-0" class="reference"><a href="#cite_note-XuJordam-16">&#91;16&#93;</a></sup>
</p><p>Other common objections to the use of EM are that it has a propensity to spuriously identify local maxima, as well as displaying sensitivity to initial values.<sup id="cite_ref-McLachlan_2_17-0" class="reference"><a href="#cite_note-McLachlan_2-17">&#91;17&#93;</a></sup><sup id="cite_ref-botev2004global_18-0" class="reference"><a href="#cite_note-botev2004global-18">&#91;18&#93;</a></sup> One may address these problems by evaluating EM at several initial points in the parameter space but this is computationally costly and other approaches, such as the annealing EM method of Udea and Nakano (1998) (in which the initial components are essentially forced to overlap, providing a less heterogeneous basis for initial guesses), may be preferable.
</p><p>Figueiredo and Jain<sup id="cite_ref-Jain_13-1" class="reference"><a href="#cite_note-Jain-13">&#91;13&#93;</a></sup> note that convergence to 'meaningless' parameter values obtained at the boundary (where regularity conditions breakdown, e.g., Ghosh and Sen (1985)) is frequently observed when the number of model components exceeds the optimal/true one.  On this basis they suggest a unified approach to estimation and identification in which the initial <i>n</i> is chosen to greatly exceed the expected optimal value.  Their optimization routine is constructed via a minimum message length (MML) criterion that effectively eliminates a candidate component if there is insufficient information to support it. In this way it is possible to systematize reductions in <i>n</i> and consider estimation and identification jointly.
</p><p>The <a href="/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">Expectation-maximization algorithm</a> can be used to compute the parameters of a parametric mixture model distribution (the <i>a<sub>i</sub></i> and <i>θ<sub>i</sub></i>).  It is an <a href="/wiki/Iterative_algorithm" class="mw-redirect" title="Iterative algorithm">iterative algorithm</a> with two steps: an <i>expectation step</i> and a <i>maximization step</i>. <a rel="nofollow" class="external text" href="http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture">Practical examples of EM and Mixture Modeling</a> are included in the <a href="/wiki/SOCR" class="mw-redirect" title="SOCR">SOCR</a> demonstrations.
</p>
<h4><span class="mw-headline" id="The_expectation_step">The expectation step</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=22" title="Edit section: The expectation step">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>With initial guesses for the parameters of our mixture model, "partial membership" of each data point in each constituent distribution is computed by calculating <a href="/wiki/Expectation_value" class="mw-redirect" title="Expectation value">expectation values</a> for the membership variables of each data point.  That is, for each data point <i>x<sub>j</sub></i> and distribution <i>Y<sub>i</sub></i>, the membership value <i>y</i><sub><i>i</i>, <i>j</i></sub> is:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle y_{i,j}={\frac {a_{i}f_{Y}(x_{j};\theta _{i})}{f_{X}(x_{j})}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>,</mo>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <msub>
                <mi>a</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
              <msub>
                <mi>f</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>Y</mi>
                </mrow>
              </msub>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo>;</mo>
              <msub>
                <mi>&#x03B8;<!-- θ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
            </mrow>
            <mrow>
              <msub>
                <mi>f</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>X</mi>
                </mrow>
              </msub>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
            </mrow>
          </mfrac>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle y_{i,j}={\frac {a_{i}f_{Y}(x_{j};\theta _{i})}{f_{X}(x_{j})}}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/35539145d63931b0d991dde2e447573bdbf6f091" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.671ex; width:19.283ex; height:6.509ex;" alt="y_{i,j}={\frac {a_{i}f_{Y}(x_{j};\theta _{i})}{f_{X}(x_{j})}}."/></span></dd></dl>
<h4><span class="mw-headline" id="The_maximization_step">The maximization step</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=23" title="Edit section: The maximization step">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>With expectation values in hand for group membership, <a href="/w/index.php?title=Plug-in_estimates&amp;action=edit&amp;redlink=1" class="new" title="Plug-in estimates (page does not exist)">plug-in estimates</a> are recomputed for the distribution parameters.
</p><p>The mixing coefficients <i>a<sub>i</sub></i> are the <a href="/wiki/Arithmetic_mean" title="Arithmetic mean">means</a> of the membership values over the <i>N</i> data points.
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle a_{i}={\frac {1}{N}}\sum _{j=1}^{N}y_{i,j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>a</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mi>N</mi>
          </mfrac>
        </mrow>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>N</mi>
          </mrow>
        </munderover>
        <msub>
          <mi>y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>,</mo>
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle a_{i}={\frac {1}{N}}\sum _{j=1}^{N}y_{i,j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/eb54eeccd6253130fbb4ed09048f427d7c045553" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:15.231ex; height:7.676ex;" alt="a_{i}={\frac {1}{N}}\sum _{j=1}^{N}y_{i,j}"/></span></dd></dl>
<p>The component model parameters <i>θ<sub>i</sub></i> are also calculated by expectation maximization using data points <i>x<sub>j</sub></i> that have been weighted using the membership values.  For example, if <i>θ</i> is a mean <i>μ</i>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mu _{i}={\frac {\sum _{j}y_{i,j}x_{j}}{\sum _{j}y_{i,j}}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>&#x03BC;<!-- μ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <munder>
                <mo>&#x2211;<!-- ∑ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </munder>
              <msub>
                <mi>y</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mo>,</mo>
                  <mi>j</mi>
                </mrow>
              </msub>
              <msub>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <munder>
                <mo>&#x2211;<!-- ∑ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </munder>
              <msub>
                <mi>y</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mo>,</mo>
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mu _{i}={\frac {\sum _{j}y_{i,j}x_{j}}{\sum _{j}y_{i,j}}}.}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7b854c575da4da2c5334f5d67c6c6b911d3e6f3f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:15.847ex; height:7.176ex;" alt="\mu _{i}={\frac {\sum _{j}y_{i,j}x_{j}}{\sum _{j}y_{i,j}}}."/></span></dd></dl>
<p>With new estimates for <i>a<sub>i</sub></i> and the <i>θ<sub>i</sub>'</i>s, the expectation step is repeated to recompute new membership values.  The entire procedure is repeated until model parameters converge.
</p>
<h3><span class="mw-headline" id="Markov_chain_Monte_Carlo">Markov chain Monte Carlo</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=24" title="Edit section: Markov chain Monte Carlo">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>As an alternative to the EM algorithm, the mixture model parameters can be deduced using <a href="/w/index.php?title=Posterior_sampling&amp;action=edit&amp;redlink=1" class="new" title="Posterior sampling (page does not exist)">posterior sampling</a> as indicated by <a href="/wiki/Bayes%27_theorem" title="Bayes&#39; theorem">Bayes' theorem</a>.  This is still regarded as an incomplete data problem whereby membership of data points is the missing data.  A two-step iterative procedure known as <a href="/wiki/Gibbs_sampling" title="Gibbs sampling">Gibbs sampling</a> can be used.
</p><p>The previous example of a mixture of two <a href="/wiki/Gaussian_distribution" class="mw-redirect" title="Gaussian distribution">Gaussian distributions</a> can demonstrate how the method works.  As before, initial guesses of the parameters for the mixture model are made.  Instead of computing partial memberships for each elemental distribution, a membership value for each data point is drawn from a <a href="/wiki/Bernoulli_distribution" title="Bernoulli distribution">Bernoulli distribution</a> (that is, it will be assigned to either the first or the second Gaussian).  The Bernoulli parameter <i>θ</i> is determined for each data point on the basis of one of the constituent distributions.<sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Vagueness" title="Wikipedia:Vagueness"><span title="This information is too vague. (March 2008)">vague</span></a></i>&#93;</sup>  Draws from the distribution generate membership associations for each data point.  Plug-in estimators can then be used as in the M step of EM to generate a new set of mixture model parameters, and the binomial draw step repeated.
</p>
<h3><span class="mw-headline" id="Moment_matching">Moment matching</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=25" title="Edit section: Moment matching">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <a href="/wiki/Method_of_moments_(statistics)" title="Method of moments (statistics)">method of moment matching</a> is one of the oldest techniques for determining the mixture parameters dating back to Karl Pearson's seminal work of 1894.
In this approach the parameters of the mixture are determined such that the composite distribution has moments matching some given value.  In many instances extraction of solutions to the moment equations may present non-trivial algebraic or computational problems.  Moreover, numerical analysis by Day<sup id="cite_ref-day_19-0" class="reference"><a href="#cite_note-day-19">&#91;19&#93;</a></sup> has indicated that such methods may be inefficient compared to EM. Nonetheless there has been renewed interest in this method, e.g., Craigmile and Titterington (1998) and Wang.<sup id="cite_ref-wang_20-0" class="reference"><a href="#cite_note-wang-20">&#91;20&#93;</a></sup>
</p><p>McWilliam and Loh (2009) consider the characterisation of a hyper-cuboid normal mixture <a href="/wiki/Copula_(statistics)" class="mw-redirect" title="Copula (statistics)">copula</a> in large dimensional systems for which EM would be computationally prohibitive.  Here a pattern analysis routine is used to generate multivariate tail-dependencies consistent with a set of univariate and (in some sense) bivariate moments.  The performance of this method is then evaluated using equity log-return data with <a href="/wiki/Kolmogorov%E2%80%93Smirnov" class="mw-redirect" title="Kolmogorov–Smirnov">Kolmogorov–Smirnov</a> test statistics suggesting a good descriptive fit.
</p>
<h3><span class="mw-headline" id="Spectral_method">Spectral method</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=26" title="Edit section: Spectral method">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Some problems in mixture model estimation can be solved using <a href="/wiki/Spectral_method" title="Spectral method">spectral methods</a>.
In particular it becomes useful if data points <i>x<sub>i</sub></i> are points in high-dimensional <a href="/wiki/Real_coordinate_space" title="Real coordinate space">real space</a>, and the hidden distributions are known to be <a href="/wiki/Logarithmically_concave_function" title="Logarithmically concave function">log-concave</a> (such as <a href="/wiki/Gaussian_distribution" class="mw-redirect" title="Gaussian distribution">Gaussian distribution</a> or <a href="/wiki/Exponential_distribution" title="Exponential distribution">Exponential distribution</a>).
</p><p>Spectral methods of learning mixture models are based on the use of <a href="/wiki/Singular_Value_Decomposition" class="mw-redirect" title="Singular Value Decomposition">Singular Value Decomposition</a> of a matrix which contains data points.
The idea is to consider the top <i>k</i> singular vectors, where <i>k</i> is the number of distributions to be learned. The projection
of each data point to a <a href="/wiki/Linear_subspace" title="Linear subspace">linear subspace</a> spanned by those vectors groups points originating from the same distribution
very close together, while points from different distributions stay far apart.
</p><p>One distinctive feature of the spectral method is that it allows us to <a href="/wiki/Mathematical_proof" title="Mathematical proof">prove</a> that if
distributions satisfy certain separation condition (e.g., not too close), then the estimated mixture will be very close to the true one with high probability.
</p>
<h3><span class="mw-headline" id="Graphical_Methods">Graphical Methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=27" title="Edit section: Graphical Methods">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Tarter and Lock<sup id="cite_ref-tart_12-1" class="reference"><a href="#cite_note-tart-12">&#91;12&#93;</a></sup> describe a graphical approach to mixture identification in which a kernel function is applied to an empirical frequency plot so to reduce intra-component variance.  In this way one may more readily identify components having differing means.  While this <i>λ</i>-method does not require prior knowledge of the number or functional form of the components its success does rely on the choice of the kernel parameters which to some extent implicitly embeds assumptions about the component structure.
</p>
<h3><span class="mw-headline" id="Other_methods">Other methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=28" title="Edit section: Other methods">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Some of them can even probably learn mixtures of <a href="/wiki/Heavy-tailed_distribution" title="Heavy-tailed distribution">heavy-tailed distributions</a> including those with
infinite <a href="/wiki/Variance" title="Variance">variance</a> (see <a href="#Recent_Papers">links to papers</a> below).
In this setting, EM based methods would not work, since the Expectation step would diverge due to presence of
<a href="/wiki/Outlier" title="Outlier">outliers</a>.
</p>
<h3><span class="mw-headline" id="A_simulation">A simulation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=29" title="Edit section: A simulation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>To simulate a sample of size <i>N</i> that is from a mixture of distributions <i>F</i><sub><i>i</i></sub>, <i>i</i>=1 to <i>n</i>, with probabilities <i>p</i><sub><i>i</i></sub> (sum=&#160;<i>p</i><sub><i>i</i></sub>&#160;=&#160;1):
</p>
<ol><li>Generate <i>N</i> random numbers from a <a href="/wiki/Categorical_distribution" title="Categorical distribution">categorical distribution</a> of size <i>n</i> and probabilities <i>p</i><sub><i>i</i></sub> for <i>i</i>=&#160;1=&#160;to&#160;<i>n</i>.  These tell you which of the <i>F</i><sub><i>i</i></sub> each of the <i>N</i> values will come from.  Denote by <i>m<sub>i</sub></i> the quantity of random numbers assigned to the <i>i</i><sup>th</sup> category.</li>
<li>For each <i>i</i>, generate <i>m<sub>i</sub></i> random numbers from the <i>F</i><sub><i>i</i></sub> distribution.</li></ol>
<h2><span class="mw-headline" id="Extensions">Extensions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=30" title="Edit section: Extensions">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In a <a href="/wiki/Bayesian_inference" title="Bayesian inference">Bayesian setting</a>, additional levels can be added to the <a href="/wiki/Graphical_model" title="Graphical model">graphical model</a> defining the mixture model.  For example, in the common <a href="/wiki/Latent_Dirichlet_allocation" title="Latent Dirichlet allocation">latent Dirichlet allocation</a> <a href="/wiki/Topic_model" title="Topic model">topic model</a>, the observations are sets of words drawn from <i>D</i> different documents and the <i>K</i> mixture components represent topics that are shared across documents.  Each document has a different set of mixture weights, which specify the topics prevalent in that document.  All sets of mixture weights share common <a href="/wiki/Hyperparameter" title="Hyperparameter">hyperparameters</a>.
</p><p>A very common extension is to connect the <a href="/wiki/Latent_variable" title="Latent variable">latent variables</a> defining the mixture component identities into a <a href="/wiki/Markov_chain" title="Markov chain">Markov chain</a>, instead of assuming that they are <a href="/wiki/Independent_identically_distributed" class="mw-redirect" title="Independent identically distributed">independent identically distributed</a> random variables.  The resulting model is termed a <a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">hidden Markov model</a> and is one of the most common sequential hierarchical models.  Numerous extensions of hidden Markov models have been developed; see the resulting article for more information.
</p>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=31" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Mixture distributions and the problem of mixture decomposition, that is the identification of its constituent components and the parameters thereof, has been cited in the literature as far back as 1846 (Quetelet in McLachlan, <sup id="cite_ref-McLachlan_2_17-1" class="reference"><a href="#cite_note-McLachlan_2-17">&#91;17&#93;</a></sup> 2000) although common reference is made to the work of <a href="/wiki/Karl_Pearson" title="Karl Pearson">Karl Pearson</a> (1894)<sup id="cite_ref-Amendola2015_21-0" class="reference"><a href="#cite_note-Amendola2015-21">&#91;21&#93;</a></sup> as the first author to explicitly address the decomposition problem in characterising non-normal attributes of forehead to body length ratios in female shore crab populations.  The motivation for this work was provided by the zoologist <a href="/wiki/Walter_Frank_Raphael_Weldon" class="mw-redirect" title="Walter Frank Raphael Weldon">Walter Frank Raphael Weldon</a> who had speculated in 1893 (in Tarter and Lock<sup id="cite_ref-tart_12-2" class="reference"><a href="#cite_note-tart-12">&#91;12&#93;</a></sup>) that asymmetry in the histogram of these ratios could signal evolutionary divergence. Pearson's approach was to fit a univariate mixture of two normals to the data by choosing the five parameters of the mixture such that the empirical moments matched that of the model.
</p><p>While his work was successful in identifying two potentially distinct sub-populations and in demonstrating the flexibility of mixtures as a moment matching tool, the formulation required the solution of a 9th degree (nonic) polynomial which at the time posed a significant computational challenge.
</p><p>Subsequent works focused on addressing these problems, but it was not until the advent of the modern computer and the popularisation of <a href="/wiki/Maximum_Likelihood" class="mw-redirect" title="Maximum Likelihood">Maximum Likelihood</a> (MLE) parameterisation techniques that research really took off.<sup id="cite_ref-McLachlan_1_22-0" class="reference"><a href="#cite_note-McLachlan_1-22">&#91;22&#93;</a></sup> Since that time there has been a vast body of research on the subject spanning areas such as <a href="/wiki/Fishery" title="Fishery">fisheries research</a>, <a href="/wiki/Agriculture" title="Agriculture">agriculture</a>, <a href="/wiki/Botany" title="Botany">botany</a>, <a href="/wiki/Economics" title="Economics">economics</a>, <a href="/wiki/Medicine" title="Medicine">medicine</a>, <a href="/wiki/Genetics" title="Genetics">genetics</a>, <a href="/wiki/Psychology" title="Psychology">psychology</a>, <a href="/wiki/Palaeontology" class="mw-redirect" title="Palaeontology">palaeontology</a>, <a href="/wiki/Electrophoresis" title="Electrophoresis">electrophoresis</a>, <a href="/wiki/Finance" title="Finance">finance</a>, <a href="/wiki/Geology" title="Geology">geology</a> and <a href="/wiki/Zoology" title="Zoology">zoology</a>.<sup id="cite_ref-titter_1_23-0" class="reference"><a href="#cite_note-titter_1-23">&#91;23&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=32" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Mixture">Mixture</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=33" title="Edit section: Mixture">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li><a href="/wiki/Mixture_density" class="mw-redirect" title="Mixture density">Mixture density</a></li>
<li><a href="/wiki/Mixture_(probability)" title="Mixture (probability)">Mixture (probability)</a></li>
<li><a href="/w/index.php?title=Flexible_Mixture_Model_(FMM)&amp;action=edit&amp;redlink=1" class="new" title="Flexible Mixture Model (FMM) (page does not exist)">Flexible Mixture Model (FMM)</a></li></ul>
<h3><span class="mw-headline" id="Hierarchical_models">Hierarchical models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=34" title="Edit section: Hierarchical models">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical model</a></li>
<li><a href="/wiki/Hierarchical_Bayes_model" class="mw-redirect" title="Hierarchical Bayes model">Hierarchical Bayes model</a></li></ul>
<h3><span class="mw-headline" id="Outlier_detection">Outlier detection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=35" title="Edit section: Outlier detection">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li><a href="/wiki/RANSAC" class="mw-redirect" title="RANSAC">RANSAC</a></li></ul>
<table class="box-More_footnotes plainlinks metadata ambox ambox-style ambox-More_footnotes" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/40px-Text_document_with_red_question_mark.svg.png" decoding="async" width="40" height="40" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/60px-Text_document_with_red_question_mark.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/80px-Text_document_with_red_question_mark.svg.png 2x" data-file-width="48" data-file-height="48" /></div></td><td class="mbox-text"><div class="mbox-text-span">This article includes a <a href="/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources">list of references</a>, but <b>its sources remain unclear</b> because it has <b>insufficient <a href="/wiki/Wikipedia:Citing_sources#Inline_citations" title="Wikipedia:Citing sources">inline citations</a></b>.<span class="hide-when-compact"> Please help to <a href="/wiki/Wikipedia:WikiProject_Fact_and_Reference_Check" class="mw-redirect" title="Wikipedia:WikiProject Fact and Reference Check">improve</a> this article by <a href="/wiki/Wikipedia:When_to_cite" title="Wikipedia:When to cite">introducing</a> more precise citations.</span>  <small class="date-container"><i>(<span class="date">November 2010</span>)</i></small><small class="hide-when-compact"><i> (<a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=36" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text">Sotirios P. Chatzis, Dimitrios I. Kosmopoulos, Theodora A. Varvarigou, "Signal Modeling and Classification Using a Robust Latent Space Model Based on t Distributions," IEEE Transactions on Signal Processing, vol. 56, no. 3, pp. 949–963, March 2008. <a rel="nofollow" class="external autonumber" href="https://ieeexplore.ieee.org/document/4451278/">[1]</a></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text">
<cite class="citation journal">Yu, Guoshen (2012). "Solving Inverse Problems with Piecewise Linear Estimators: From Gaussian Mixture Models to Structured Sparsity". <i>IEEE Transactions on Image Processing</i>. <b>21</b> (5): 2481–2499. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1006.3056">1006.3056</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2012ITIP...21.2481G">2012ITIP...21.2481G</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2Ftip.2011.2176743">10.1109/tip.2011.2176743</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/22180506">22180506</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Image+Processing&amp;rft.atitle=Solving+Inverse+Problems+with+Piecewise+Linear+Estimators%3A+From+Gaussian+Mixture+Models+to+Structured+Sparsity&amp;rft.volume=21&amp;rft.issue=5&amp;rft.pages=2481-2499&amp;rft.date=2012&amp;rft_id=info%3Aarxiv%2F1006.3056&amp;rft_id=info%3Apmid%2F22180506&amp;rft_id=info%3Adoi%2F10.1109%2Ftip.2011.2176743&amp;rft_id=info%3Abibcode%2F2012ITIP...21.2481G&amp;rft.aulast=Yu&amp;rft.aufirst=Guoshen&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text">Dinov, ID. "<a rel="nofollow" class="external text" href="http://repositories.cdlib.org/socr/EM_MM/">Expectation Maximization and Mixture Modeling Tutorial</a>". <i><a rel="nofollow" class="external text" href="http://repositories.cdlib.org/escholarship">California Digital Library</a></i>, Statistics Online Computational Resource, Paper EM_MM, <a rel="nofollow" class="external free" href="http://repositories.cdlib.org/socr/EM_MM">http://repositories.cdlib.org/socr/EM_MM</a>, December 9, 2008</span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation book">Bishop, Christopher (2006). <i>Pattern recognition and machine learning</i>. New York: Springer. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-387-31073-2" title="Special:BookSources/978-0-387-31073-2"><bdi>978-0-387-31073-2</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Pattern+recognition+and+machine+learning&amp;rft.place=New+York&amp;rft.pub=Springer&amp;rft.date=2006&amp;rft.isbn=978-0-387-31073-2&amp;rft.aulast=Bishop&amp;rft.aufirst=Christopher&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">Spall, J. C. and Maryak, J. L. (1992). "A feasible Bayesian estimator of quantiles for projectile accuracy from non-i.i.d. data." <i>Journal of the American Statistical Association</i>, vol. 87 (419), pp. 676–681. <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a>&#160;<a rel="nofollow" class="external text" href="https://www.jstor.org/stable/2290205">2290205</a></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation book">Amruthnath, Nagdev; Gupta, Tarun (2018-02-02). <a rel="nofollow" class="external text" href="https://www.researchgate.net/publication/322900854"><i>Fault Class Prediction in Unsupervised Learning using Model-Based Clustering Approach</i></a>. Unpublished. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.13140%2Frg.2.2.22085.14563">10.13140/rg.2.2.22085.14563</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Fault+Class+Prediction+in+Unsupervised+Learning+using+Model-Based+Clustering+Approach&amp;rft.pub=Unpublished&amp;rft.date=2018-02-02&amp;rft_id=info%3Adoi%2F10.13140%2Frg.2.2.22085.14563&amp;rft.aulast=Amruthnath&amp;rft.aufirst=Nagdev&amp;rft.au=Gupta%2C+Tarun&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F322900854&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite class="citation book">Amruthnath, Nagdev; Gupta, Tarun (2018-02-01). <a rel="nofollow" class="external text" href="https://www.researchgate.net/publication/322869981"><i>A Research Study on Unsupervised Machine Learning Algorithms for Fault Detection in Predictive Maintenance</i></a>. Unpublished. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.13140%2Frg.2.2.28822.24648">10.13140/rg.2.2.28822.24648</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+Research+Study+on+Unsupervised+Machine+Learning+Algorithms+for+Fault+Detection+in+Predictive+Maintenance&amp;rft.pub=Unpublished&amp;rft.date=2018-02-01&amp;rft_id=info%3Adoi%2F10.13140%2Frg.2.2.28822.24648&amp;rft.aulast=Amruthnath&amp;rft.aufirst=Nagdev&amp;rft.au=Gupta%2C+Tarun&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F322869981&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">
<cite class="citation journal">Shen, Jianhong (Jackie) (2006). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC2324060">"A stochastic-variational model for soft Mumford-Shah segmentation"</a>. <i>International Journal of Biomedical Imaging</i>. <b>2006</b>: 2–16. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2006IJBI.200649515H">2006IJBI.200649515H</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1155%2FIJBI%2F2006%2F92329">10.1155/IJBI/2006/92329</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC2324060">2324060</a></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/23165059">23165059</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Biomedical+Imaging&amp;rft.atitle=A+stochastic-variational+model+for+soft+Mumford-Shah+segmentation&amp;rft.volume=2006&amp;rft.pages=2-16&amp;rft.date=2006&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2324060&amp;rft_id=info%3Apmid%2F23165059&amp;rft_id=info%3Adoi%2F10.1155%2FIJBI%2F2006%2F92329&amp;rft_id=info%3Abibcode%2F2006IJBI.200649515H&amp;rft.aulast=Shen&amp;rft.aufirst=Jianhong+%28Jackie%29&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2324060&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">
<cite class="citation journal">Myronenko, Andriy; Song, Xubo (2010). "Point set registration: Coherent point drift". <i>IEEE Trans. Pattern Anal. Mach. Intell</i>. <b>32</b> (12): 2262–2275. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/0905.2635">0905.2635</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTPAMI.2010.46">10.1109/TPAMI.2010.46</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/20975122">20975122</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Trans.+Pattern+Anal.+Mach.+Intell.&amp;rft.atitle=Point+set+registration%3A+Coherent+point+drift&amp;rft.volume=32&amp;rft.issue=12&amp;rft.pages=2262-2275&amp;rft.date=2010&amp;rft_id=info%3Aarxiv%2F0905.2635&amp;rft_id=info%3Apmid%2F20975122&amp;rft_id=info%3Adoi%2F10.1109%2FTPAMI.2010.46&amp;rft.aulast=Myronenko&amp;rft.aufirst=Andriy&amp;rft.au=Song%2C+Xubo&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">
<cite class="citation journal">Ravikumar, Nishant; Gooya, Ali; Cimen, Serkan; Frangi, Alexjandro; Taylor, Zeike (2018). "Group-wise similarity registration of point sets using Student's t-mixture model for statistical shape models". <i>Med. Image. Anal</i>. <b>44</b>: 156–176. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.media.2017.11.012">10.1016/j.media.2017.11.012</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/29248842">29248842</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Med.+Image.+Anal.&amp;rft.atitle=Group-wise+similarity+registration+of+point+sets+using+Student%27s+t-mixture+model+for+statistical+shape+models&amp;rft.volume=44&amp;rft.pages=156-176&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.1016%2Fj.media.2017.11.012&amp;rft_id=info%3Apmid%2F29248842&amp;rft.aulast=Ravikumar&amp;rft.aufirst=Nishant&amp;rft.au=Gooya%2C+Ali&amp;rft.au=Cimen%2C+Serkan&amp;rft.au=Frangi%2C+Alexjandro&amp;rft.au=Taylor%2C+Zeike&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">
<cite class="citation conference">Bayer, Siming; Ravikumar, Nishant; Strumia, Maddalena; Tong, Xiaoguang; Gao, Ying; Ostermeier, Martin; Fahrig, Rebecca; Maier, Andreas (2018). <a rel="nofollow" class="external text" href="https://www.miccai2018.org/en/">"Intraoperative brain shift compensation using a hybrid mixture model"</a>. <i>Medical Image Computing and Computer Assisted Intervention – MICCAI 2018</i>. Granada, Spain: Springer, Cham. pp.&#160;116–124. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-3-030-00937-3_14">10.1007/978-3-030-00937-3_14</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Intraoperative+brain+shift+compensation+using+a+hybrid+mixture+model&amp;rft.btitle=Medical+Image+Computing+and+Computer+Assisted+Intervention+%E2%80%93+MICCAI+2018&amp;rft.place=Granada%2C+Spain&amp;rft.pages=116-124&amp;rft.pub=Springer%2C+Cham&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-030-00937-3_14&amp;rft.aulast=Bayer&amp;rft.aufirst=Siming&amp;rft.au=Ravikumar%2C+Nishant&amp;rft.au=Strumia%2C+Maddalena&amp;rft.au=Tong%2C+Xiaoguang&amp;rft.au=Gao%2C+Ying&amp;rft.au=Ostermeier%2C+Martin&amp;rft.au=Fahrig%2C+Rebecca&amp;rft.au=Maier%2C+Andreas&amp;rft_id=https%3A%2F%2Fwww.miccai2018.org%2Fen%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-tart-12"><span class="mw-cite-backlink">^ <a href="#cite_ref-tart_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-tart_12-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-tart_12-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFTarter1993" class="citation">Tarter, Michael E. (1993), <i>Model Free Curve Estimation</i>, Chapman and Hall</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Model+Free+Curve+Estimation&amp;rft.pub=Chapman+and+Hall&amp;rft.date=1993&amp;rft.aulast=Tarter&amp;rft.aufirst=Michael+E.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-Jain-13"><span class="mw-cite-backlink">^ <a href="#cite_ref-Jain_13-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Jain_13-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Figueiredo, M.A.T.; Jain, A.K. (March 2002). "Unsupervised Learning of Finite Mixture Models". <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>24</b> (3): 381–396. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.362.9811">10.1.1.362.9811</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2F34.990138">10.1109/34.990138</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.atitle=Unsupervised+Learning+of+Finite+Mixture+Models&amp;rft.volume=24&amp;rft.issue=3&amp;rft.pages=381-396&amp;rft.date=2002-03&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.362.9811&amp;rft_id=info%3Adoi%2F10.1109%2F34.990138&amp;rft.aulast=Figueiredo&amp;rft.aufirst=M.A.T.&amp;rft.au=Jain%2C+A.K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-mcwilli-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-mcwilli_14-0">^</a></b></span> <span class="reference-text">
<cite id="CITEREFMcWilliamLoh2008" class="citation">McWilliam, N.; Loh, K. (2008), <i>Incorporating Multidimensional Tail-Dependencies in the Valuation of Credit Derivatives (Working Paper)</i></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Incorporating+Multidimensional+Tail-Dependencies+in+the+Valuation+of+Credit+Derivatives+%28Working+Paper%29&amp;rft.date=2008&amp;rft.aulast=McWilliam&amp;rft.aufirst=N.&amp;rft.au=Loh%2C+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/> <a rel="nofollow" class="external autonumber" href="http://www.misys.com/cds-portlets/digitalAssets/4/2797_CDsAndTailDep_forPublication_final1.pdf">[2]</a></span>
</li>
<li id="cite_note-dempster1977-15"><span class="mw-cite-backlink">^ <a href="#cite_ref-dempster1977_15-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-dempster1977_15-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Dempster, A.P.; Laird, N.M.; Rubin, D.B. (1977). "Maximum Likelihood from Incomplete Data via the EM Algorithm". <i>Journal of the Royal Statistical Society, Series B</i>. <b>39</b> (1): 1–38. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.163.7580">10.1.1.163.7580</a></span>. <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a>&#160;<a rel="nofollow" class="external text" href="//www.jstor.org/stable/2984875">2984875</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Royal+Statistical+Society%2C+Series+B&amp;rft.atitle=Maximum+Likelihood+from+Incomplete+Data+via+the+EM+Algorithm&amp;rft.volume=39&amp;rft.issue=1&amp;rft.pages=1-38&amp;rft.date=1977&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.163.7580&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2984875&amp;rft.aulast=Dempster&amp;rft.aufirst=A.P.&amp;rft.au=Laird%2C+N.M.&amp;rft.au=Rubin%2C+D.B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-XuJordam-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-XuJordam_16-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Xu, L.; Jordan, M.I. (January 1996). "On Convergence Properties of the EM Algorithm for Gaussian Mixtures". <i>Neural Computation</i>. <b>8</b> (1): 129–151. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1162%2Fneco.1996.8.1.129">10.1162/neco.1996.8.1.129</a>. <a href="/wiki/Handle_System" title="Handle System">hdl</a>:<a rel="nofollow" class="external text" href="//hdl.handle.net/10338.dmlcz%2F135225">10338.dmlcz/135225</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=On+Convergence+Properties+of+the+EM+Algorithm+for+Gaussian+Mixtures&amp;rft.volume=8&amp;rft.issue=1&amp;rft.pages=129-151&amp;rft.date=1996-01&amp;rft_id=info%3Ahdl%2F10338.dmlcz%2F135225&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.1996.8.1.129&amp;rft.aulast=Xu&amp;rft.aufirst=L.&amp;rft.au=Jordan%2C+M.I.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-McLachlan_2-17"><span class="mw-cite-backlink">^ <a href="#cite_ref-McLachlan_2_17-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-McLachlan_2_17-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFMcLachlan2000" class="citation">McLachlan, G.J. (2000), <i>Finite Mixture Models</i>, Wiley</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Finite+Mixture+Models&amp;rft.pub=Wiley&amp;rft.date=2000&amp;rft.aulast=McLachlan&amp;rft.aufirst=G.J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-botev2004global-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-botev2004global_18-0">^</a></b></span> <span class="reference-text"><cite class="citation book">Botev, Z.I.; <a href="/wiki/Dirk_Kroese" title="Dirk Kroese">Kroese, D.P.</a> (2004). <i>Global likelihood optimization via the cross-entropy method with an application to mixture models</i>. <i><a href="/w/index.php?title=Proceedings_of_the_2004_Winter_Simulation_Conference&amp;action=edit&amp;redlink=1" class="new" title="Proceedings of the 2004 Winter Simulation Conference (page does not exist)">Proceedings of the 2004 Winter Simulation Conference</a></i>. <b>1</b>. p.&#160;517. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.331.2319">10.1.1.331.2319</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FWSC.2004.1371358">10.1109/WSC.2004.1371358</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-7803-8786-7" title="Special:BookSources/978-0-7803-8786-7"><bdi>978-0-7803-8786-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Global+likelihood+optimization+via+the+cross-entropy+method+with+an+application+to+mixture+models&amp;rft.pages=517&amp;rft.date=2004&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.331.2319&amp;rft_id=info%3Adoi%2F10.1109%2FWSC.2004.1371358&amp;rft.isbn=978-0-7803-8786-7&amp;rft.au=Botev%2C+Z.I.&amp;rft.au=Kroese%2C+D.P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-day-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-day_19-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Day, N. E. (1969). "Estimating the Components of a Mixture of Normal Distributions". <i>Biometrika</i>. <b>56</b> (3): 463–474. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.2307%2F2334652">10.2307/2334652</a>. <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a>&#160;<a rel="nofollow" class="external text" href="//www.jstor.org/stable/2334652">2334652</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biometrika&amp;rft.atitle=Estimating+the+Components+of+a+Mixture+of+Normal+Distributions&amp;rft.volume=56&amp;rft.issue=3&amp;rft.pages=463-474&amp;rft.date=1969&amp;rft_id=info%3Adoi%2F10.2307%2F2334652&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2334652&amp;rft.aulast=Day&amp;rft.aufirst=N.+E.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-wang-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-wang_20-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFWang2001" class="citation">Wang, J. (2001), "Generating daily changes in market variables using a multivariate mixture of normal distributions", <i>Proceedings of the 33rd Winter Conference on Simulation</i>: 283–289</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+33rd+Winter+Conference+on+Simulation&amp;rft.atitle=Generating+daily+changes+in+market+variables+using+a+multivariate+mixture+of+normal+distributions&amp;rft.pages=283-289&amp;rft.date=2001&amp;rft.aulast=Wang&amp;rft.aufirst=J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-Amendola2015-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-Amendola2015_21-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Améndola, Carlos;  et al. (2015). "Moment varieties of Gaussian mixtures". <i>Journal of Algebraic Statistics</i>. <b>7</b>. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1510.04654">1510.04654</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2015arXiv151004654A">2015arXiv151004654A</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.18409%2Fjas.v7i1.42">10.18409/jas.v7i1.42</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Algebraic+Statistics&amp;rft.atitle=Moment+varieties+of+Gaussian+mixtures&amp;rft.volume=7&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1510.04654&amp;rft_id=info%3Adoi%2F10.18409%2Fjas.v7i1.42&amp;rft_id=info%3Abibcode%2F2015arXiv151004654A&amp;rft.aulast=Am%C3%A9ndola&amp;rft.aufirst=Carlos&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-McLachlan_1-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-McLachlan_1_22-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFMcLachlanBasford1988" class="citation">McLachlan, G.J.; Basford, K.E. (1988), "Mixture Models: inference and applications to clustering", <i>Statistics: Textbooks and Monographs</i>, <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/1988mmia.book.....M">1988mmia.book.....M</a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Statistics%3A+Textbooks+and+Monographs&amp;rft.atitle=Mixture+Models%3A+inference+and+applications+to+clustering&amp;rft.date=1988&amp;rft_id=info%3Abibcode%2F1988mmia.book.....M&amp;rft.aulast=McLachlan&amp;rft.aufirst=G.J.&amp;rft.au=Basford%2C+K.E.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-titter_1-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-titter_1_23-0">^</a></b></span> <span class="reference-text"><a href="#CITEREFTitteringtonSmithMakov1985">Titterington, Smith &amp; Makov 1985</a></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=37" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Books_on_mixture_models">Books on mixture models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=38" title="Edit section: Books on mixture models">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li><cite class="citation book">Everitt, B.S.; Hand, D.J. (1981). <i>Finite mixture distributions</i>. Chapman &amp; Hall. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-412-22420-1" title="Special:BookSources/978-0-412-22420-1"><bdi>978-0-412-22420-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Finite+mixture+distributions&amp;rft.pub=Chapman+%26+Hall&amp;rft.date=1981&amp;rft.isbn=978-0-412-22420-1&amp;rft.aulast=Everitt&amp;rft.aufirst=B.S.&amp;rft.au=Hand%2C+D.J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite class="citation book"><a href="/wiki/Bruce_G._Lindsay" title="Bruce G. Lindsay">Lindsay, B. G.</a> (1995). <i>Mixture Models: Theory, Geometry, and Applications</i>. NSF-CBMS Regional Conference Series in Probability and Statistics. <b>5</b>. Hayward: Institute of Mathematical Statistics.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Mixture+Models%3A+Theory%2C+Geometry%2C+and+Applications&amp;rft.place=Hayward&amp;rft.series=NSF-CBMS+Regional+Conference+Series+in+Probability+and+Statistics&amp;rft.pub=Institute+of+Mathematical+Statistics&amp;rft.date=1995&amp;rft.aulast=Lindsay&amp;rft.aufirst=B.+G.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite class="citation book">Marin, J.M.; <a href="/wiki/Kerrie_Mengersen" title="Kerrie Mengersen">Mengersen, K.</a>; Robert, C.P. (2011). <a rel="nofollow" class="external text" href="http://www.ceremade.dauphine.fr/%7Exian/mixo.pdf">"Bayesian modelling and inference on mixtures of distributions"</a> <span class="cs1-format">(PDF)</span>.  In Dey, D.; Rao, C.R. (eds.). <i>Essential Bayesian models</i>. Handbook of statistics: Bayesian thinking - modeling and computation. <b>25</b>. Elsevier. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780444537324" title="Special:BookSources/9780444537324"><bdi>9780444537324</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Bayesian+modelling+and+inference+on+mixtures+of+distributions&amp;rft.btitle=Essential+Bayesian+models&amp;rft.series=Handbook+of+statistics%3A+Bayesian+thinking+-+modeling+and+computation&amp;rft.pub=Elsevier&amp;rft.date=2011&amp;rft.isbn=9780444537324&amp;rft.aulast=Marin&amp;rft.aufirst=J.M.&amp;rft.au=Mengersen%2C+K.&amp;rft.au=Robert%2C+C.P.&amp;rft_id=http%3A%2F%2Fwww.ceremade.dauphine.fr%2F%257Exian%2Fmixo.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite class="citation book">McLachlan, G.J.; Peel, D. (2000). <span class="cs1-lock-registration" title="Free registration required"><a rel="nofollow" class="external text" href="https://archive.org/details/finitemixturemod00geof"><i>Finite Mixture Models</i></a></span>. Wiley. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-471-00626-8" title="Special:BookSources/978-0-471-00626-8"><bdi>978-0-471-00626-8</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Finite+Mixture+Models&amp;rft.pub=Wiley&amp;rft.date=2000&amp;rft.isbn=978-0-471-00626-8&amp;rft.aulast=McLachlan&amp;rft.aufirst=G.J.&amp;rft.au=Peel%2C+D.&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Ffinitemixturemod00geof&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite class="citation book">Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007). <a rel="nofollow" class="external text" href="http://apps.nrbook.com/empanel/index.html#pg=842">"Section 16.1. Gaussian Mixture Models and k-Means Clustering"</a>. <i>Numerical Recipes: The Art of Scientific Computing</i> (3rd ed.). New York: Cambridge University Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-521-88068-8" title="Special:BookSources/978-0-521-88068-8"><bdi>978-0-521-88068-8</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Section+16.1.+Gaussian+Mixture+Models+and+k-Means+Clustering&amp;rft.btitle=Numerical+Recipes%3A+The+Art+of+Scientific+Computing&amp;rft.place=New+York&amp;rft.edition=3rd&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2007&amp;rft.isbn=978-0-521-88068-8&amp;rft.aulast=Press&amp;rft.aufirst=WH&amp;rft.au=Teukolsky%2C+SA&amp;rft.au=Vetterling%2C+WT&amp;rft.au=Flannery%2C+BP&amp;rft_id=http%3A%2F%2Fapps.nrbook.com%2Fempanel%2Findex.html%23pg%3D842&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite id="CITEREFTitteringtonSmithMakov1985" class="citation book">Titterington, D.; Smith, A.; Makov, U. (1985). <i>Statistical Analysis of Finite Mixture Distributions</i>. Wiley. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-471-90763-3" title="Special:BookSources/978-0-471-90763-3"><bdi>978-0-471-90763-3</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Statistical+Analysis+of+Finite+Mixture+Distributions&amp;rft.pub=Wiley&amp;rft.date=1985&amp;rft.isbn=978-0-471-90763-3&amp;rft.aulast=Titterington&amp;rft.aufirst=D.&amp;rft.au=Smith%2C+A.&amp;rft.au=Makov%2C+U.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li></ul>
<h3><span class="mw-headline" id="Application_of_Gaussian_mixture_models">Application of Gaussian mixture models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=39" title="Edit section: Application of Gaussian mixture models">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ol><li><cite class="citation journal">Reynolds, D.A.; Rose, R.C. (January 1995). "Robust text-independent speaker identification using Gaussian mixture speaker models". <i>IEEE Transactions on Speech and Audio Processing</i>. <b>3</b> (1): 72–83. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2F89.365379">10.1109/89.365379</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Speech+and+Audio+Processing&amp;rft.atitle=Robust+text-independent+speaker+identification+using+Gaussian+mixture+speaker+models&amp;rft.volume=3&amp;rft.issue=1&amp;rft.pages=72-83&amp;rft.date=1995-01&amp;rft_id=info%3Adoi%2F10.1109%2F89.365379&amp;rft.aulast=Reynolds&amp;rft.aufirst=D.A.&amp;rft.au=Rose%2C+R.C.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite class="citation conference">Permuter, H.; Francos, J.; Jermyn, I.H. (2003). <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1199538"><i>Gaussian mixture models of texture and colour for image database retrieval</i></a>. IEEE <a href="/wiki/International_Conference_on_Acoustics,_Speech,_and_Signal_Processing" title="International Conference on Acoustics, Speech, and Signal Processing">International Conference on Acoustics, Speech, and Signal Processing</a>, 2003. Proceedings (ICASSP '03).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Gaussian+mixture+models+of+texture+and+colour+for+image+database+retrieval&amp;rft.date=2003&amp;rft.aulast=Permuter&amp;rft.aufirst=H.&amp;rft.au=Francos%2C+J.&amp;rft.au=Jermyn%2C+I.H.&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1199538&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/>
<ul><li><cite class="citation journal">Permuter, Haim; Francos, Joseph; Jermyn, Ian (2006). <a rel="nofollow" class="external text" href="http://dro.dur.ac.uk/16022/1/16022.pdf">"A study of Gaussian mixture models of color and texture features for image classification and segmentation"</a> <span class="cs1-format">(PDF)</span>. <i>Pattern Recognition</i>. <b>39</b> (4): 695–706. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.patcog.2005.10.028">10.1016/j.patcog.2005.10.028</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pattern+Recognition&amp;rft.atitle=A+study+of+Gaussian+mixture+models+of+color+and+texture+features+for+image+classification+and+segmentation&amp;rft.volume=39&amp;rft.issue=4&amp;rft.pages=695-706&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patcog.2005.10.028&amp;rft.aulast=Permuter&amp;rft.aufirst=Haim&amp;rft.au=Francos%2C+Joseph&amp;rft.au=Jermyn%2C+Ian&amp;rft_id=http%3A%2F%2Fdro.dur.ac.uk%2F16022%2F1%2F16022.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li></ul></li>
<li><cite class="citation book">Lemke, Wolfgang (2005). <i>Term Structure Modeling and Estimation in a State Space Framework</i>. Springer Verlag. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-540-28342-3" title="Special:BookSources/978-3-540-28342-3"><bdi>978-3-540-28342-3</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Term+Structure+Modeling+and+Estimation+in+a+State+Space+Framework&amp;rft.pub=Springer+Verlag&amp;rft.date=2005&amp;rft.isbn=978-3-540-28342-3&amp;rft.aulast=Lemke&amp;rft.aufirst=Wolfgang&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite class="citation conference"><a href="/wiki/Damiano_Brigo" title="Damiano Brigo">Brigo, Damiano</a>; <a href="/wiki/Fabio_Mercurio" title="Fabio Mercurio">Mercurio, Fabio</a> (2001). <i>Displaced and Mixture Diffusions for Analytically-Tractable Smile Models</i>. Mathematical Finance – Bachelier Congress 2000. Proceedings. Springer Verlag.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Displaced+and+Mixture+Diffusions+for+Analytically-Tractable+Smile+Models&amp;rft.pub=Springer+Verlag&amp;rft.date=2001&amp;rft.aulast=Brigo&amp;rft.aufirst=Damiano&amp;rft.au=Mercurio%2C+Fabio&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite class="citation journal">Brigo, Damiano; Mercurio, Fabio (June 2002). "Lognormal-mixture dynamics and calibration to market volatility smiles". <i>International Journal of Theoretical and Applied Finance</i>. <b>5</b> (4): 427. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.210.4165">10.1.1.210.4165</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1142%2FS0219024902001511">10.1142/S0219024902001511</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Theoretical+and+Applied+Finance&amp;rft.atitle=Lognormal-mixture+dynamics+and+calibration+to+market+volatility+smiles&amp;rft.volume=5&amp;rft.issue=4&amp;rft.pages=427&amp;rft.date=2002-06&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.210.4165&amp;rft_id=info%3Adoi%2F10.1142%2FS0219024902001511&amp;rft.aulast=Brigo&amp;rft.aufirst=Damiano&amp;rft.au=Mercurio%2C+Fabio&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite class="citation journal">Spall, J. C.; Maryak, J. L. (1992). "A feasible Bayesian estimator of quantiles for projectile accuracy from non-i.i.d. data". <i>Journal of the American Statistical Association</i>. <b>87</b> (419): 676–681. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F01621459.1992.10475269">10.1080/01621459.1992.10475269</a>. <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a>&#160;<a rel="nofollow" class="external text" href="//www.jstor.org/stable/2290205">2290205</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+American+Statistical+Association&amp;rft.atitle=A+feasible+Bayesian+estimator+of+quantiles+for+projectile+accuracy+from+non-i.i.d.+data&amp;rft.volume=87&amp;rft.issue=419&amp;rft.pages=676-681&amp;rft.date=1992&amp;rft_id=info%3Adoi%2F10.1080%2F01621459.1992.10475269&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2290205&amp;rft.aulast=Spall&amp;rft.aufirst=J.+C.&amp;rft.au=Maryak%2C+J.+L.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite class="citation journal">Alexander, Carol (December 2004). <a rel="nofollow" class="external text" href="http://www.carolalexander.org/publish/download/JournalArticles/PDFs/JBF2004.pdf">"Normal mixture diffusion with uncertain volatility: Modelling short- and long-term smile effects"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Banking &amp; Finance</i>. <b>28</b> (12): 2957–80. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.jbankfin.2003.10.017">10.1016/j.jbankfin.2003.10.017</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Banking+%26+Finance&amp;rft.atitle=Normal+mixture+diffusion+with+uncertain+volatility%3A+Modelling+short-+and+long-term+smile+effects&amp;rft.volume=28&amp;rft.issue=12&amp;rft.pages=2957-80&amp;rft.date=2004-12&amp;rft_id=info%3Adoi%2F10.1016%2Fj.jbankfin.2003.10.017&amp;rft.aulast=Alexander&amp;rft.aufirst=Carol&amp;rft_id=http%3A%2F%2Fwww.carolalexander.org%2Fpublish%2Fdownload%2FJournalArticles%2FPDFs%2FJBF2004.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite class="citation conference">Stylianou, Yannis; Pantazis, Yannis; Calderero, Felipe; Larroy, Pedro; Severin, Francois; Schimke, Sascha; Bonal, Rolando; Matta, Federico; Valsamakis, Athanasios (2005). <a rel="nofollow" class="external text" href="http://www.enterface.net/enterface05/docs/results/reports/project5.pdf"><i>GMM-Based Multimodal Biometric Verification</i></a> <span class="cs1-format">(PDF)</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=GMM-Based+Multimodal+Biometric+Verification&amp;rft.date=2005&amp;rft.aulast=Stylianou&amp;rft.aufirst=Yannis&amp;rft.au=Pantazis%2C+Yannis&amp;rft.au=Calderero%2C+Felipe&amp;rft.au=Larroy%2C+Pedro&amp;rft.au=Severin%2C+Francois&amp;rft.au=Schimke%2C+Sascha&amp;rft.au=Bonal%2C+Rolando&amp;rft.au=Matta%2C+Federico&amp;rft.au=Valsamakis%2C+Athanasios&amp;rft_id=http%3A%2F%2Fwww.enterface.net%2Fenterface05%2Fdocs%2Fresults%2Freports%2Fproject5.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li><cite class="citation conference">Chen, J.; Adebomi, 0.E.; Olusayo, O.S.; Kulesza, W. (2010). <a rel="nofollow" class="external text" href="https://ieeexplore.ieee.org/document/5548541/"><i>The Evaluation of the Gaussian Mixture Probability Hypothesis Density approach for multi-target tracking</i></a>. IEEE <a href="/w/index.php?title=International_Conference_on_Imaging_Systems_and_Techniques&amp;action=edit&amp;redlink=1" class="new" title="International Conference on Imaging Systems and Techniques (page does not exist)">International Conference on Imaging Systems and Techniques</a>, 2010.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=The+Evaluation+of+the+Gaussian+Mixture+Probability+Hypothesis+Density+approach+for+multi-target+tracking&amp;rft.date=2010&amp;rft.aulast=Chen&amp;rft.aufirst=J.&amp;rft.au=Adebomi%2C+0.E.&amp;rft.au=Olusayo%2C+O.S.&amp;rft.au=Kulesza%2C+W.&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F5548541%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li></ol>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Mixture_model&amp;action=edit&amp;section=40" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><cite class="citation book">Nielsen, Frank (23 March 2012). <i><span></span></i>k<i>-MLE: A fast algorithm for learning statistical mixture models</i>. <i>2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>. pp.&#160;869–872. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1203.5181">1203.5181</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2012arXiv1203.5181N">2012arXiv1203.5181N</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FICASSP.2012.6288022">10.1109/ICASSP.2012.6288022</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4673-0046-9" title="Special:BookSources/978-1-4673-0046-9"><bdi>978-1-4673-0046-9</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=k-MLE%3A+A+fast+algorithm+for+learning+statistical+mixture+models&amp;rft.pages=869-872&amp;rft.date=2012-03-23&amp;rft_id=info%3Aarxiv%2F1203.5181&amp;rft_id=info%3Adoi%2F10.1109%2FICASSP.2012.6288022&amp;rft_id=info%3Abibcode%2F2012arXiv1203.5181N&amp;rft.isbn=978-1-4673-0046-9&amp;rft.aulast=Nielsen&amp;rft.aufirst=Frank&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMixture+model" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></li>
<li>The <a rel="nofollow" class="external text" href="http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_2D_PointSegmentation_EM_Mixture">SOCR demonstrations of EM and Mixture Modeling</a></li>
<li><a rel="nofollow" class="external text" href="http://www.csse.monash.edu.au/~dld/mixturemodel.html">Mixture modelling page</a> (and the <a rel="nofollow" class="external text" href="http://www.csse.monash.edu.au/~dld/Snob.html">Snob</a> program for <a href="/wiki/Minimum_Message_Length" class="mw-redirect" title="Minimum Message Length">Minimum Message Length</a> (<a href="/wiki/Minimum_Message_Length" class="mw-redirect" title="Minimum Message Length">MML</a>) applied to finite mixture models), maintained by D.L. Dowe.</li>
<li><a rel="nofollow" class="external text" href="http://www.pymix.org">PyMix</a> – Python Mixture Package, algorithms and data structures for a broad variety of mixture model based data mining applications in Python</li>
<li><a rel="nofollow" class="external text" href="http://scikit-learn.org/stable/modules/mixture.html">sklearn.mixture</a> – A Python package for learning Gaussian Mixture Models (and sampling from them), previously packaged with <a href="/wiki/SciPy" title="SciPy">SciPy</a> and now packaged as a <a rel="nofollow" class="external text" href="https://scikits.appspot.com/">SciKit</a></li>
<li><a rel="nofollow" class="external text" href="http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=18785&amp;objectType=FILE">GMM.m</a> Matlab code for GMM Implementation</li>
<li><a rel="nofollow" class="external text" href="http://stat.duke.edu/gpustatsci/software.html">GPUmix</a> C++ implementation of Bayesian Mixture Models using EM and MCMC with 100x speed acceleration using GPGPU.</li>
<li><a rel="nofollow" class="external autonumber" href="http://www.cs.ru.nl/~ali/index_files/EM.m">[3]</a> Matlab code for GMM Implementation using EM algorithm</li>
<li><a rel="nofollow" class="external autonumber" href="https://vincentfpgarcia.github.com/jMEF/">[4]</a> jMEF: A Java open source library for learning and processing mixtures of exponential families (using duality with Bregman divergences). Includes a Matlab wrapper.</li>
<li>Very Fast and clean C implementation of the <a rel="nofollow" class="external text" href="https://github.com/juandavm/em4gmm">Expectation Maximization</a> (EM) algorithm for estimating <a rel="nofollow" class="external text" href="https://github.com/juandavm/em4gmm">Gaussian Mixture Models</a> (GMMs).</li>
<li><a rel="nofollow" class="external text" href="https://cran.r-project.org/web/packages/mclust/index.html">mclust</a> is an R package for mixture modeling.</li>
<li><a rel="nofollow" class="external text" href="https://github.com/thaines/helit/tree/master/dpgmm">dpgmm</a> Pure Python Dirichlet process Gaussian mixture model implementation (variational).</li></ul>
<!-- 
NewPP limit report
Parsed by mw1322
Cached time: 20200202040521
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 1.196 seconds
Real time usage: 4.431 seconds
Preprocessor visited node count: 2640/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 75380/2097152 bytes
Template argument size: 1356/2097152 bytes
Highest expansion depth: 13/40
Expensive parser function count: 10/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 103107/5000000 bytes
Number of Wikibase entities loaded: 7/400
Lua time usage: 0.619/10.000 seconds
Lua memory usage: 5.85 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 3794.233      1 -total
 49.31% 1871.005      1 Template:Reflist
 49.12% 1863.706     14 Template:Cite_journal
 16.50%  625.950      1 Template:Distinguish
 13.67%  518.749      1 Template:Full_citation_needed
  3.87%  146.832     12 Template:Cite_book
  3.42%  129.815      1 Template:See_also
  2.55%   96.682      9 Template:Nowrap
  2.44%   92.755      1 Template:More_footnotes
  2.33%   88.346      2 Template:Fix
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:871681-0!canonical!math=5 and timestamp 20200202040517 and revision id 926043903
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Mixture_model&amp;oldid=926043903#Gaussian_mixture_model">https://en.wikipedia.org/w/index.php?title=Mixture_model&amp;oldid=926043903#Gaussian_mixture_model</a>"</div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Cluster_analysis" title="Category:Cluster analysis">Cluster analysis</a></li><li><a href="/wiki/Category:Latent_variable_models" title="Category:Latent variable models">Latent variable models</a></li><li><a href="/wiki/Category:Probabilistic_models" title="Category:Probabilistic models">Probabilistic models</a></li><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Articles_with_incomplete_citations_from_November_2012" title="Category:Articles with incomplete citations from November 2012">Articles with incomplete citations from November 2012</a></li><li><a href="/wiki/Category:All_Wikipedia_articles_needing_clarification" title="Category:All Wikipedia articles needing clarification">All Wikipedia articles needing clarification</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_March_2008" title="Category:Wikipedia articles needing clarification from March 2008">Wikipedia articles needing clarification from March 2008</a></li><li><a href="/wiki/Category:Articles_lacking_in-text_citations_from_November_2010" title="Category:Articles lacking in-text citations from November 2010">Articles lacking in-text citations from November 2010</a></li><li><a href="/wiki/Category:All_articles_lacking_in-text_citations" title="Category:All articles lacking in-text citations">All articles lacking in-text citations</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
    <h2>Navigation menu</h2>
    <div id="mw-head">
        
<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
	<h3 id="p-personal-label">Personal tools</h3>
	<ul >
		
		<li id="pt-anonuserpage">Not logged in</li>
		<li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Mixture+model" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Mixture+model" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>
	</ul>
</div>

        <div id="left-navigation">
            <div id="p-namespaces" role="navigation" class="vectorTabs " aria-labelledby="p-namespaces-label">
	<h3 id="p-namespaces-label">Namespaces</h3>
	<ul >
		<li id="ca-nstab-main" class="selected"><a href="/wiki/Mixture_model" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="/wiki/Talk:Mixture_model" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></li>
	</ul>
</div>
<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
	<h3 id="p-variants-label">
		<span>Variants</span>
	</h3>
	<ul class="menu" >
		
	</ul>
</div>

        </div>
        <div id="right-navigation">
            <div id="p-views" role="navigation" class="vectorTabs " aria-labelledby="p-views-label">
	<h3 id="p-views-label">Views</h3>
	<ul >
		<li id="ca-view" class="collapsible selected"><a href="/wiki/Mixture_model">Read</a></li><li id="ca-edit" class="collapsible"><a href="/w/index.php?title=Mixture_model&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="/w/index.php?title=Mixture_model&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li>
	</ul>
</div>
<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
	<h3 id="p-cactions-label">
		<span>More</span>
	</h3>
	<ul class="menu" >
		
	</ul>
</div>
<div id="p-search" role="search">
	<h3 >
		<label for="searchInput">Search</label>
	</h3>
	<form action="/w/index.php" id="searchform">
		<div id="simpleSearch">
			<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
			<input type="hidden" value="Special:Search" name="title"/>
			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
		</div>
	</form>
</div>

        </div>
    </div>
    <div id="mw-panel">
        <div id="p-logo" role="banner">
            <a  title="Visit the main page" class="mw-wiki-logo" href="/wiki/Main_Page"></a>
        </div>
        
<div class="portal" role="navigation" id="p-navigation"  aria-labelledby="p-navigation-label">
	<h3  id="p-navigation-label">
		Navigation
	</h3>
	<div class="body">
		<ul><li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-interaction"  aria-labelledby="p-interaction-label">
	<h3  id="p-interaction-label">
		Interaction
	</h3>
	<div class="body">
		<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-tb"  aria-labelledby="p-tb-label">
	<h3  id="p-tb-label">
		Tools
	</h3>
	<div class="body">
		<ul><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Mixture_model" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Mixture_model" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Mixture_model&amp;oldid=926043903" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Mixture_model&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q2260434" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Mixture_model&amp;id=926043903" title="Information on how to cite this page">Cite this page</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-coll-print_export"  aria-labelledby="p-coll-print_export-label">
	<h3  id="p-coll-print_export-label">
		Print/export
	</h3>
	<div class="body">
		<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Mixture+model">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Mixture+model&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Mixture_model&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-lang"  aria-labelledby="p-lang-label">
	<h3  id="p-lang-label">
		Languages
	</h3>
	<div class="body">
		<ul><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/Mod%C3%A8le_de_m%C3%A9lange" title="Modèle de mélange – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li><li class="interlanguage-link interwiki-ko"><a href="https://ko.wikipedia.org/wiki/%ED%98%BC%ED%95%A9_%EB%AA%A8%EB%8D%B8" title="혼합 모델 – Korean" lang="ko" hreflang="ko" class="interlanguage-link-target">한국어</a></li><li class="interlanguage-link interwiki-pt"><a href="https://pt.wikipedia.org/wiki/Modelo_mistura" title="Modelo mistura – Portuguese" lang="pt" hreflang="pt" class="interlanguage-link-target">Português</a></li><li class="interlanguage-link interwiki-ta"><a href="https://ta.wikipedia.org/wiki/%E0%AE%95%E0%AE%B2%E0%AE%AA%E0%AF%8D%E0%AE%AA%E0%AE%AE%E0%AF%88%E0%AE%AA%E0%AF%8D%E0%AE%AA%E0%AF%81" title="கலப்பமைப்பு – Tamil" lang="ta" hreflang="ta" class="interlanguage-link-target">தமிழ்</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B" title="混合模型 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li></ul>
		<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q2260434#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
	</div>
</div>

    </div>
</div>


<div id="footer" role="contentinfo" >
	<ul id="footer-info" class="">
		<li id="footer-info-lastmod"> This page was last edited on 13 November 2019, at 22:28<span class="anonymous-show">&#160;(UTC)</span>.</li>
		<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
	</ul>
	<ul id="footer-places" class="">
		<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
		<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
		<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
		<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
		<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
		<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Mixture_model&amp;mobileaction=toggle_view_mobile#Gaussian_mixture_model" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a></li>
		<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a></li>
	</ul>
	<div style="clear: both;"></div>
</div>


<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"1.196","walltime":"4.431","ppvisitednodes":{"value":2640,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":75380,"limit":2097152},"templateargumentsize":{"value":1356,"limit":2097152},"expansiondepth":{"value":13,"limit":40},"expensivefunctioncount":{"value":10,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":103107,"limit":5000000},"entityaccesscount":{"value":7,"limit":400},"timingprofile":["100.00% 3794.233      1 -total"," 49.31% 1871.005      1 Template:Reflist"," 49.12% 1863.706     14 Template:Cite_journal"," 16.50%  625.950      1 Template:Distinguish"," 13.67%  518.749      1 Template:Full_citation_needed","  3.87%  146.832     12 Template:Cite_book","  3.42%  129.815      1 Template:See_also","  2.55%   96.682      9 Template:Nowrap","  2.44%   92.755      1 Template:More_footnotes","  2.33%   88.346      2 Template:Fix"]},"scribunto":{"limitreport-timeusage":{"value":"0.619","limit":"10.000"},"limitreport-memusage":{"value":6138973,"limit":52428800}},"cachereport":{"origin":"mw1322","timestamp":"20200202040521","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Mixture model","url":"https:\/\/en.wikipedia.org\/wiki\/Mixture_model#Gaussian_mixture_model","sameAs":"http:\/\/www.wikidata.org\/entity\/Q2260434","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q2260434","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2004-08-01T20:06:29Z","dateModified":"2019-11-13T22:28:20Z"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":117,"wgHostname":"mw1349"});});</script></body></html>
