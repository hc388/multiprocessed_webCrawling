<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Human Compatible - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRequestId":"Xk5cQApAAD8AAAMueTAAAAEN","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Human_Compatible","wgTitle":"Human Compatible","wgCurRevisionId":940281141,"wgRevisionId":940281141,"wgArticleId":63005595,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles with short description","All stub articles","2019 non-fiction books","Existential risk from artificial general intelligence","Futurology books","Artificial intelligence stubs","Computer book stubs"
],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Human_Compatible","wgRelevantArticleId":63005595,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready",
"mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","jquery.makeCollapsible.styles":"ready","mediawiki.toc.styles":"ready","skins.vector.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init",
"ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.tokens@tffin",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.35.0-wmf.19"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta name="robots" content="noindex,nofollow"/>
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/en/d/db/Human_Compatible_%28Stuart_J._Russell%2C_2019%29_book_cover.jpg"/>
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Human_Compatible"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Human_Compatible&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Human_Compatible&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Human_Compatible"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Human_Compatible rootpage-Human_Compatible skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en"><i>Human Compatible</i></h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">2019 book by Stuart J. Russell</div>
<table class="infobox vcard" style="width:22em"><caption style="font-style:italic;padding-bottom:0.2em;">Human Compatible <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Human+Compatible&amp;rft.author=%5B%5BStuart+J.+Russell%5D%5D&amp;rft.date=October+8%2C+2019&amp;rft.pub=Viking&amp;rft.pages=352&amp;rft_id=info:oclcnum/1083694322"></span></caption><tbody><tr><td colspan="2" style="text-align:center"><a href="/wiki/File:Human_Compatible_(Stuart_J._Russell,_2019)_book_cover.jpg" class="image"><img alt="Human Compatible (Stuart J. Russell, 2019) book cover.jpg" src="//upload.wikimedia.org/wikipedia/en/thumb/d/db/Human_Compatible_%28Stuart_J._Russell%2C_2019%29_book_cover.jpg/220px-Human_Compatible_%28Stuart_J._Russell%2C_2019%29_book_cover.jpg" decoding="async" width="220" height="332" srcset="//upload.wikimedia.org/wikipedia/en/d/db/Human_Compatible_%28Stuart_J._Russell%2C_2019%29_book_cover.jpg 1.5x" data-file-width="257" data-file-height="388" /></a><div>Hardcover edition</div></td></tr><tr><th scope="row">Author</th><td><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a></td></tr><tr><th scope="row">Country</th><td>United States</td></tr><tr><th scope="row">Language</th><td>English</td></tr><tr><th scope="row">Subject</th><td><a href="/wiki/AI_control_problem" title="AI control problem">AI control problem</a></td></tr><tr><th scope="row">Genre</th><td>Non-fiction</td></tr><tr><th scope="row">Publisher</th><td>Viking</td></tr><tr><th scope="row"><div style="padding:0.1em 0;line-height:1.2em;">Publication date</div></th><td>October 8, 2019</td></tr><tr><th scope="row">Pages</th><td>352</td></tr><tr><th scope="row"><a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a></th><td><style data-mw-deduplicate="TemplateStyles:r935243608">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style><a href="/wiki/Special:BookSources/978-0-525-55861-3" title="Special:BookSources/978-0-525-55861-3">978-0-525-55861-3</a></td></tr><tr><th scope="row"><a href="/wiki/OCLC#Identifiers_and_linked_data" title="OCLC"><abbr title="Online Computer Library Center number">OCLC</abbr></a></th><td><a rel="nofollow" class="external text" href="https://www.worldcat.org/oclc/1083694322">1083694322</a></td></tr></tbody></table>
<p><i><b>Human Compatible: Artificial Intelligence and the Problem of Control</b></i> is a 2019 non-fiction book by computer scientist <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a>.  It asserts that <a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">risk to humanity</a> from advanced <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> (AI) is a serious concern despite the uncertainty surrounding future progress in AI.  It also proposes an approach to the <a href="/wiki/AI_control_problem" title="AI control problem">AI control problem</a>.
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Summary"><span class="tocnumber">1</span> <span class="toctext">Summary</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Russell&#39;s_three_principles"><span class="tocnumber">2</span> <span class="toctext">Russell's three principles</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Reception"><span class="tocnumber">3</span> <span class="toctext">Reception</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#See_also"><span class="tocnumber">4</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#References"><span class="tocnumber">5</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#External_links"><span class="tocnumber">6</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Summary">Summary</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Human_Compatible&amp;action=edit&amp;section=1" title="Edit section: Summary">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Russell begins by asserting that the standard model of AI research, in which the primary definition of success is getting better and better at achieving rigid human-specified goals, is dangerously misguided.  Such goals may not actually reflect what human designers intend, such as by failing to take into account any human values not included in the goals.  If an AI developed according to the standard model were to become <a href="/wiki/Superintelligence" title="Superintelligence">superintelligent</a>, it would likely not fully reflect human values and could be catastrophic to humanity. Russell asserts that precisely because the timeline for developing human-level or superintelligent AI is highly uncertain, safety research should be begun as soon as possible, as it is also highly uncertain how long it would take to complete such research.
</p><p>Russell argues that continuing progress in AI capability is inevitable because of economic pressures.  Such pressures can already be seen in the development of existing AI technologies such as self-driving cars and personal assistant software.  Moreover, human-level AI could be worth many trillions of dollars.  Russell then examines the current debate surrounding AI risk.  He offers refutations to a number of common arguments dismissing AI risk and attributes much of their persistence to tribalism--AI researchers may see AI risk concerns as an "attack" on their field.  However, Russell reiterates that there are legitimate reasons to take AI risk concerns seriously and that economic pressures make continued innovation in AI inevitable.
</p><p>Russell then proposes <a href="/wiki/Human_Compatible#Russell&#39;s_three_principles" title="Human Compatible">an approach</a> to developing provably beneficial machines that focuses on deference to humans. Unlike in the standard model of AI,  where the objective is rigid and certain, this approach would have the AI's true objective remain uncertain, with the AI only approaching certainty about it as it gains more information about humans and the world.  This uncertainty would, ideally, prevent catastrophic misunderstandings of human preferences and encourage cooperation and communication with humans.  Russell concludes by calling for tighter governance of AI research and development as well as cultural introspection about the appropriate amount of autonomy to retain in an AI-dominated world.
</p>
<h2><span id="Russell.27s_three_principles"></span><span class="mw-headline" id="Russell's_three_principles">Russell's three principles</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Human_Compatible&amp;action=edit&amp;section=2" title="Edit section: Russell&#039;s three principles">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Russell lists three principles to guide the development of beneficial machines.  He emphasizes that these principles are not meant to be explicitly coded into the machines; rather, they are intended for the human developers.  The principles are as follows:<sup id="cite_ref-HC_1-0" class="reference"><a href="#cite_note-HC-1">&#91;1&#93;</a></sup><sup class="reference" style="white-space:nowrap;">:<span>173</span></sup>
</p>
<style data-mw-deduplicate="TemplateStyles:r886047036">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}</style><blockquote class="templatequote"><p>1. The machine's only objective is to maximize the realization of human preferences.
</p><p>2. The machine is initially uncertain about what those preferences are.
</p><p>3. The ultimate source of information about human preferences is human behavior.
</p>
</blockquote>
<p>The "preferences" Russell refers to "are all-encompassing; they cover everything you might care about, arbitrarily far into the future."<sup id="cite_ref-HC_1-1" class="reference"><a href="#cite_note-HC-1">&#91;1&#93;</a></sup><sup class="reference" style="white-space:nowrap;">:<span>173</span></sup>  Similarly, "behavior" includes any choice between options,<sup id="cite_ref-HC_1-2" class="reference"><a href="#cite_note-HC-1">&#91;1&#93;</a></sup><sup class="reference" style="white-space:nowrap;">:<span>177</span></sup> and the uncertainty is such that some probability, which may be quite small, must be assigned to every logically possible human preference.<sup id="cite_ref-HC_1-3" class="reference"><a href="#cite_note-HC-1">&#91;1&#93;</a></sup><sup class="reference" style="white-space:nowrap;">:<span>201</span></sup>
</p><p>Russell explores <a href="/wiki/Reinforcement_learning#Inverse_reinforcement_learning" title="Reinforcement learning">inverse reinforcement learning</a>, in which a machine infers a reward function from observed behavior, as a possible basis for a mechanism for learning human preferences.<sup id="cite_ref-HC_1-4" class="reference"><a href="#cite_note-HC-1">&#91;1&#93;</a></sup><sup class="reference" style="white-space:nowrap;">:<span>191-193</span></sup>
</p>
<h2><span class="mw-headline" id="Reception">Reception</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Human_Compatible&amp;action=edit&amp;section=3" title="Edit section: Reception">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><i>Human Compatible</i> was reviewed by Ian Sample in <i><a href="/wiki/The_Guardian" title="The Guardian">The Guardian</a></i>,<sup id="cite_ref-sample_2-0" class="reference"><a href="#cite_note-sample-2">&#91;2&#93;</a></sup> James McConnachie in <i><a href="/wiki/The_Times" title="The Times">The Times</a></i>,<sup id="cite_ref-mcconnachie_3-0" class="reference"><a href="#cite_note-mcconnachie-3">&#91;3&#93;</a></sup> Matthew Hutson in <i><a href="/wiki/The_Wall_Street_Journal" title="The Wall Street Journal">The Wall Street Journal</a></i>,<sup id="cite_ref-hutson_4-0" class="reference"><a href="#cite_note-hutson-4">&#91;4&#93;</a></sup> and blogger Scott Alexander.<sup id="cite_ref-alexander_5-0" class="reference"><a href="#cite_note-alexander-5">&#91;5&#93;</a></sup>
</p><p><i>Human Compatible</i> was criticized in a <i><a href="/wiki/New_York_Times" class="mw-redirect" title="New York Times">New York Times</a></i> opinion essay by <a href="/wiki/Melanie_Mitchell" title="Melanie Mitchell">Melanie Mitchell</a>, arguing that AI risk cannot be seriously engaged with until intelligence is better understood.<sup id="cite_ref-mitchell_6-0" class="reference"><a href="#cite_note-mitchell-6">&#91;6&#93;</a></sup> However, it was praised by Ned Desmond in <a href="/wiki/TechCrunch" title="TechCrunch">TechCrunch</a>, calling it "a carefully written explanation of the concepts underlying AI as well as the history of their development."<sup id="cite_ref-desmond_7-0" class="reference"><a href="#cite_note-desmond-7">&#91;7&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Human_Compatible&amp;action=edit&amp;section=4" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><i><a href="/wiki/Artificial_Intelligence:_A_Modern_Approach" title="Artificial Intelligence: A Modern Approach">Artificial Intelligence: A Modern Approach</a></i></li>
<li><a href="/wiki/Center_for_Human-Compatible_Artificial_Intelligence" title="Center for Human-Compatible Artificial Intelligence">Center for Human-Compatible Artificial Intelligence</a></li>
<li><i><a href="/wiki/Slaughterbots" title="Slaughterbots">Slaughterbots</a></i></li>
<li><i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Human_Compatible&amp;action=edit&amp;section=5" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-HC-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-HC_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-HC_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-HC_1-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-HC_1-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-HC_1-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell, Stuart</a> (October 8, 2019). <i>Human Compatible: Artificial Intelligence and the Problem of Control</i>. United States: Viking. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-525-55861-3" title="Special:BookSources/978-0-525-55861-3"><bdi>978-0-525-55861-3</bdi></a>. <a href="/wiki/OCLC" title="OCLC">OCLC</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/oclc/1083694322">1083694322</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Human+Compatible%3A+Artificial+Intelligence+and+the+Problem+of+Control&amp;rft.place=United+States&amp;rft.pub=Viking&amp;rft.date=2019-10-08&amp;rft_id=info%3Aoclcnum%2F1083694322&amp;rft.isbn=978-0-525-55861-3&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHuman+Compatible" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-sample-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-sample_2-0">^</a></b></span> <span class="reference-text"><cite class="citation news">Sample, Ian (October 24, 2019). <a rel="nofollow" class="external text" href="https://www.theguardian.com/books/2019/oct/24/human-compatible-ai-problem-control-stuart-russell-review">"Human Compatible by Stuart Russell review – AI and our future"</a>. <i><a href="/wiki/The_Guardian" title="The Guardian">The Guardian</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=Human+Compatible+by+Stuart+Russell+review+%E2%80%93+AI+and+our+future&amp;rft.date=2019-10-24&amp;rft.aulast=Sample&amp;rft.aufirst=Ian&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fbooks%2F2019%2Foct%2F24%2Fhuman-compatible-ai-problem-control-stuart-russell-review&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHuman+Compatible" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-mcconnachie-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-mcconnachie_3-0">^</a></b></span> <span class="reference-text"><cite class="citation news">McConnachie, James (October 6, 2019). <a rel="nofollow" class="external text" href="https://www.thetimes.co.uk/magazine/culture/human-compatible-by-stuart-russell-review-an-ai-experts-chilling-warning-k2p0j3hw6">"Human Compatible by Stuart Russell review — an AI expert's chilling warning"</a>. <i><a href="/wiki/The_Times" title="The Times">The Times</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Times&amp;rft.atitle=Human+Compatible+by+Stuart+Russell+review+%E2%80%94+an+AI+expert%E2%80%99s+chilling+warning&amp;rft.date=2019-10-06&amp;rft.aulast=McConnachie&amp;rft.aufirst=James&amp;rft_id=https%3A%2F%2Fwww.thetimes.co.uk%2Fmagazine%2Fculture%2Fhuman-compatible-by-stuart-russell-review-an-ai-experts-chilling-warning-k2p0j3hw6&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHuman+Compatible" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-hutson-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-hutson_4-0">^</a></b></span> <span class="reference-text"><cite class="citation news">Hutson, Matthew (November 19, 2019). <a rel="nofollow" class="external text" href="https://www.wsj.com/articles/human-compatible-and-artificial-intelligence-review-learn-like-a-machine-11574207170">"<span class="cs1-kern-left">'</span>Human Compatible' and 'Artificial Intelligence' Review: Learn Like a Machine"</a>. <i><a href="/wiki/The_Wall_Street_Journal" title="The Wall Street Journal">The Wall Street Journal</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Wall+Street+Journal&amp;rft.atitle=%E2%80%98Human+Compatible%E2%80%99+and+%E2%80%98Artificial+Intelligence%E2%80%99+Review%3A+Learn+Like+a+Machine&amp;rft.date=2019-11-19&amp;rft.aulast=Hutson&amp;rft.aufirst=Matthew&amp;rft_id=https%3A%2F%2Fwww.wsj.com%2Farticles%2Fhuman-compatible-and-artificial-intelligence-review-learn-like-a-machine-11574207170&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHuman+Compatible" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-alexander-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-alexander_5-0">^</a></b></span> <span class="reference-text"><cite class="citation web">Alexander, Scott (January 30, 2020). <a rel="nofollow" class="external text" href="https://slatestarcodex.com/2020/01/30/book-review-human-compatible/">"Book Review: Human Compatible"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Book+Review%3A+Human+Compatible&amp;rft.date=2020-01-30&amp;rft.aulast=Alexander&amp;rft.aufirst=Scott&amp;rft_id=https%3A%2F%2Fslatestarcodex.com%2F2020%2F01%2F30%2Fbook-review-human-compatible%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHuman+Compatible" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-mitchell-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-mitchell_6-0">^</a></b></span> <span class="reference-text"><cite class="citation news">Mitchell, Melanie (October 31, 2019). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2019/10/31/opinion/superintelligent-artificial-intelligence.html">"We Shouldn't be Scared by 'Superintelligent A.I.<span class="cs1-kern-right">'</span>"</a>. <i><a href="/wiki/The_New_York_Times" title="The New York Times">The New York Times</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=We+Shouldn%E2%80%99t+be+Scared+by+%E2%80%98Superintelligent+A.I.%E2%80%99&amp;rft.date=2019-10-31&amp;rft.aulast=Mitchell&amp;rft.aufirst=Melanie&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2019%2F10%2F31%2Fopinion%2Fsuperintelligent-artificial-intelligence.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHuman+Compatible" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
<li id="cite_note-desmond-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-desmond_7-0">^</a></b></span> <span class="reference-text"><cite class="citation news">Desmond, Ned (October 6, 2019). <a rel="nofollow" class="external text" href="https://techcrunch.com/2019/10/06/human-compatible-is-a-provocative-prescription-to-re-think-ai-before-its-too-late/">"<span class="cs1-kern-left">'</span>Human Compatible' is a provocative prescription to re-think AI before it's too late"</a>. <i><a href="/wiki/TechCrunch" title="TechCrunch">TechCrunch</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=TechCrunch&amp;rft.atitle=%27Human+Compatible%27+is+a+provocative+prescription+to+re-think+AI+before+it%27s+too+late&amp;rft.date=2019-10-06&amp;rft.aulast=Desmond&amp;rft.aufirst=Ned&amp;rft_id=https%3A%2F%2Ftechcrunch.com%2F2019%2F10%2F06%2Fhuman-compatible-is-a-provocative-prescription-to-re-think-ai-before-its-too-late%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHuman+Compatible" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r935243608"/></span>
</li>
</ol></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Human_Compatible&amp;action=edit&amp;section=6" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="https://www.ft.com/content/0e79832c-ef48-11e9-bfa4-b25f11f42901">Financial Times review</a></li>
<li><a rel="nofollow" class="external text" href="https://techcrunch.com/2019/10/06/an-interview-with-dr-stuart-russell-author-of-human-compatible-artificial-intelligence-and-the-problem-of-control/">Interview with Stuart J. Russell</a></li>
<li><a rel="nofollow" class="external text" href="https://www.kirkusreviews.com/book-reviews/stuart-russell/human-compatible/">Kirkus review</a></li></ul>
<div role="navigation" class="navbox" aria-labelledby="Existential_risk_from_artificial_intelligence" style="padding:3px"><table class="nowraplinks mw-collapsible expanded navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Existential_risk_from_artificial_intelligence" title="Template:Existential risk from artificial intelligence"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Existential_risk_from_artificial_intelligence" title="Template talk:Existential risk from artificial intelligence"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Existential_risk_from_artificial_intelligence" style="font-size:114%;margin:0 4em"><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk</a> from <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Accelerating_change" title="Accelerating change">Accelerating change</a></li>
<li><a href="/wiki/AI_box" title="AI box">AI box</a></li>
<li><a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></li>
<li><a href="/wiki/AI_control_problem" title="AI control problem">Control problem</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly artificial intelligence</a></li>
<li><a href="/wiki/Instrumental_convergence" title="Instrumental convergence">Instrumental convergence</a></li>
<li><a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">Intelligence explosion</a></li>
<li><a href="/wiki/Machine_ethics" title="Machine ethics">Machine ethics</a></li>
<li><a href="/wiki/Superintelligence" title="Superintelligence">Superintelligence</a></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Allen_Institute_for_Artificial_Intelligence" class="mw-redirect" title="Allen Institute for Artificial Intelligence">Allen Institute for Artificial Intelligence</a></li>
<li><a href="/wiki/Center_for_Applied_Rationality" title="Center for Applied Rationality">Center for Applied Rationality</a></li>
<li><a href="/wiki/Center_for_Human-Compatible_Artificial_Intelligence" title="Center for Human-Compatible Artificial Intelligence">Center for Human-Compatible Artificial Intelligence</a></li>
<li><a href="/wiki/Center_for_Security_and_Emerging_Technology" title="Center for Security and Emerging Technology">Center for Security and Emerging Technology</a></li>
<li><a href="/wiki/Centre_for_the_Study_of_Existential_Risk" title="Centre for the Study of Existential Risk">Centre for the Study of Existential Risk</a></li>
<li><a href="/wiki/DeepMind" title="DeepMind">DeepMind</a></li>
<li><a href="/wiki/Foundational_Questions_Institute" title="Foundational Questions Institute">Foundational Questions Institute</a></li>
<li><a href="/wiki/Future_of_Humanity_Institute" title="Future of Humanity Institute">Future of Humanity Institute</a></li>
<li><a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a></li>
<li><a href="/wiki/Humanity%2B" title="Humanity+">Humanity+</a></li>
<li><a href="/wiki/Institute_for_Ethics_and_Emerging_Technologies" title="Institute for Ethics and Emerging Technologies">Institute for Ethics and Emerging Technologies</a></li>
<li><a href="/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence" title="Leverhulme Centre for the Future of Intelligence">Leverhulme Centre for the Future of Intelligence</a></li>
<li><a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a></li>
<li><a href="/wiki/K._Eric_Drexler" title="K. Eric Drexler">Eric Drexler</a></li>
<li><a href="/wiki/Sam_Harris" title="Sam Harris">Sam Harris</a></li>
<li><a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a></li>
<li><a href="/wiki/Bill_Hibbard" title="Bill Hibbard">Bill Hibbard</a></li>
<li><a href="/wiki/Bill_Joy" title="Bill Joy">Bill Joy</a></li>
<li><a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a></li>
<li><a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a></li>
<li><a href="/wiki/Huw_Price" title="Huw Price">Huw Price</a></li>
<li><a href="/wiki/Martin_Rees" title="Martin Rees">Martin Rees</a></li>
<li><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a></li>
<li><a href="/wiki/Jaan_Tallinn" title="Jaan Tallinn">Jaan Tallinn</a></li>
<li><a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a></li>
<li><a href="/wiki/Frank_Wilczek" title="Frank Wilczek">Frank Wilczek</a></li>
<li><a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a></li>
<li><a href="/wiki/Andrew_Yang" title="Andrew Yang">Andrew Yang</a></li>
<li><a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Other</th><td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="/wiki/Global_catastrophic_risk#Artificial_intelligence" title="Global catastrophic risk">Artificial intelligence as a global catastrophic risk</a></li>
<li><a href="/wiki/Artificial_general_intelligence#Controversies_and_dangers" title="Artificial general intelligence">Controversies and dangers of artificial general intelligence</a></li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of artificial intelligence</a></li>
<li><i><a class="mw-selflink selflink">Human Compatible</a></i></li>
<li><a href="/wiki/Open_Letter_on_Artificial_Intelligence" title="Open Letter on Artificial Intelligence">Open Letter on Artificial Intelligence</a></li>
<li><i><a href="/wiki/Our_Final_Invention" title="Our Final Invention">Our Final Invention</a></i></li>
<li><i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<dl><dt><img alt="Category" src="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" decoding="async" title="Category" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31" /> <a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Category</a></dt></dl>
</div></td></tr></tbody></table></div>
<p><br />
</p>
<table class="metadata plainlinks stub" role="presentation" style="background:transparent"><tbody><tr><td><a href="/wiki/File:LampFlowchart.svg" class="image"><img alt="Stub icon" src="//upload.wikimedia.org/wikipedia/commons/thumb/9/91/LampFlowchart.svg/20px-LampFlowchart.svg.png" decoding="async" width="20" height="27" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/9/91/LampFlowchart.svg/30px-LampFlowchart.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/91/LampFlowchart.svg/40px-LampFlowchart.svg.png 2x" data-file-width="324" data-file-height="442" /></a></td><td><i>This article about a book on <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> is a <a href="/wiki/Wikipedia:Stub" title="Wikipedia:Stub">stub</a>. You can help Wikipedia by <a class="external text" href="https://en.wikipedia.org/w/index.php?title=Human_Compatible&amp;action=edit">expanding it</a>.</i><div class="plainlinks hlist navbar mini" style="position: absolute; right: 15px; display: none;"><ul><li class="nv-view"><a href="/wiki/Template:Ai-book-stub" title="Template:Ai-book-stub"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Ai-book-stub" title="Template talk:Ai-book-stub"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Ai-book-stub&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>

<!-- 
NewPP limit report
Parsed by mw1275
Cached time: 20200220084902
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.328 seconds
Real time usage: 0.490 seconds
Preprocessor visited node count: 1607/1000000
Post‐expand include size: 37440/2097152 bytes
Template argument size: 2203/2097152 bytes
Highest expansion depth: 19/40
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 25230/5000000 bytes
Number of Wikibase entities loaded: 1/400
Lua time usage: 0.174/10.000 seconds
Lua memory usage: 4.85 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  447.598      1 -total
 36.38%  162.849      1 Template:Reflist
 29.43%  131.735      1 Template:Infobox_book
 26.62%  119.131      1 Template:Infobox
 25.62%  114.665      1 Template:Cite_book
 15.37%   68.786      1 Template:ISBNT
 12.75%   57.073      1 Template:Short_description
  9.54%   42.703      1 Template:Pagetype
  8.84%   39.567      1 Template:Catalog_lookup_link
  6.59%   29.486      1 Template:Ai-book-stub
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:63005595-0!canonical and timestamp 20200220084902 and revision id 940281141
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Human_Compatible&amp;oldid=940281141">https://en.wikipedia.org/w/index.php?title=Human_Compatible&amp;oldid=940281141</a>"</div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:2019_non-fiction_books" title="Category:2019 non-fiction books">2019 non-fiction books</a></li><li><a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li><li><a href="/wiki/Category:Futurology_books" title="Category:Futurology books">Futurology books</a></li><li><a href="/wiki/Category:Artificial_intelligence_stubs" title="Category:Artificial intelligence stubs">Artificial intelligence stubs</a></li><li><a href="/wiki/Category:Computer_book_stubs" title="Category:Computer book stubs">Computer book stubs</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:All_stub_articles" title="Category:All stub articles">All stub articles</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
    <h2>Navigation menu</h2>
    <div id="mw-head">
        
<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
	<h3 id="p-personal-label">Personal tools</h3>
	<ul >
		
		<li id="pt-anonuserpage">Not logged in</li>
		<li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Human+Compatible" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Human+Compatible" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>
	</ul>
</div>

        <div id="left-navigation">
            <div id="p-namespaces" role="navigation" class="vectorTabs " aria-labelledby="p-namespaces-label">
	<h3 id="p-namespaces-label">Namespaces</h3>
	<ul >
		<li id="ca-nstab-main" class="selected"><a href="/wiki/Human_Compatible" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="/wiki/Talk:Human_Compatible" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></li>
	</ul>
</div>
<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
	<h3 id="p-variants-label">
		<span>Variants</span>
	</h3>
	<ul class="menu" >
		
	</ul>
</div>

        </div>
        <div id="right-navigation">
            <div id="p-views" role="navigation" class="vectorTabs " aria-labelledby="p-views-label">
	<h3 id="p-views-label">Views</h3>
	<ul >
		<li id="ca-view" class="collapsible selected"><a href="/wiki/Human_Compatible">Read</a></li><li id="ca-edit" class="collapsible"><a href="/w/index.php?title=Human_Compatible&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="/w/index.php?title=Human_Compatible&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li>
	</ul>
</div>
<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
	<h3 id="p-cactions-label">
		<span>More</span>
	</h3>
	<ul class="menu" >
		
	</ul>
</div>
<div id="p-search" role="search">
	<h3 >
		<label for="searchInput">Search</label>
	</h3>
	<form action="/w/index.php" id="searchform">
		<div id="simpleSearch">
			<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
			<input type="hidden" value="Special:Search" name="title"/>
			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
		</div>
	</form>
</div>

        </div>
    </div>
    <div id="mw-panel">
        <div id="p-logo" role="banner">
            <a  title="Visit the main page" class="mw-wiki-logo" href="/wiki/Main_Page"></a>
        </div>
        
<div class="portal" role="navigation" id="p-navigation"  aria-labelledby="p-navigation-label">
	<h3  id="p-navigation-label">
		Navigation
	</h3>
	<div class="body">
		<ul><li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-interaction"  aria-labelledby="p-interaction-label">
	<h3  id="p-interaction-label">
		Interaction
	</h3>
	<div class="body">
		<ul><li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-tb"  aria-labelledby="p-tb-label">
	<h3  id="p-tb-label">
		Tools
	</h3>
	<div class="body">
		<ul><li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Human_Compatible" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Human_Compatible" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Human_Compatible&amp;oldid=940281141" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Human_Compatible&amp;action=info" title="More information about this page">Page information</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Human_Compatible&amp;id=940281141" title="Information on how to cite this page">Cite this page</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-coll-print_export"  aria-labelledby="p-coll-print_export-label">
	<h3  id="p-coll-print_export-label">
		Print/export
	</h3>
	<div class="body">
		<ul><li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Human+Compatible">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Human+Compatible&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Human_Compatible&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-lang"  aria-labelledby="p-lang-label">
	<h3  id="p-lang-label">
		Languages
	</h3>
	<div class="body">
		<ul></ul>
		<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-add wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:NewItem?site=enwiki&amp;page=Human+Compatible" title="Add interlanguage links" class="wbc-editpage">Add links</a></span></div>
	</div>
</div>

    </div>
</div>


<div id="footer" role="contentinfo" >
	<ul id="footer-info" class="">
		<li id="footer-info-lastmod"> This page was last edited on 11 February 2020, at 16:23<span class="anonymous-show">&#160;(UTC)</span>.</li>
		<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
	</ul>
	<ul id="footer-places" class="">
		<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
		<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
		<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
		<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/v2/#/en.wikipedia.org">Statistics</a></li>
		<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
		<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Human_Compatible&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a></li>
		<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a></li>
	</ul>
	<div style="clear: both;"></div>
</div>


<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.328","walltime":"0.490","ppvisitednodes":{"value":1607,"limit":1000000},"postexpandincludesize":{"value":37440,"limit":2097152},"templateargumentsize":{"value":2203,"limit":2097152},"expansiondepth":{"value":19,"limit":40},"expensivefunctioncount":{"value":1,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":25230,"limit":5000000},"entityaccesscount":{"value":1,"limit":400},"timingprofile":["100.00%  447.598      1 -total"," 36.38%  162.849      1 Template:Reflist"," 29.43%  131.735      1 Template:Infobox_book"," 26.62%  119.131      1 Template:Infobox"," 25.62%  114.665      1 Template:Cite_book"," 15.37%   68.786      1 Template:ISBNT"," 12.75%   57.073      1 Template:Short_description","  9.54%   42.703      1 Template:Pagetype","  8.84%   39.567      1 Template:Catalog_lookup_link","  6.59%   29.486      1 Template:Ai-book-stub"]},"scribunto":{"limitreport-timeusage":{"value":"0.174","limit":"10.000"},"limitreport-memusage":{"value":5082736,"limit":52428800}},"cachereport":{"origin":"mw1275","timestamp":"20200220084902","ttl":2592000,"transientcontent":false}}});mw.config.set({"wgBackendResponseTime":89,"wgHostname":"mw1268"});});</script></body></html>
